{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f1e609-89de-450d-9518-2341f2269853",
   "metadata": {},
   "source": [
    "\n",
    "## 1일차 수업: 순환 신경망 (RNN), 장단기 기억망 (LSTM), 게이트 순환 유닛 (GRU) 심층 분석 및 파이토치 실습\n",
    "\n",
    "오늘 수업에서는 기본적인 순환 신경망(RNN)부터 시작하여, 장기 의존성 문제를 해결하기 위해 등장한 LSTM, 그리고 LSTM의 간소화된 형태인 GRU까지 세 가지 순환 신경망 모델의 이론적 배경과 구조를 심층적으로 분석해 보겠습니다. 이론 학습 후에는 파이토치를 이용하여 각 모델을 간단하게 구현하고 텍스트 분류 작업을 수행하는 실습 시간을 갖도록 하겠습니다.\n",
    "\n",
    "### 1. 순환 신경망 (RNN - Recurrent Neural Network)\n",
    "\n",
    "**1.1 이론 복습:**\n",
    "\n",
    "* RNN은 순차적인 데이터(sequential data)를 처리하기 위해 설계된 신경망으로, 이전 시점의 정보를 현재 시점의 입력과 함께 고려하여 문맥을 파악하는 데 강점을 가집니다.\n",
    "* **핵심 아이디어:** 내부의 순환 구조를 통해 이전 시점의 은닉 상태(hidden state)를 현재 시점으로 전달하여 과거 정보를 기억하고 활용합니다.\n",
    "* **수학적 표현 (간략화):**\n",
    "    $\\qquad h_t = \\tanh(W_{ih} x_t + W_{hh} h_{t-1} + b_h)$\n",
    "    $\\qquad y_t = W_{ho} h_t + b_o$\n",
    "    * $x_t$: 현재 시점의 입력\n",
    "    * $h_t$: 현재 시점의 은닉 상태\n",
    "    * $h_{t-1}$: 이전 시점의 은닉 상태\n",
    "    * $y_t$: 현재 시점의 출력\n",
    "    * $W_{ih}, W_{hh}, W_{ho}$: 학습 가능한 가중치 행렬\n",
    "    * $b_h, b_o$: 학습 가능한 편향 벡터\n",
    "    * $\\tanh$: 활성화 함수\n",
    "\n",
    "**1.2 주요 문제점:**\n",
    "\n",
    "* **기울기 소실/폭주 (Vanishing/Exploding Gradient):** 긴 시퀀스 데이터를 처리할 때, 역전파 과정에서 기울기가 점차 작아지거나 커져 학습이 제대로 이루어지지 않는 문제입니다.\n",
    "* **장기 의존성 문제 (Long-Term Dependency Problem):** 멀리 떨어진 단어 간의 관계를 학습하기 어렵습니다. 초기 시점의 정보가 뒤쪽 시점까지 제대로 전달되지 못하는 경향이 있습니다.\n",
    "\n",
    "### 2. 장단기 기억망 (LSTM - Long Short-Term Memory)\n",
    "\n",
    "**2.1 핵심 아이디어:**\n",
    "\n",
    "* RNN의 장기 의존성 문제를 해결하기 위해 제안된 모델로, **게이트(gate)**라는 메커니즘을 도입하여 불필요한 정보를 잊고 필요한 정보를 기억하는 능력을 향상시켰습니다.\n",
    "* **셀 상태 (Cell State):** 장기적인 정보를 저장하고 전달하는 역할을 하는 핵심적인 요소입니다. 은닉 상태와 별도로 존재하며, 게이트에 의해 제어됩니다.\n",
    "\n",
    "**2.2 LSTM의 구조:**\n",
    "\n",
    "* **입력 게이트 (Input Gate):** 현재 입력과 이전 은닉 상태를 기반으로 셀 상태에 저장할 새로운 정보를 결정합니다.\n",
    "* **망각 게이트 (Forget Gate):** 이전 셀 상태에서 버릴 정보를 결정합니다.\n",
    "* **출력 게이트 (Output Gate):** 현재 입력, 이전 은닉 상태, 그리고 업데이트된 셀 상태를 기반으로 현재 은닉 상태를 결정합니다.\n",
    "\n",
    "**2.3 수학적 표현:**\n",
    "\n",
    "$\\qquad i_t = \\sigma(W_{xi} x_t + W_{hi} h_{t-1} + b_i)$ (입력 게이트)\n",
    "$\\qquad f_t = \\sigma(W_{xf} x_t + W_{hf} h_{t-1} + b_f)$ (망각 게이트)\n",
    "$\\qquad o_t = \\sigma(W_{xo} x_t + W_{ho} h_{t-1} + b_o)$ (출력 게이트)\n",
    "$\\qquad \\tilde{C}_t = \\tanh(W_{xc} x_t + W_{hc} h_{t-1} + b_c)$ (새로운 셀 상태 후보)\n",
    "$\\qquad C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$ (현재 셀 상태)\n",
    "$\\qquad h_t = o_t \\odot \\tanh(C_t)$ (현재 은닉 상태)\n",
    "\n",
    "* $\\sigma$: 시그모이드 함수\n",
    "* $\\tanh$: 하이퍼볼릭 탄젠트 함수\n",
    "* $\\odot$: 요소별 곱셈\n",
    "\n",
    "### 3. 게이트 순환 유닛 (GRU - Gated Recurrent Unit)\n",
    "\n",
    "**3.1 핵심 아이디어:**\n",
    "\n",
    "* LSTM의 복잡한 구조를 간소화하면서도 비슷한 성능을 내도록 설계된 모델입니다. 망각 게이트와 입력 게이트를 하나의 **업데이트 게이트 (Update Gate)**로 통합하고, 셀 상태와 은닉 상태를 합쳐 구조를 단순화했습니다.\n",
    "\n",
    "**3.2 GRU의 구조:**\n",
    "\n",
    "* **업데이트 게이트 (Update Gate):** 이전 은닉 상태를 얼마나 유지하고 새로운 입력을 얼마나 반영할지를 결정합니다. (LSTM의 망각 게이트와 입력 게이트의 역할을 통합)\n",
    "* **리셋 게이트 (Reset Gate):** 이전 은닉 상태가 현재 상태 계산에 얼마나 영향을 미칠지를 결정합니다.\n",
    "\n",
    "**3.3 수학적 표현:**\n",
    "\n",
    "$\\qquad z_t = \\sigma(W_{xz} x_t + W_{hz} h_{t-1} + b_z)$ (업데이트 게이트)\n",
    "$\\qquad r_t = \\sigma(W_{xr} x_t + W_{hr} h_{t-1} + b_r)$ (리셋 게이트)\n",
    "$\\qquad \\tilde{h}_t = \\tanh(W_{xh} x_t + W_{hh} (r_t \\odot h_{t-1}) + b_h)$ (새로운 은닉 상태 후보)\n",
    "$\\qquad h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$ (현재 은닉 상태)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2a06e7f6-5237-450d-a62d-25d1dfeefba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader as loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1564889f-201f-42c8-bf5f-00351594668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'acting', 'bad', 'enjoyed', 'film', 'great', 'i', 'is', 'it', 'movie', 'terrible', 'the', 'this', 'was']\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# 간단한 예시 데이터 (단어와 레이블)\n",
    "train_data = [\n",
    "    (\"this movie is great\", 1),  # 1: positive\n",
    "    (\"the acting is bad\", 0),   # 0: negative\n",
    "    (\"i enjoyed the film\", 1),\n",
    "    (\"it was a terrible movie\", 0),\n",
    "]\n",
    "\n",
    "vocab=set()\n",
    "for sentence,_ in train_data:    \n",
    "    for word in sentence.split():\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab=sorted(list(vocab))\n",
    "print(vocab)\n",
    "vocab_size=len(vocab)\n",
    "print(vocab_size)\n",
    "word_to_index={word:i for i, word in enumerate(vocab)}\n",
    "index_to_word={i: word for word, i in word_to_index.items()}\n",
    "\n",
    "\n",
    "def sentence_to_indices(sentence):\n",
    "    return [word_to_index[word]for word in sentence.split()]\n",
    "\n",
    "\n",
    "#indexed_data=[(torch.tensor(sentence_to_indices(s)),torch.tensor([l]))for s,l in train_data]\n",
    "indexed_data = [(torch.tensor(sentence_to_indices(s)).unsqueeze(0), torch.tensor([l]).float().unsqueeze(0)) for s, l in train_data]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0bd8c9e0-1fc9-4ed5-a167-591f4c436954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, out_size):\n",
    "        super(RNN,self).__init__()\n",
    "        self.embbading=nn.Embedding(vocab_size,embedding_size)\n",
    "        self.rnn=nn.RNN(embedding_size, hidden_size,batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        embedded=self.embbading(x)\n",
    "        out, hidden=self.rnn(embedded)\n",
    "        return torch.sigmoid(self.fc(hidden[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ea522a87-3abf-4ba2-8d36-e8088684a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, out_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm=nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding=self.embedding(x)\n",
    "        out, (hidden,cell)=self.lstm(embedding)\n",
    "        return torch.sigmoid(self.fc(hidden[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a5e71f21-3516-4572-8e6b-c8675bb49f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_size, hidden_size, out_size):\n",
    "        super(GRU,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_size)\n",
    "        self.gru=nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding=self.embedding(x)\n",
    "        out,hidden=self.gru(embedding)\n",
    "        return torch.sigmoid(self.fc(hidden[-1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1634c863-3f43-425f-a1f1-897c85f08243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda mode\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available()else'cpu')\n",
    "print(device,\"mode\")\n",
    "def train(model,num_epoch, train_data, optim, criter):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    for epoch in range(num_epoch):\n",
    "        for data, target in train_data:\n",
    "            model=model.to(device)\n",
    "            data, target=data.to(device), target.to(device)\n",
    "            optim.zero_grad()\n",
    "            out=model(data)\n",
    "            loss=criter(out, target.float())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss+=loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_data):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "23600644-c3a3-4641-ba65-f07b6cfe2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "embedding_dim = 10\n",
    "hidden_dim = 16\n",
    "output_dim = 1  # 긍정/부정 (binary classification)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e579341e-0358-4bac-a91c-59ac9d7ac205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7104\n",
      "Epoch 2, Loss: 1.2524\n",
      "Epoch 3, Loss: 1.6702\n",
      "Epoch 4, Loss: 1.9707\n",
      "Epoch 5, Loss: 2.1692\n",
      "Epoch 6, Loss: 2.2949\n",
      "Epoch 7, Loss: 2.3766\n",
      "Epoch 8, Loss: 2.4324\n",
      "Epoch 9, Loss: 2.4727\n",
      "Epoch 10, Loss: 2.5032\n",
      "/////////////////////////////////////////////////////////////////\n",
      "Epoch 1, Loss: 0.7055\n",
      "Epoch 2, Loss: 1.3511\n",
      "Epoch 3, Loss: 1.9475\n",
      "Epoch 4, Loss: 2.4814\n",
      "Epoch 5, Loss: 2.9331\n",
      "Epoch 6, Loss: 3.2858\n",
      "Epoch 7, Loss: 3.5363\n",
      "Epoch 8, Loss: 3.6981\n",
      "Epoch 9, Loss: 3.7952\n",
      "Epoch 10, Loss: 3.8519\n",
      "/////////////////////////////////////////////////////////////////\n",
      "Epoch 1, Loss: 0.7250\n",
      "Epoch 2, Loss: 1.3704\n",
      "Epoch 3, Loss: 1.9584\n",
      "Epoch 4, Loss: 2.4821\n",
      "Epoch 5, Loss: 2.9282\n",
      "Epoch 6, Loss: 3.2841\n",
      "Epoch 7, Loss: 3.5458\n",
      "Epoch 8, Loss: 3.7231\n",
      "Epoch 9, Loss: 3.8369\n",
      "Epoch 10, Loss: 3.9093\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnn_model=RNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "lstm_model=LSTM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "gru_model=GRU(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "optimy_R=optim.Adam(rnn_model.parameters(),lr=learning_rate)\n",
    "optimy_L=optim.Adam(lstm_model.parameters(),lr=learning_rate)\n",
    "optimy_G=optim.Adam(gru_model.parameters(),lr=learning_rate)\n",
    "\n",
    "criter=nn.BCELoss()\n",
    "\n",
    "train(rnn_model, num_epochs, indexed_data, optimy_R, criter)\n",
    "print(\"/////////////////////////////////////////////////////////////////\")\n",
    "train(lstm_model, num_epochs, indexed_data, optimy_L, criter)\n",
    "print(\"/////////////////////////////////////////////////////////////////\")\n",
    "train(gru_model, num_epochs, indexed_data, optimy_G, criter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20267bd-1f06-4cce-b056-347f5581bc21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "123bdaca-c3d5-4a86-b6bd-94b127680b19",
   "metadata": {},
   "source": [
    " **은닉 상태($h_t$) 자체가 리셋 게이트($r_t$)와 업데이트 게이트($z_t$)의 역할을 수행하는 것은 아닙니다.**\r\n",
    "\r\n",
    "**리셋 게이트($r_t$)와 업데이트 게이트($z_t$)는 별도의 계산 과정을 거쳐 생성되는 벡터입니다.** 이 게이트들은 **이전 은닉 상태($h_{t-1}$)와 현재 입력($x_t$)을 기반으로** 각각 어떤 정보를 버릴지 (리셋 게이트) 그리고 어떤 정보를 유지하고 새로운 정보를 얼마나 반영할지 (업데이트 게이트)를 결정하는 역할을 합니다.\r\n",
    "\r\n",
    "**GRU의 정보 흐름을 다시 한번 정리해 드리겠습니다.**\r\n",
    "\r\n",
    "1.  **업데이트 게이트($z_t$) 계산:** 이전 은닉 상태($h_{t-1}$)와 현재 입력($x_t$)을 가중치와 함께 시그모이드 함수에 통과시켜 0과 1 사이의 값을 갖는 벡터를 얻습니다. 이 값은 이전 은닉 상태를 얼마나 유지할지를 결정합니다.\r\n",
    "\r\n",
    "2.  **리셋 게이트($r_t$) 계산:** 이전 은닉 상태($h_{t-1}$)와 현재 입력($x_t$)을 또 다른 가중치와 함께 시그모이드 함수에 통과시켜 0과 1 사이의 값을 갖는 벡터를 얻습니다. 이 값은 이전 은닉 상태가 새로운 후보 은닉 상태를 계산하는 데 얼마나 영향을 미칠지를 결정합니다.\r\n",
    "\r\n",
    "3.  **새로운 은닉 상태 후보($\\tilde{h}_t$) 계산:** 현재 입력($x_t$)과 **리셋 게이트($r_t$)와 요소별 곱셈($\\odot$)을 거친 이전 은닉 상태($r_t \\odot h_{t-1}$)** 를 함께 $\\tanh$ 함수에 통과시켜 새로운 은닉 상태 후보를 생성합니다. 리셋 게이트가 0에 가까워지면 이전 은닉 상태의 영향이 거의 사라지고, 현재 입력에 더 집중하게 됩니다.\r\n",
    "\r\n",
    "4.  **현재 은닉 상태($h_t$) 업데이트:** 업데이트 게이트($z_t$)를 사용하여 이전 은닉 상태($h_{t-1}$)와 새로운 은닉 상태 후보($\\tilde{h}_t$)를 조합합니다.\r\n",
    "    $\\qquad h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$\r\n",
    "    업데이트 게이트 값이 1에 가까우면 새로운 은닉 상태 후보의 정보가 많이 반영되고, 0에 가까우면 이전 은닉 상태의 정보가 많이 유지됩니다.\r\n",
    "\r\n",
    "**결론적으로, 은닉 상태($h_t$)는 과거 정보를 저장하고 전달하는 역할을 수행하지만, 리셋 게이트($r_t$)와 업데이트 게이트($z_t$)는 이전 은닉 상태와 현재 입력을 기반으로 *별도로 계산되어* 정보의 흐름을 제어하는 역할을 합니다.** 이 게이트들의 조절을 통해 GRU는 장기 의존성을 효과적있다면 다시 편하게 질문해주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8c26b-16ed-4fb7-8329-f13b65141a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CUDA 12.4)",
   "language": "python",
   "name": "cuda124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
