{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f86fd09c-a511-4e40-92a1-1a6628e80d75",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\jh\\anaconda3\\lib\\site-packages (0.32.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\jh\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\jh\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.13.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from huggingface_hub[hf_xet]) (1.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\jh\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub[hf_xet]\n",
    "#!pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89fc0882-323e-4441-ba17-e790e024a55e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\jh\\anaconda3\\lib\\site-packages (7.8.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.6 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipywidgets) (3.6.6)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipywidgets) (1.0.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipython>=4.0.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from widgetsnbextension~=3.6.6->ipywidgets) (7.0.8)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.14.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.25.1)\n",
      "Requirement already satisfied: jupyterlab<4.1,>=4.0.2 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jh\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\jh\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\jh\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\jh\\anaconda3\\lib\\site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.1.4)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (7.10.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.9.2)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (7.4.0)\n",
      "Requirement already satisfied: packaging>=22.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (23.2)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.14.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.10)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.28.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.19.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.32.2)\n",
      "Requirement already satisfied: six in c:\\users\\jh\\anaconda3\\lib\\site-packages (from asttokens->stack-data->ipython>=4.0.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\jh\\anaconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2024.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (305.1)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\jh\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\jh\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\jh\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2024.8.30)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\jh\\anaconda3\\lib\\site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\jh\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.1)\n",
      "Requirement already satisfied: uri-template in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (24.11.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jh\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.6->ipywidgets) (1.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d40ed01-dc19-4de6-a129-babd4aac127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0ce3071-e87a-423b-ba02-57bdafcf9758",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_mel_spectrogram = torch.randn(4, 128, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "645b0381-d783-443c-a52e-7a2c7fb1845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleASRModel(nn.Module):\n",
    "    def __init__(self, num_mels, num_classes):\n",
    "        super().__init__()\n",
    "        # 인코더: 멜 스펙트로그램에서 특징 추출\n",
    "        # 여기서는 간단한 CNN 레이어만 예시로 사용. 실제는 더 복잡한 Conformer/Transformer 블록\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(num_mels, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # 필요한 경우 추가 레이어 (RNN, Transformer Encoder 등)\n",
    "            # 최종적으로 시퀀스 길이가 줄어들고 채널 수는 늘어난 특징 벡터 생성\n",
    "        )\n",
    "        # 디코더: 특징 벡터에서 텍스트 시퀀스 생성 (간략화)\n",
    "        # 실제로는 트랜스포머 디코더 또는 LSTM/GRU 디코더 사용\n",
    "        # 여기서는 마지막 특징 벡터를 클래스(토큰) 개수로 매핑하는 간단한 선형 레이어\n",
    "        self.decoder_output = nn.Linear(512, num_classes) # num_classes는 토큰(문자/음소) 개수\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x는 (batch_size, n_mels, time_steps) 형태의 멜 스펙트로그램\n",
    "        x = self.encoder(x) # 인코딩된 특징 벡터 (batch_size, feature_dim, new_time_steps)\n",
    "        # 디코딩을 위해 차원 조절 (예시)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, new_time_steps, feature_dim)\n",
    "        output = self.decoder_output(x) # (batch_size, new_time_steps, num_classes)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d260eb4-cc7e-4de0-bb11-e62022302df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 128\n",
    "output_dim = 100 # 예시로 100개의 가능한 출력 토큰이 있다고 가정\n",
    "model = SimpleASRModel(input_dim, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06590d78-b0db-4b98-a192-859ae7bef820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 출력 형태: torch.Size([4, 125, 100])\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_mel_spectrogram)\n",
    "print(f\"모델 출력 형태: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9c7facc-004b-4444-8321-d21f733c1df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer ASR 모델 출력 형태: torch.Size([4, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.nn import TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer\n",
    "\n",
    "# 편의상 아래처럼 임포트하는 경우가 많습니다.\n",
    "from torch.nn import (\n",
    "    TransformerEncoder,\n",
    "    TransformerDecoder,\n",
    "    TransformerEncoderLayer,\n",
    "    TransformerDecoderLayer\n",
    ")\n",
    "\n",
    "# --- 멜 스펙트로그램 특징 (가상의 입력) ---\n",
    "# 배치 크기=4, 멜 밴드=128, 시간 스텝=500\n",
    "# Transformer는 일반적으로 (시퀀스 길이, 배치 크기, 특징 차원) 또는\n",
    "# (배치 크기, 시퀀스 길이, 특징 차원) 형태를 기대합니다.\n",
    "# 멜 스펙트로그램은 (배치 크기, 멜 밴드 수, 시간 스텝 수) 형태이므로,\n",
    "# Transformer 입력에 맞게 차원 순서를 변경하고 (transpose)\n",
    "# 멜 밴드 수를 특징 차원으로 사용하거나, 별도의 선형 변환을 거칩니다.\n",
    "\n",
    "batch_size = 4\n",
    "n_mels = 128\n",
    "time_steps = 500\n",
    "vocab_size = 100 # 출력 토큰(문자/음소) 개수\n",
    "\n",
    "dummy_mel_spectrogram = torch.randn(batch_size, n_mels, time_steps)\n",
    "\n",
    "# --- 실제 ASR 모델 (Transformer 기반, Simplified) ---\n",
    "class TransformerASRModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_mels=128,          # 멜 스펙트로그램의 멜 밴드 개수 (입력 특징 차원)\n",
    "                 d_model=512,         # Transformer 내부 모델 차원\n",
    "                 nhead=8,             # 멀티 헤드 어텐션의 헤드 개수\n",
    "                 num_encoder_layers=6, # 인코더 레이어 개수\n",
    "                 num_decoder_layers=6, # 디코더 레이어 개수\n",
    "                 dim_feedforward=2048, # 피드포워드 네트워크의 차원\n",
    "                 dropout=0.1,\n",
    "                 num_classes=100,     # 출력 토큰(문자/음소) 개수\n",
    "                 max_len=1000):       # 최대 입력 시퀀스 길이 (위치 인코딩용)\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Input Embedding / Feature Projection for Encoder\n",
    "        # 멜 스펙트로그램의 n_mels 차원을 d_model 차원으로 매핑\n",
    "        self.input_projection = nn.Linear(n_mels, d_model)\n",
    "\n",
    "        # 2. Positional Encoding for Encoder (시간 순서 정보를 주입)\n",
    "        # 트랜스포머는 순서 정보를 직접 학습하지 않으므로 위치 인코딩이 필요\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "\n",
    "\n",
    "        # 3. Transformer Encoder\n",
    "        encoder_layer = TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True # (batch, seq_len, feature) 순서로 입력받도록 설정\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "        # 4. Target Embedding for Decoder (출력 텍스트 토큰 임베딩)\n",
    "        self.tgt_embedding = nn.Embedding(num_classes, d_model)\n",
    "        # 5. Positional Encoding for Decoder\n",
    "        self.tgt_positional_encoding = nn.Parameter(torch.randn(1, max_len, d_model))\n",
    "\n",
    "        # 6. Transformer Decoder\n",
    "        decoder_layer = TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "        # 7. Output Layer\n",
    "        self.output_linear = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src (Tensor): 인코더 입력 (멜 스펙트로그램).\n",
    "                          Shape: (batch_size, n_mels, time_steps)\n",
    "            tgt (Tensor): 디코더 입력 (타겟 텍스트 토큰 ID 시퀀스).\n",
    "                          Shape: (batch_size, target_seq_len)\n",
    "        \"\"\"\n",
    "        # --- Encoder Input Processing ---\n",
    "        # 멜 스펙트로그램 (batch, n_mels, time_steps) -> (batch, time_steps, n_mels)\n",
    "        src = src.permute(0, 2, 1)\n",
    "        # n_mels 차원을 d_model 차원으로 투영\n",
    "        src = self.input_projection(src) # (batch_size, time_steps, d_model)\n",
    "        # 위치 인코딩 추가\n",
    "        src = src + self.positional_encoding[:, :src.size(1), :] # (batch_size, time_steps, d_model)\n",
    "\n",
    "        # --- Encoder Forward ---\n",
    "        memory = self.transformer_encoder(src) # (batch_size, time_steps, d_model)\n",
    "\n",
    "        # --- Decoder Input Processing ---\n",
    "        # 타겟 토큰 ID를 임베딩\n",
    "        tgt = self.tgt_embedding(tgt) # (batch_size, target_seq_len, d_model)\n",
    "        # 위치 인코딩 추가\n",
    "        tgt = tgt + self.tgt_positional_encoding[:, :tgt.size(1), :] # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "\n",
    "        # --- Decoder Forward ---\n",
    "        # 디코더 마스크 생성 (미래 토큰을 참조하지 않도록)\n",
    "        # tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        # 소스 패딩 마스크 (패딩 토큰을 어텐션에서 제외)\n",
    "        # src_padding_mask = (src_lengths == 0) # 실제로는 패딩 인덱스를 기반으로 마스크 생성\n",
    "\n",
    "        # 간략화를 위해 마스크는 제외 (실제 구현에서는 필수)\n",
    "        output = self.transformer_decoder(tgt, memory) # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        # --- Output Layer ---\n",
    "        output = self.output_linear(output) # (batch_size, target_seq_len, num_classes)\n",
    "\n",
    "        return output\n",
    "\n",
    "# --- 모델 초기화 및 더미 데이터 통과 예시 ---\n",
    "# 모델 인스턴스 생성\n",
    "model = TransformerASRModel(\n",
    "    n_mels=n_mels,\n",
    "    d_model=512, # 트랜스포머 내부 차원\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    num_classes=vocab_size\n",
    ")\n",
    "\n",
    "# 더미 타겟 텍스트 시퀀스 (예: [START_TOKEN_ID, 10, 20, 30, END_TOKEN_ID])\n",
    "# (batch_size, target_seq_len)\n",
    "dummy_target_text_tokens = torch.randint(0, vocab_size, (batch_size, 30))\n",
    "\n",
    "# 모델에 입력 (멜 스펙트로그램과 타겟 토큰 시퀀스)\n",
    "output = model(dummy_mel_spectrogram, dummy_target_text_tokens)\n",
    "print(f\"Transformer ASR 모델 출력 형태: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19954f8-40bc-4b7e-bb0c-24e1875474a2",
   "metadata": {},
   "source": [
    "---\r\n",
    "모델의 출력 형태를 계산하는 것은 딥러닝 모델을 설계하고 디버깅하는 데 있어 굉장히 중요해요. 특히 복잡한 모델이나 여러 레이어를 거칠 때는 각 레이어가 입력 형태를 어떻게 바꾸는지 이해해야 해요.\r\n",
    "\r\n",
    "모델의 출력 형태는 주로 **입력 형태**와 모델을 구성하는 각 **레이어(층)의 특성 및 순서**에 따라 결정돼요.\r\n",
    "\r\n",
    "### 기본 원리: 각 레이어의 변환 규칙 이해하기\r\n",
    "\r\n",
    "각 레이어는 정해진 규칙에 따라 입력 텐서(tensor)의 형태(shape)를 변경해요. 가장 흔히 사용되는 레이어들의 기본적인 변환 규칙을 알면 모델 전체의 출력 형태를 추적할 수 있어요.\r\n",
    "\r\n",
    "**주요 레이어별 출력 형태 계산:**\r\n",
    "\r\n",
    "1.  **`nn.Linear` (선형 변환 / Fully Connected Layer):**\r\n",
    "    * **입력 형태:** `(..., input_features)` (가장 마지막 차원이 입력 특징의 수)\r\n",
    "    * **출력 형태:** `(..., output_features)` (마지막 차원이 출력 특징의 수로 바뀜)\r\n",
    "    * **예시:** `nn.Linear(in_features=128, out_features=256)`\r\n",
    "        * 입력이 `(batch_size, 10, 128)`이면, 출력은 `(batch_size, 10, 256)`\r\n",
    "        * 입력이 `(batch_size, 128)`이면, 출력은 `(batch_size, 256)`\r\n",
    "\r\n",
    "2.  **`nn.Conv1d` (1D 컨볼루션 레이어):**\r\n",
    "    * 음성 신호 처리(예: 스펙트로그램의 시간 축)에 자주 사용돼요.\r\n",
    "    * **입력 형태:** `(batch_size, in_channels, length)`\r\n",
    "    * **출력 형태:** `(batch_size, out_channels, new_length)`\r\n",
    "    * **`new_length` 계산 공식:**\r\n",
    "        $\\text{new\\_length} = \\lfloor \\frac{\\text{length} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1 \\rfloor$\r\n",
    "    * **예시:** `nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)`\r\n",
    "        * 입력이 `(4, 128, 500)`이면 (batch\\_size, 멜 밴드 수, 시간 스텝)\r\n",
    "        * `out_channels`는 `256`이 됩니다.\r\n",
    "        * `new_length`는 $\\lfloor \\frac{500 + 2 \\times 1 - 1 \\times (3 - 1) - 1}{2} + 1 \\rfloor = \\lfloor \\frac{500 + 2 - 2 - 1}{2} + 1 \\rfloor = \\lfloor \\frac{499}{2} + 1 \\rfloor = \\lfloor 249.5 + 1 \\rfloor = 249 + 1 = 250$\r\n",
    "        * 출력은 `(4, 256, 250)`\r\n",
    "\r\n",
    "3.  **`nn.Conv2d` (2D 컨볼루션 레이어):**\r\n",
    "    * 이미지 처리나 멜 스펙트로그램 같은 2D 데이터에 사용돼요.\r\n",
    "    * **입력 형태:** `(batch_size, in_channels, height, width)`\r\n",
    "    * **출력 형태:** `(batch_size, out_channels, new_height, new_width)`\r\n",
    "    * **`new_height` 계산 공식:**\r\n",
    "        $\\text{new\\_height} = \\lfloor \\frac{\\text{height} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1 \\rfloor$\r\n",
    "    * **`new_width` 계산 공식:**\r\n",
    "        $\\text{new\\_width} = \\lfloor \\frac{\\text{width} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1 \\rfloor$\r\n",
    "    * `kernel_size`, `stride`, `padding`, `dilation`이 단일 숫자면 높이/너비에 동일하게 적용되고, 튜플이면 각각 적용돼요.\r\n",
    "\r\n",
    "4.  **`nn.MaxPool1d` / `nn.AvgPool1d` (풀링 레이어):**\r\n",
    "    * 컨볼루션과 유사하게 길이를 줄여요.\r\n",
    "    * **입력 형태:** `(batch_size, in_channels, length)`\r\n",
    "    * **출력 형태:** `(batch_size, in_channels, new_length)`\r\n",
    "    * **`new_length` 계산 공식:** 컨볼루션과 동일한 공식에 `kernel_size`, `stride`, `padding`, `dilation` 값을 대입해요.\r\n",
    "\r\n",
    "5.  **`nn.RNN`, `nn.LSTM`, `nn.GRU` (RNN 계열):**\r\n",
    "    * **입력 형태:** `(seq_len, batch_size, input_size)` 또는 `(batch_size, seq_len, input_size)` ( `batch_first=True`일 경우)\r\n",
    "    * **출력 형태:** `(seq_len, batch_size, hidden_size * num_directions)` 또는 `(batch_size, seq_len, hidden_size * num_directions)`\r\n",
    "    * `hidden_size`는 RNN 셀의 은닉 상태 크기, `num_directions`는 양방향(bidirectional)일 경우 2, 단방향일 경우 1이에요.\r\n",
    "\r\n",
    "6.  **`nn.TransformerEncoderLayer`, `nn.TransformerDecoderLayer`:**\r\n",
    "    * 내부적으로 복잡한 어텐션과 피드포워드 네트워크를 거치지만, **기본적으로 입력 시퀀스 길이와 특징 차원(`d_model`)은 유지**돼요.\r\n",
    "    * **입력 형태:** `(seq_len, batch_size, d_model)` 또는 `(batch_size, seq_len, d_model)`\r\n",
    "    * **출력 형태:** 입력 형태와 **동일**해요. (Multi-Head Attention 등의 내부 연산 후에도 최종 출력 형태는 유지)\r\n",
    "\r\n",
    "7.  **`nn.Flatten`:**\r\n",
    "    * **입력 형태:** `(batch_size, C, H, W)` 또는 `(batch_size, H, W)` 등\r\n",
    "    * **출력 형태:** `(batch_size, C * H * W)` 또는 `(batch_size, H * W)` 등 (배치 차원을 제외한 나머지 차원들을 일렬로 펼침)\r\n",
    "\r\n",
    "### 출력 형태 계산 방법:\r\n",
    "\r\n",
    "1.  **손으로 직접 계산:**\r\n",
    "    각 레이어의 공식과 현재 텐서의 형태를 바탕으로 순서대로 계산해나가요. 이는 모델의 개념을 이해하는 데 가장 좋은 방법이에요.\r\n",
    "\r\n",
    "2.  **더미 데이터(Dummy Data)를 이용한 확인:**\r\n",
    "    가장 빠르고 정확한 방법이에요. 실제 모델에 들어갈 것과 동일한 형태의 임의의 텐서(더미 데이터)를 만들어서 모델의 `forward` 메서드를 통과시켜보면 돼요.\r\n",
    "\r\n",
    "    ```python\r\n",
    "    import torch\r\n",
    "    import torch.nn as nn\r\n",
    "\r\n",
    "    # 예시 모델\r\n",
    "    class MyModel(nn.Module):\r\n",
    "        def __init__(self):\r\n",
    "            super().__init__()\r\n",
    "            self.conv1 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1)\r\n",
    "            self.relu1 = nn.ReLU()\r\n",
    "            self.conv2 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=2, padding=1)\r\n",
    "            self.relu2 = nn.ReLU()\r\n",
    "            # conv2 이후의 출력 형태를 알 수 없다고 가정\r\n",
    "            # self.flatten = nn.Flatten() # 만약 Flatten을 쓴다면\r\n",
    "            # self.linear = nn.Linear(?, 10) # ?를 찾아야 함\r\n",
    "\r\n",
    "        def forward(self, x):\r\n",
    "            x = self.relu1(self.conv1(x))\r\n",
    "            x = self.relu2(self.conv2(x))\r\n",
    "            print(f\"After conv2: {x.shape}\") # 중간 출력 형태 확인\r\n",
    "            # x = self.flatten(x)\r\n",
    "            # x = self.linear(x)\r\n",
    "            return x\r\n",
    "\r\n",
    "    # 더미 입력 생성 (예: batch_size=4, n_mels=128, time_steps=500)\r\n",
    "    dummy_input = torch.randn(4, 128, 500)\r\n",
    "\r\n",
    "    model = MyModel()\r\n",
    "    output = model(dummy_input)\r\n",
    "    print(f\"Final output shape: {output.shape}\")\r\n",
    "    ```\r\n",
    "\r\n",
    "3.  **라이브러리 활용 (`torchsummary`, `torchinfo`):**\r\n",
    "    모델의 요약 정보를 출력해주는 유용한 라이브러리들이 있어요. 이들은 각 레이어별 입출력 형태와 파라미터 수를 자동으로 계산해서 보여줘요.\r\n",
    "\r\n",
    "    * **`torchsummary`:** (설치: `pip install torchsummary`)\r\n",
    "        ```python\r\n",
    "        from torchsummary import summary\r\n",
    "\r\n",
    "        # 위 MyModel 정의 후\r\n",
    "        model = MyModel()\r\n",
    "        summary(model, input_size=(128, 500)) # (in_channels, length)\r\n",
    "        ```\r\n",
    "    * **`torchinfo`:** (설치: `pip install torchinfo`)\r\n",
    "        ```python\r\n",
    "        from torchinfo import summary\r\n",
    "\r\n",
    "        # 위 MyModel 정의 후\r\n",
    "        model = MyModel()\r\n",
    "        summary(model, input_size=(4, 128, 500)) # (batch_size, in_channels, length)\r\n",
    "        ```\r\n",
    "\r\n",
    "    이 라이브러리들을 사용하면 복잡한 모델의 형태를 한눈에 파악할 수 있어서 디버깅 및 설계에 큰 도움이 돼요.\r\n",
    "\r\n",
    "모델의 출력 형태를 정확히 아는 것은 다음 레이어의 입력 형태로 연결하거나, 최종 출력 레이어의 크기를 설정하는 등 모델 설계의 필수적인 단계입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300c429-dbd7-49d5-ab8f-99ef5c4df6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f419d547-e875-409f-b81c-a4bd0f63102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers  import AutoProcessor,AutoModelForCTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e926f74-3054-4030-a331-3252d39d0c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. 사전 학습된 ASR 모델 로드 (Wav2Vec2)\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForCTC.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "619200e9-29d7-4d96-b1a7-cc3b82867995",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_size=30\n",
    "model.lm_head=nn.Linear(model.config.hidden_size,new_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e5f650e-eade-4ccd-87e6-6b95c6ccde17",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimy= torch.optim.AdamW(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37dd2eee-01a1-4f72-8529-30ea3a366967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 파인튜닝 설정 완료 ---\n",
      "Layer: wav2vec2.masked_spec_embed, requires_grad: True\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.0.conv.weight, requires_grad: True\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.1.conv.weight, requires_grad: True\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.2.conv.weight, requires_grad: True\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.3.conv.weight, requires_grad: True\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.4.conv.weight, requires_grad: True\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.5.conv.weight, requires_grad: True\n",
      "Layer: wav2vec2.feature_extractor.conv_layers.6.conv.weight, requires_grad: True\n",
      "Layer: wav2vec2.feature_projection.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.feature_projection.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.feature_projection.projection.weight, requires_grad: True\n",
      "Layer: wav2vec2.feature_projection.projection.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.pos_conv_embed.conv.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0, requires_grad: True\n",
      "Layer: wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.0.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.1.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.2.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.3.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.4.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.5.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.6.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.7.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.8.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.9.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.10.final_layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.attention.k_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.attention.k_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.attention.v_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.attention.v_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.attention.q_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.attention.q_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.attention.out_proj.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.attention.out_proj.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.layer_norm.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.feed_forward.output_dense.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.feed_forward.output_dense.bias, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.final_layer_norm.weight, requires_grad: True\n",
      "Layer: wav2vec2.encoder.layers.11.final_layer_norm.bias, requires_grad: True\n",
      "Layer: lm_head.weight, requires_grad: True\n",
      "Layer: lm_head.bias, requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 파인튜닝 설정 완료 ---\")\n",
    "for name, param in model.named_parameters():\n",
    "    # 학습 가능한 파라미터만 출력 (대부분 True일 것)\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b02c1e-b852-4107-812a-552aaa29fb29",
   "metadata": {},
   "source": [
    "네, 코드상에서 **전이 학습**과 **파인튜닝**의 구현 차이점은 주로 **모델의 어떤 부분(레이어)을 학습시킬 것인가**에 따라 달라집니다. PyTorch에서는 이를 **각 레이어의 `requires_grad` 속성**을 조절함으로써 구현합니다.\n",
    "\n",
    "### 1. 전이 학습 (Pre-trained Model as Feature Extractor) 방식의 코드 구현\n",
    "\n",
    "이 방식은 사전 학습된 모델의 **대부분을 고정(Freeze)시키고**, 주로 **마지막 출력 레이어만 새로 만들어서 학습**시키는 방법입니다. 마치 사전 학습 모델이 데이터에서 고수준의 특징(feature)을 뽑아주는 역할만 하고, 이 특징을 받아서 새로운 작업을 수행하는 작은 분류기(classifier)만 학습하는 것과 같습니다.\n",
    "\n",
    "**코드 구현의 핵심:**\n",
    "* 사전 학습된 모델의 모든 `parameters()`에 대해 `requires_grad = False`로 설정하여 **기울기 계산을 비활성화**하고, 따라서 **가중치 업데이트가 일어나지 않도록** 합니다.\n",
    "* 모델의 **마지막 출력 레이어**(`nn.Linear` 등)를 새로운 작업(새로운 클래스 수)에 맞게 **재정의**합니다. 이 새로 정의된 레이어는 기본적으로 `requires_grad = True`이므로 학습이 진행됩니다.\n",
    "* 옵티마이저를 정의할 때, **`requires_grad = True`인 파라미터만 전달**합니다.\n",
    "\n",
    "**예시 코드 (컴퓨터 비전 모델 ResNet18을 예시로, ASR도 유사하게 적용 가능):**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models # 비전 모델 예시 (ASR도 유사)\n",
    "\n",
    "# 1. 사전 학습된 모델 로드 (예: ImageNet으로 학습된 ResNet18)\n",
    "# ASR에서는 Wav2Vec2, Whisper 등 사전 학습된 ASR 모델을 로드합니다.\n",
    "model = models.resnet18(weights='DEFAULT') # weights=ResNet18_Weights.IMAGENET1K_V1\n",
    "\n",
    "# 2. 모든 파라미터 고정 (Freeze)\n",
    "# 이 부분이 '전이 학습'의 특징을 가장 잘 나타냅니다.\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False # 기울기 계산 비활성화 -> 가중치 업데이트 안 됨\n",
    "\n",
    "# 3. 마지막 출력 레이어만 새로운 작업에 맞게 수정\n",
    "# ResNet18의 마지막 fully connected layer는 'fc'라는 이름으로 접근 가능.\n",
    "# ASR 모델의 경우, 마지막 ASR 헤드(예: `lm_head` 또는 `decoder_output`)를 변경합니다.\n",
    "num_ftrs = model.fc.in_features # 기존 마지막 레이어의 입력 피처 수\n",
    "num_classes = 10 # 새로운 분류 작업의 클래스 수 (예: 10개 음성 명령)\n",
    "model.fc = nn.Linear(num_ftrs, num_classes) # 새로운 레이어는 기본적으로 requires_grad=True\n",
    "\n",
    "# 4. 옵티마이저 정의\n",
    "# 새로 추가된 (학습 가능한) 파라미터만 옵티마이저에 전달.\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001) # model.fc.parameters()만 학습\n",
    "\n",
    "# 5. 학습 루프 (생략)\n",
    "# 이 모델을 가지고 새로운 데이터셋으로 학습을 진행하면,\n",
    "# model.fc 레이어만 업데이트되고, 나머지 사전 학습된 레이어의 가중치는 고정됩니다.\n",
    "\n",
    "print(\"--- 전이 학습 (Feature Extractor) 설정 완료 ---\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name}, requires_grad: {param.requires_grad}\")\n",
    "```\n",
    "\n",
    "### 2. 파인튜닝 (Fine-tuning) 방식의 코드 구현\n",
    "\n",
    "파인튜닝은 사전 학습된 모델의 **모든 또는 대부분의 레이어를 새로운 데이터셋에 맞춰 미세 조정**하는 방식입니다. 초기 가중치로 사전 학습된 가중치를 사용하되, 이를 새로운 데이터에 최적화하도록 학습합니다.\n",
    "\n",
    "**코드 구현의 핵심:**\n",
    "* 사전 학습된 모델을 로드한 후, **모든 파라미터 또는 대부분의 파라미터에 대해 `requires_grad = True` 상태를 유지**합니다. (기본값이 `True`이므로 특별히 건드리지 않아도 됩니다.)\n",
    "* 필요하다면 (클래스 개수가 다르다면) 마지막 출력 레이어만 수정합니다.\n",
    "* **학습률(Learning Rate)을 매우 작게 설정**합니다 (예: $10^{-5}$ 또는 $10^{-6}$). 이는 사전 학습된 지식을 급격히 훼손하는 것을 방지하고, 미세한 조정이 이루어지도록 합니다.\n",
    "* 때로는 **차등 학습률(Discriminative Learning Rates)**을 사용하여, 초기 레이어는 더 작은 학습률로, 후반부 레이어는 조금 더 큰 학습률로 학습하기도 합니다.\n",
    "\n",
    "**예시 코드 (ASR 모델 Wav2Vec2를 예시로):**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, AutoModelForCTC # Hugging Face Transformers\n",
    "\n",
    "# 1. 사전 학습된 ASR 모델 로드 (Wav2Vec2)\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForCTC.from_pretrained(model_name)\n",
    "\n",
    "# 2. 마지막 출력 레이어 수정 (필요하다면)\n",
    "# Wav2Vec2의 경우, `lm_head`가 최종 분류기 역할\n",
    "new_vocab_size = 30 # 예: 한국어 음성 인식을 위한 새로운 토큰(문자) 개수\n",
    "model.lm_head = nn.Linear(model.config.hidden_size, new_vocab_size)\n",
    "\n",
    "# 3. 모든 파라미터(또는 대부분)가 학습 가능하도록 설정\n",
    "# Hugging Face 모델은 기본적으로 모든 파라미터가 requires_grad=True로 로드됩니다.\n",
    "# 따라서 이 부분은 별도로 코드를 작성할 필요가 없습니다.\n",
    "# 만약 특정 레이어를 고정하고 싶다면 for param in model.some_layer.parameters(): param.requires_grad = False 처럼 설정합니다.\n",
    "\n",
    "# 4. 옵티마이저 정의\n",
    "# 모든 학습 가능한 파라미터를 옵티마이저에 전달합니다.\n",
    "# **파인튜닝의 핵심: 낮은 학습률**\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) # 매우 낮은 학습률\n",
    "\n",
    "# 5. 학습 루프 (생략)\n",
    "# 이 모델을 가지고 새로운 데이터셋으로 학습을 진행하면,\n",
    "# 모든 레이어의 가중치가 낮은 학습률로 미세 조정됩니다.\n",
    "\n",
    "print(\"\\n--- 파인튜닝 설정 완료 ---\")\n",
    "for name, param in model.named_parameters():\n",
    "    # 학습 가능한 파라미터만 출력 (대부분 True일 것)\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name}, requires_grad: {param.requires_grad}\")\n",
    "\n",
    "```\n",
    "\n",
    "### 핵심 차이점 정리 (코드 관점):\n",
    "\n",
    "* **`param.requires_grad = False`**: 특정 레이어의 가중치를 **고정(Freeze)**시켜 학습되지 않게 합니다. (전이 학습 - 특징 추출기 방식에서 주로 사용)\n",
    "* **옵티마이저에 전달하는 파라미터**: `optimizer = torch.optim.Adam(model.parameters(), ...)`에서 `model.parameters()` 대신 `model.fc.parameters()`와 같이 **일부만 전달**하면 그 부분만 학습됩니다. (`requires_grad=False`인 파라미터는 어차피 기울기가 계산되지 않아 학습에 포함되지 않습니다.)\n",
    "* **학습률(Learning Rate)**: 파인튜닝 시에는 일반적으로 **매우 낮은 학습률**을 사용합니다.\n",
    "\n",
    "이러한 코드상의 차이점을 이해하고 활용하는 것이 전이 학습과 파인튜닝을 성공적으로 적용하는 핵심입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5042ae0-a3b8-4e96-95be-4e3080d35e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Environment",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
