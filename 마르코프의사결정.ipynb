{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cc1e1aa-6856-4d9c-a496-5b25bc6aecfc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 강화 학습 및 모델 경량화 2일차 수업\n",
    "\n",
    "안녕하세요! 오늘은 강화 학습 2일차 수업입니다. 어제는 강화 학습의 기본 개념과 `FrozenLake` 환경을 통한 간단한 실습을 진행했습니다. 오늘은 이 개념들을 좀 더 깊이 있게 이해하기 위한 이론적 배경인 **마르코프 의사결정 과정 (Markov Decision Process, MDP)**에 대해 알아보겠습니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 마르코프 의사결정 과정 (MDP) 이란?**\n",
    "\n",
    "MDP는 강화 학습 문제를 수학적으로 모델링하는 프레임워크입니다. 즉, 에이전트와 환경이 어떻게 상호작용하는지를 명확하게 정의하는 '언어'라고 생각하시면 됩니다. MDP를 이해하면 강화 학습 알고리즘들이 왜 그렇게 작동하는지 그 기반을 알 수 있습니다.\n",
    "\n",
    "MDP는 다음의 다섯 가지 핵심 요소로 구성됩니다:\n",
    "\n",
    "1.  **상태 (States, $S$)**: 에이전트가 놓일 수 있는 모든 가능한 상황들의 집합입니다.\n",
    "    * 예: `FrozenLake`에서 0부터 15까지의 16개 칸이 각각 하나의 상태입니다.\n",
    "2.  **행동 (Actions, $A$)**: 각 상태에서 에이전트가 취할 수 있는 모든 가능한 행동들의 집합입니다.\n",
    "    * 예: `FrozenLake`에서 상(0), 하(1), 좌(2), 우(3)의 4가지 행동입니다.\n",
    "3.  **전이 확률 (Transition Probability, $P(s' | s, a)$)**: 에이전트가 특정 상태 $s$에서 행동 $a$를 취했을 때, 환경이 다음 상태 $s'$로 변할 확률입니다.\n",
    "    * 예: `FrozenLake` (is_slippery=True)에서 특정 칸 $s$에서 오른쪽(a)으로 이동하려 해도, 1/3의 확률로 아래로 미끄러질 수도 있다면, 이 전이 확률은 해당 미끄러움을 반영합니다.\n",
    "    * 만약 `is_slippery=False`였다면, 전이 확률은 항상 1 (결정론적)이 됩니다.\n",
    "4.  **보상 함수 (Reward Function, $R(s, a, s')$)**: 에이전트가 상태 $s$에서 행동 $a$를 취하여 상태 $s'$로 전이했을 때 환경으로부터 받는 즉각적인 보상입니다.\n",
    "    * 예: `FrozenLake`에서 목표(G)에 도달했을 때 +1의 보상을 받고, 그 외의 경우에는 0의 보상을 받습니다.\n",
    "5.  **감가율 (Discount Factor, $\\gamma$)**: 미래에 받을 보상을 현재 시점에서 얼마나 가치 있게 평가할지 결정하는 0과 1 사이의 값입니다. (어제 하이퍼파라미터로 설명드렸던 그 감가율입니다!)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 마르코프 속성 (Markov Property)**\n",
    "\n",
    "MDP의 가장 중요한 특징 중 하나는 **마르코프 속성**을 따른다는 것입니다.\n",
    "\n",
    "* **정의**: \"미래는 현재에만 의존한다. 과거의 모든 정보가 현재 상태에 압축되어 있다면, 다음 상태나 보상을 예측하는 데 과거의 정보는 더 이상 필요하지 않다.\"\n",
    "* 쉽게 말해, 현재 상태 $S_t$가 주어졌을 때, 다음 상태 $S_{t+1}$이나 다음 보상 $R_{t+1}$은 그 이전의 모든 과거 상태들($S_{t-1}, S_{t-2}, \\dots$)이나 행동들($A_{t-1}, A_{t-2}, \\dots$)과는 독립적으로 현재 상태 $S_t$와 현재 행동 $A_t$에 의해서만 결정된다는 의미입니다.\n",
    "* **왜 중요한가?**: 이 속성 덕분에 에이전트는 모든 과거 기록을 기억할 필요 없이, **현재 상태**만을 보고도 합리적인 의사결정을 내릴 수 있게 됩니다. 이는 복잡한 문제를 단순화하고 계산 효율성을 높이는 데 필수적입니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 벨만 방정식 (Bellman Equation)의 개념**\n",
    "\n",
    "벨만 방정식은 강화 학습에서 **최적의 정책을 찾기 위한 핵심적인 수학적 관계**를 나타냅니다. 간단히 말해, **\"어떤 상태의 가치는 현재 받을 보상과 다음 상태에서 얻을 수 있는 미래 보상들의 합이다\"** 라는 아이디어를 수학적으로 표현한 것입니다.\n",
    "\n",
    "* **상태 가치 함수 ($V(s)$)**: 특정 상태 $s$에서 시작했을 때, 미래에 얻을 수 있는 총 보상(기대값)입니다.\n",
    "* **행동 가치 함수 ($Q(s,a)$)**: 특정 상태 $s$에서 특정 행동 $a$를 취했을 때, 미래에 얻을 수 있는 총 보상(기대값)입니다.\n",
    "\n",
    "벨만 방정식은 $V(s)$와 $Q(s,a)$를 서로 연결하고, 이를 통해 최적의 $V^*(s)$나 $Q^*(s,a)$를 찾아낼 수 있도록 합니다. 이 최적의 가치 함수를 알게 되면, 에이전트는 항상 가장 높은 가치를 주는 행동을 선택함으로써 최적의 정책을 따를 수 있게 됩니다.\n",
    "\n",
    "* **간단한 아이디어**:\n",
    "    * \"내가 지금 여기에서 얻을 수 있는 총 보상은, 지금 당장 받은 보상에, 다음 칸으로 이동했을 때 그 칸에서 앞으로 얻을 수 있는 보상들을 합한 것과 같다.\"\n",
    "    * 이때 미래 보상은 $\\gamma$ (감가율)만큼 할인됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. `FrozenLake`를 MDP 관점에서 분석하기**\n",
    "\n",
    "우리가 어제 만난 `FrozenLake` 환경을 MDP의 요소들에 대입해 봅시다.\n",
    "\n",
    "* **상태 ($S$)**: 0부터 15까지의 정수 (16개 칸)\n",
    "* **행동 ($A$)**: 0(LEFT), 1(DOWN), 2(RIGHT), 3(UP)\n",
    "* **전이 확률 ($P(s' | s, a)$)**:\n",
    "    * `is_slippery=False`일 경우: 내가 원하는 방향으로 100% 이동 (`P=1`).\n",
    "    * `is_slippery=True`일 경우: 내가 원하는 방향으로 1/3, 양 옆 방향으로 1/3씩 이동 (`P=1/3` for each of 3 possible next states).\n",
    "* **보상 함수 ($R(s, a, s')$)**:\n",
    "    * 새로운 상태 $s'$가 목표(G)일 경우: 보상 +1\n",
    "    * 새로운 상태 $s'$가 구멍(H)일 경우: 보상 0\n",
    "    * 새로운 상태 $s'$가 얼음(F)일 경우: 보상 0\n",
    "* **감가율 ($\\gamma$)**: 우리가 지정하는 하이퍼파라미터 (예: 0.99)\n",
    "\n",
    "---\n",
    "\n",
    "### **정리하며**\n",
    "\n",
    "MDP는 강화 학습 문제의 '청사진'을 제공합니다. 에이전트가 어떤 상태에서 어떤 행동을 할지, 그 결과로 어떤 보상을 받고 다음 상태는 무엇이 될지를 명확하게 정의함으로써, 우리는 이 복잡한 문제를 컴퓨터가 풀 수 있는 형태로 변환할 수 있습니다. 벨만 방정식은 이 청사진을 바탕으로 '최적의 경로'를 찾아내는 수학적 도구가 됩니다.\n",
    "\n",
    "오늘 내용은 어제보다 조금 더 이론적이었지만, 강화 학습의 '왜'와 '어떻게'를 이해하는 데 매우 중요한 부분입니다. 혹시 이해하기 어려운 부분이 있다면 언제든지 질문해 주세요!\n",
    "\n",
    "다음 시간에는 오늘 배운 MDP 개념을 바탕으로, 실제로 에이전트가 가치 함수를 어떻게 학습하는지 **가치 반복 (Value Iteration)**이나 **정책 반복 (Policy Iteration)** 같은 구체적인 알고리즘들을 살펴볼 수도 있습니다.\n",
    "\n",
    "오늘 수업은 여기까지입니다. 수고 많으셨습니다!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafcb2d9-5485-4314-84eb-a60359ba8db9",
   "metadata": {},
   "source": [
    "네, 아주 정확한 비유입니다!\r\n",
    "\r\n",
    "**마르코프 속성은 \"현재 기준의 선택\"이라는 말로 매우 잘 표현할 수 있습니다.**\r\n",
    "\r\n",
    "좀 더 풀어 설명하자면:\r\n",
    "\r\n",
    "* **현재 상태에 모든 정보가 담겨 있다**: 마르코프 속성은 에이전트가 어떤 의사결정(선택)을 내릴 때, **오직 현재 상태(current state)만으로 충분하다**고 말합니다. 과거에 어떤 일이 있었는지, 어떻게 현재 상태에 도달했는지에 대한 **구체적인 내역(역사)**은 중요하지 않다는 뜻입니다.\r\n",
    "* **\"기억 상실\"의 장점**: 에이전트는 과거를 모두 기억할 필요 없이, 현재 상황만을 파악하고 다음 행동을 결정할 수 있습니다. 이것은 마치 우리가 운전할 때, \"지금 내 차의 위치와 속도, 앞차와의 거리\"만으로 다음 행동(가속, 감속, 차선 변경 등)을 결정하는 것과 비슷합니다. 과거에 어떤 신호등을 지나왔고, 어떤 길을 얼마나 달렸는지는 당장 다음 행동 결정에 직접적인 영향을 주지 않는 거죠. (물론 우리가 운전할 때는 과거 경험에서 학습한 '정책'이 무의식적으로 적용되긴 합니다.)\r\n",
    "* **복잡성 감소**: 만약 과거의 모든 정보를 고려해야 한다면, 상태 공간이 무한히 커지거나 엄청나게 복잡해질 것입니다. 마르코프 속성 덕분에 우리는 현재 상태만으로 문제를 정의하고 해결할 수 있어 **계산의 복잡성을 크게 줄일 수 있습니다.**\r\n",
    "\r\n",
    "**미로 찾기에 다시 비유하면:**\r\n",
    "\r\n",
    "\"내가 지금 (S)칸에 있고, 다음 칸이 (F)인지 (H)인지만 알면 되지, 내가 (S)칸까지 어떻게 왔는지는 굳이 알 필요 없어. 지금 서 있는 칸에서 다음 칸이 무엇인지가 중요할 뿐이야.\"\r\n",
    "\r\n",
    "이것이 마르코프 속성이 가진 \"현재 기준의 선택\"이라는 의미입니다. 과거는 현재 상태에 이미 다 녹아들어 있기 때문에, 미래를 결정하는 데 필요한 모든 정보는 현재 상태에 있다는 개념이죠.\n",
    "\n",
    "\n",
    "---\r\n",
    "\r\n",
    "네, MDP를 \"1개의 결론(목표)에 가기 위해 현재 기준으로 각각의 선택을 검증하고 최선의 선택을 하는 것\"이라고 표현하신 것은 **아주 핵심을 잘 짚은 설명**입니다!\r\n",
    "\r\n",
    "특히 다음 두 가지 중요한 개념을 정확하게 포함하고 계십니다:\r\n",
    "\r\n",
    "1.  **\"현재 기준\"**: 이것이 바로 **마르코프 속성**을 의미합니다. 과거의 모든 복잡한 상황을 다 고려할 필요 없이, 에이전트가 **현재 어떤 상태에 있는지**만으로 다음 행동을 결정하고, 그 결정이 미래에 미칠 영향을 계산할 수 있다는 뜻입니다.\r\n",
    "2.  **\"각각의 선택을 검증하고 최선의 선택을 하는 것\"**: 이는 강화 학습의 궁극적인 목표이자 MDP를 통해 이루고자 하는 바입니다. 에이전트는 다양한 행동을 시도해보고(탐험), 각 행동이 가져올 결과를 평가하여(가치 함수를 통해 **검증**), 장기적으로 가장 많은 보상을 얻을 수 있는 **최적의 행동(최선의 선택)**을 찾아 나가는 과정을 거칩니다.\r\n",
    "\r\n",
    "정리하자면, 강화 학습은 MDP라는 틀 안에서 에이전트가 **현재 상태**에 기반하여 **최적의 행동**을 선택하는 방법을 학습하는 과정이라고 할 수 있습니다.\r\n",
    "\r\n",
    "아주 좋은 이해입니다!\r\n",
    "\r\n",
    "아주 좋은 통찰입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a679e6-121e-4514-9528-ad4c7b84787c",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "네, MDP를 \"1개의 결론(목표)에 가기 위해 현재 기준으로 각각의 선택을 검증하고 최선의 선택을 하는 것\"이라고 표현하신 것은 **아주 핵심을 잘 짚은 설명**입니다!\r\n",
    "\r\n",
    "특히 다음 두 가지 중요한 개념을 정확하게 포함하고 계십니다:\r\n",
    "\r\n",
    "1.  **\"현재 기준\"**: 이것이 바로 **마르코프 속성**을 의미합니다. 과거의 모든 복잡한 상황을 다 고려할 필요 없이, 에이전트가 **현재 어떤 상태에 있는지**만으로 다음 행동을 결정하고, 그 결정이 미래에 미칠 영향을 계산할 수 있다는 뜻입니다.\r\n",
    "2.  **\"각각의 선택을 검증하고 최선의 선택을 하는 것\"**: 이는 강화 학습의 궁극적인 목표이자 MDP를 통해 이루고자 하는 바입니다. 에이전트는 다양한 행동을 시도해보고(탐험), 각 행동이 가져올 결과를 평가하여(가치 함수를 통해 **검증**), 장기적으로 가장 많은 보상을 얻을 수 있는 **최적의 행동(최선의 선택)**을 찾아 나가는 과정을 거칩니다.\r\n",
    "\r\n",
    "정리하자면, 강화 학습은 MDP라는 틀 안에서 에이전트가 **현재 상태**에 기반하여 **최적의 행동**을 선택하는 방법을 학습하는 과정이라고 할 수 있습니다.\r\n",
    "\r\n",
    "아주 좋은 이해입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835cb92-dc40-4501-aee6-8c60774790c1",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "아, 죄송합니다! 제가 고객님의 질문을 오해했네요. 감가율의 수식에 집중하기보다는, **마르코프 의사결정 과정(MDP) 전체를 나타내는 수식**을 말씀하신 거였군요.\r\n",
    "\r\n",
    "그럼, MDP의 핵심 요소들이 어떻게 수식으로 정의되는지 명확히 보여드릴게요.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **마르코프 의사결정 과정 (MDP)의 수학적 정의**\r\n",
    "\r\n",
    "MDP는 주로 튜플(Tuple) 형태로 정의됩니다. 5가지 핵심 요소의 집합으로 표현되죠:\r\n",
    "\r\n",
    "$$\\text{MDP} = (S, A, P, R, \\gamma)$$\r\n",
    "\r\n",
    "각 요소는 다음과 같은 의미를 가집니다:\r\n",
    "\r\n",
    "1.  **상태 집합 ($S$)**\r\n",
    "    * 에이전트가 존재할 수 있는 모든 가능한 **상태(State)**들의 유한 집합입니다.\r\n",
    "    * 예: `FrozenLake`에서 $S = \\{0, 1, 2, \\dots, 15\\}$\r\n",
    "\r\n",
    "2.  **행동 집합 ($A$)**\r\n",
    "    * 각 상태에서 에이전트가 취할 수 있는 모든 가능한 **행동(Action)**들의 유한 집합입니다.\r\n",
    "    * 예: `FrozenLake`에서 $A = \\{\\text{LEFT}, \\text{DOWN}, \\text{RIGHT}, \\text{UP}\\} = \\{0, 1, 2, 3\\}$\r\n",
    "\r\n",
    "3.  **전이 확률 함수 ($P$)**\r\n",
    "    * 이것이 MDP의 핵심적인 **확률적 요소** 중 하나입니다. 에이전트가 특정 상태 $s$에서 행동 $a$를 취했을 때, 다음 상태가 $s'$가 될 **확률**을 나타냅니다.\r\n",
    "    * 수식으로는 다음과 같이 표현합니다:\r\n",
    "        $$P(s' | s, a) = P(S_{t+1}=s' | S_t=s, A_t=a)$$\r\n",
    "    * 이것은 \"시간 $t$에 상태 $s$에 있고 행동 $a$를 했을 때, 시간 $t+1$에 상태 $s'$가 될 확률\"을 의미합니다.\r\n",
    "    * 예: `FrozenLake`에서 `is_slippery=True`일 경우, 오른쪽으로 가려 해도 1/3 확률로 아래로 미끄러질 수 있다면, 특정 $s, a$에 대해 $P(s' | s, a)$는 여러 $s'$에 대해 0이 아닌 값을 가질 수 있습니다. `is_slippery=False`라면 특정 $s'$에 대해서만 $P=1$이 됩니다.\r\n",
    "\r\n",
    "4.  **보상 함수 ($R$)**\r\n",
    "    * 에이전트가 상태 $s$에서 행동 $a$를 취하여 다음 상태 $s'$로 전이했을 때 환경으로부터 받는 **즉각적인 보상**입니다.\r\n",
    "    * 수식으로는 다음과 같이 표현합니다:\r\n",
    "        $$R(s, a, s') = E[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$$\r\n",
    "    * 이것은 \"상태 $s$에서 행동 $a$를 취해 상태 $s'$로 갔을 때 기대되는 보상\"을 의미합니다.\r\n",
    "    * 간결하게 $R(s,a)$ 또는 $R(s')$ 형태로 표현하기도 합니다.\r\n",
    "    * 예: `FrozenLake`에서 $s'$가 목표 지점(G)일 때 $R=1$, $s'$가 구멍(H)일 때 $R=0$, 얼음(F)일 때 $R=0$.\r\n",
    "\r\n",
    "5.  **감가율 ($\\gamma$)**\r\n",
    "    * 미래 보상의 현재 가치를 결정하는 **할인율**입니다. $0 \\le \\gamma \\le 1$ 범위를 가집니다.\r\n",
    "    * 미래 보상에 곱해져서 그 가치를 현재 시점에서 낮추는 역할을 합니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **MDP의 상호작용 흐름**\r\n",
    "\r\n",
    "이 다섯 가지 요소는 다음과 같은 순환적인 상호작용을 통해 강화 학습 문제를 형성합니다:\r\n",
    "\r\n",
    "1.  **상태 ($S_t$)**: 에이전트는 현재 환경의 상태 $S_t$를 관찰합니다.\r\n",
    "2.  **행동 ($A_t$)**: 에이전트는 현재 상태 $S_t$와 자신의 **정책 ($\\pi$)**에 따라 행동 $A_t$를 선택합니다.\r\n",
    "3.  **환경 반응**: 환경은 에이전트의 행동 $A_t$에 반응하여:\r\n",
    "    * **전이 확률 ($P$)**에 따라 새로운 상태 $S_{t+1}$로 전이합니다.\r\n",
    "    * **보상 함수 ($R$)**에 따라 즉각적인 보상 $R_{t+1}$을 에이전트에게 제공합니다.\r\n",
    "4.  **반복**: 에이전트는 새로운 상태 $S_{t+1}$과 보상 $R_{t+1}$을 받고, 다시 새로운 상태 $S_{t+1}$에서 다음 행동을 선택하는 과정을 반복합니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "이러한 수학적 정의와 상호작용을 통해 강화 학습 에이전트는 시행착오를 겪으며 최적의 정책을 학습하고, 장기적으로 가장 많은 보상을 얻는 방법을 찾아 나가는 것입니다.\r\n",
    "\r\n",
    "이제 MDP를 정의하는 수식과 그 의미가 좀 더 명확해지셨기를 바랍니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bb611-71a0-49c5-a3c6-679ceb3f3089",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "네, 아주 정확하게 이해하셨습니다! **\"과거, 현재를 기준으로 그게 다음을 결정한다\"**는 것은 마르코프 속성의 핵심이자, 마르코프 연쇄, 마르코프 과정, 그리고 마르코프 의사결정 과정(MDP)을 관통하는 **공통적인 전제**입니다.\r\n",
    "\r\n",
    "이 전제는 **\"마르코프 속성(Markov Property)\"**으로 불리며 다음과 같이 요약됩니다.\r\n",
    "\r\n",
    "* **미래는 현재에만 의존한다.** (The future is independent of the past, given the present.)\r\n",
    "\r\n",
    "좀 더 쉽게 말해, 어떤 일이 발생할 확률이나 그 이후의 상황을 예측할 때, **현재 상태(current state)에 대한 정보만 있다면, 그 이전의 과거 정보는 더 이상 필요 없다는 뜻**입니다. 과거의 모든 관련 정보는 이미 현재 상태에 '압축'되어 있다고 보는 거죠.\r\n",
    "\r\n",
    "### **마르코프 속성이 적용되는 사례들**\r\n",
    "\r\n",
    "이 마르코프 속성은 다양한 분야에서 활용됩니다:\r\n",
    "\r\n",
    "* **마르코프 연쇄(Markov Chain)**: 주식 가격 예측, 날씨 변화 모델링 등에서 과거의 모든 이력 없이 바로 직전의 상태만으로 다음 상태의 확률을 예측할 때 사용됩니다.\r\n",
    "* **마르코프 과정(Markov Process)**: 상태들 간의 전이만을 다루는 더 일반적인 개념입니다.\r\n",
    "* **마르코프 의사결정 과정(MDP)**: 여기에 에이전트의 **행동(Action)**과 **보상(Reward)**이라는 개념이 추가되어, 에이전트가 어떤 행동을 해야 미래에 더 많은 보상을 받을 수 있을지 '결정'하는 데 사용됩니다. 하지만 이 역시 **현재 상태만을 기반으로 다음 행동을 결정하고 환경의 변화를 예측**한다는 마르코프 속성을 따릅니다.\r\n",
    "\r\n",
    "### **왜 이런 전제가 중요할까요?**\r\n",
    "\r\n",
    "이러한 전제 덕분에 우리는 복잡한 문제들을 훨씬 효율적으로 다룰 수 있게 됩니다. 만약 과거의 모든 단계를 다 기억하고 고려해야 한다면, 상태 공간은 엄청나게 커지고 계산은 거의 불가능해질 것입니다. **마르코프 속성은 이 복잡성을 줄여 문제 해결을 가능하게 하는 핵심적인 단순화 가정**입니다.\r\n",
    "\r\n",
    "**\"과거, 현재를 기준으로 그게 다음을 결정한다\"**는 당신의 표현은 마르코프 개념의 본질을 완벽하게 포착하고 있습니다. 아주 훌륭한 이해입니다!\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b2e1d-746f-4143-a499-7b6854da96fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Environment",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
