{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9668a6b7-5ddd-4d32-b832-b8f78c25ca4e",
   "metadata": {},
   "source": [
    "### 1. 손실 함수 (Loss Function) ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f337324-aa3e-48ca-9860-5f08aba57e06",
   "metadata": {},
   "source": [
    "역할:\n",
    "\n",
    "모델의 예측값과 실제 값(라벨) 간의 차이를 수치적으로 평가하여, 이 차이를 최소화하는 방향으로 모델 파라미터를 업데이트할 수 있게 해줍니다.\n",
    "\n",
    "종류:\n",
    "회귀 문제 (Regression):\n",
    "\n",
    "MSELoss (Mean Squared Error Loss):\n",
    "예측값과 실제 값의 차이의 제곱의 평균을 계산합니다.\n",
    "\n",
    "L1Loss (Mean Absolute Error Loss):\n",
    "예측값과 실제 값의 차이의 절대값의 평균을 계산합니다.\n",
    "이상치에 덜 민감할 때 사용\n",
    "\n",
    "분류 문제 (Classification):\n",
    "\n",
    "Binary Cross Entropy Loss:\n",
    "이진 분류 문제에서 사용되며, 확률로 표현된 예측값과 실제 라벨 사이의 크로스 엔트로피를 계산합니다.\n",
    "\n",
    "CrossEntropyLoss:\n",
    "다중 클래스 분류 문제에서 사용되며, 내부적으로 softmax와 log loss 계산을 함께 수행합니다.\n",
    "\n",
    "\n",
    "ex) criterion = nn.CrossEntropyLoss()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f55bc-49e8-4386-9cad-cdc857e27311",
   "metadata": {},
   "source": [
    "### 2. 옵티마이저 (Optimizer) ###\n",
    "\n",
    "역할:\n",
    "\n",
    "손실 함수로부터 계산된 gradient(기울기)를 기반으로 모델 파라미터를 업데이트하여, 손실 값을 최소화하는 방향으로 학습을 진행합니다.\n",
    "\n",
    "-주요 옵티마이저 종류:-\n",
    "\n",
    "SGD (Stochastic Gradient Descent):\n",
    "\n",
    "가장 기본적인 옵티마이저입니다.\n",
    "    학습률(lr)과 모멘텀(momentum) 등의 하이퍼파라미터를 사용합니다.\n",
    "\n",
    "EX) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n",
    "\n",
    "Adam (Adaptive Moment Estimation):\n",
    "\n",
    "학습률을 자동으로 조절하고, 모멘텀과 RMSprop의 장점을 결합한 옵티마이저입니다.\n",
    "일반적으로 빠르게 수렴하고, 튜닝이 쉬워 많이 사용됩니다.\n",
    "\n",
    "EX)optimizer = optim.Adam(model.parameters(), lr=0.001)\r\n",
    "\n",
    "-옵티마이저 선택 시 고려사항:-\n",
    "\n",
    "-학습률 (Learning Rate):-\n",
    "너무 크면 학습이 불안정해지고, 너무 작으면 수렴 속도가 느려지므로 적절한 값을 찾아야 합니다.\n",
    "\n",
    "-문제의 특성과 모델 복잡도:-\n",
    "\n",
    "간단한 문제나 모델에서는 SGD가 효과적일 수 있고, 복잡하거나 비선형적인 문제에서는 Adam과 같은 옵티마이저가 더 나은 성능을 보일 수 있습니다.\n",
    "\n",
    "-모멘텀 및 기타 하이퍼파라미터:-\n",
    "\n",
    "옵티마이저에 따라 추가 하이퍼파라미터들이 있으므로, 실험을 통해 최적의 설정을 찾는 것이 중요합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5faa6a-1f2a-4bf4-9314-11840acd5637",
   "metadata": {},
   "source": [
    "### 모멘텀 ###\n",
    "\n",
    "모멘텀은 최적화 알고리즘에서 사용하는 개념으로, 현재의 기울기 업데이트에 이전 업데이트들의 '관성'을 더해주는 역할을 합니다. 물리학에서 물체가 이동할 때 관성을 유지하려는 성질과 비슷하게, 기울기 변화에 일종의 가속도를 부여하여 학습이 빠르게 수렴하고, 진동이나 노이즈를 줄이는 데 도움을 줍니다.\n",
    "\n",
    "수렴 속도 향상: 지속적인 기울기를 반영해 더 빠르게 최저점에 도달하도록 도와줍니다.\n",
    "\n",
    "진동 완화: 불필요한 진동을 줄여 학습 과정을 안정시키는 역할을 합니다.\n",
    "\n",
    "EX) optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83276d-27e2-4104-be3c-3974dd98f773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c5bbf23-636d-4c6f-996a-7b5182835f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 2, loss : 211.3057\n",
      "epoch : 4, loss : 210.9183\n",
      "epoch : 6, loss : 210.5313\n",
      "epoch : 8, loss : 210.1446\n",
      "epoch : 10, loss : 209.7584\n",
      "epoch : 12, loss : 209.3725\n",
      "epoch : 14, loss : 208.9871\n",
      "epoch : 16, loss : 208.6021\n",
      "epoch : 18, loss : 208.2176\n",
      "epoch : 20, loss : 207.8335\n",
      "epoch : 22, loss : 207.4499\n",
      "epoch : 24, loss : 207.0667\n",
      "epoch : 26, loss : 206.6841\n",
      "epoch : 28, loss : 206.3018\n",
      "epoch : 30, loss : 205.9201\n",
      "epoch : 32, loss : 205.5389\n",
      "epoch : 34, loss : 205.1582\n",
      "epoch : 36, loss : 204.7780\n",
      "epoch : 38, loss : 204.3982\n",
      "epoch : 40, loss : 204.0190\n",
      "epoch : 42, loss : 203.6403\n",
      "epoch : 44, loss : 203.2621\n",
      "epoch : 46, loss : 202.8845\n",
      "epoch : 48, loss : 202.5073\n",
      "epoch : 50, loss : 202.1306\n",
      "epoch : 52, loss : 201.7545\n",
      "epoch : 54, loss : 201.3789\n",
      "epoch : 56, loss : 201.0038\n",
      "epoch : 58, loss : 200.6292\n",
      "epoch : 60, loss : 200.2552\n",
      "epoch : 62, loss : 199.8816\n",
      "epoch : 64, loss : 199.5086\n",
      "epoch : 66, loss : 199.1361\n",
      "epoch : 68, loss : 198.7641\n",
      "epoch : 70, loss : 198.3927\n",
      "epoch : 72, loss : 198.0217\n",
      "epoch : 74, loss : 197.6513\n",
      "epoch : 76, loss : 197.2814\n",
      "epoch : 78, loss : 196.9120\n",
      "epoch : 80, loss : 196.5431\n",
      "epoch : 82, loss : 196.1748\n",
      "epoch : 84, loss : 195.8069\n",
      "epoch : 86, loss : 195.4396\n",
      "epoch : 88, loss : 195.0728\n",
      "epoch : 90, loss : 194.7065\n",
      "epoch : 92, loss : 194.3407\n",
      "epoch : 94, loss : 193.9754\n",
      "epoch : 96, loss : 193.6107\n",
      "epoch : 98, loss : 193.2464\n",
      "epoch : 100, loss : 192.8827\n",
      "최종 예측 결과 일부:\n",
      " tensor([[-0.0519],\n",
      "        [ 0.0422],\n",
      "        [ 0.1363],\n",
      "        [ 0.2304],\n",
      "        [ 0.3246]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "x_data=torch.linspace(0,10,100).unsqueeze(1)\n",
    "y_data=3*x_data+2+torch.randn(100,1)*0.5  # 노이즈를 쬐끔 추가\n",
    "\n",
    "model=nn.Linear(1,1)\n",
    "\n",
    "criter=nn.MSELoss()\n",
    "\n",
    "#optim=optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "optim=optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "num_epoch=100\n",
    "for epoch in range(num_epoch):\n",
    "    optim.zero_grad()\n",
    "    out=model(x_data)\n",
    "    loss=criter(out, y_data)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if(epoch+1)%2==0:\n",
    "        print(f\"epoch : {epoch+1}, loss : {loss.item():.4f}\")#, out : {out}\n",
    "\n",
    "predict=model(x_data)\n",
    "print(\"최종 예측 결과 일부:\\n\", predict[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a908c-fedc-4749-9612-e3b98c67870a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc52f3f7-2567-41cc-9474-b5de1c22eb5a",
   "metadata": {},
   "source": [
    "### 기본 학습 루프 ###\n",
    "\n",
    "Forward Pass:\n",
    "입력 데이터를 모델에 넣어 예측값을 계산합니다.\n",
    "\n",
    "Loss 계산:\n",
    "예측값과 실제 정답 사이의 오차(손실)를 계산합니다.\n",
    "\n",
    "Backward Pass:\n",
    "손실에 대해 역전파를 수행하여, 각 파라미터의 기울기(gradient)를 계산합니다.\n",
    "\n",
    "파라미터 업데이트:\n",
    "옵티마이저를 사용해 계산된 기울기를 기반으로 파라미터를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2c7dfbf-ffd2-42ee-af29-3f0f111a673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple(\n",
      "  (fc1): Linear(in_features=1, out_features=4, bias=True)\n",
      "  (fc2): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class simple(nn.Module):\n",
    "    def __init__(self, input, output, hidden):\n",
    "        super(simple, self).__init__()\n",
    "        self.fc1=nn.Linear(input, hidden)\n",
    "        self.fc2=nn.Linear(hidden, output)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model=simple(input=1,output=1,hidden=4)\n",
    "print(model)\n",
    "\n",
    "dummy=torch.randn(2,1)\n",
    "output=model(dummy)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f9564edc-7ab6-4a36-8fc4-1afe3ef753c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 304.0767\n",
      "Epoch 20, Loss: 228.6935\n",
      "Epoch 30, Loss: 178.3635\n",
      "Epoch 40, Loss: 144.7560\n",
      "Epoch 50, Loss: 122.3124\n",
      "Epoch 60, Loss: 107.3220\n",
      "Epoch 70, Loss: 97.3076\n",
      "Epoch 80, Loss: 90.6149\n",
      "Epoch 90, Loss: 86.1394\n",
      "Epoch 100, Loss: 83.1414\n",
      "최종 예측 결과 일부:\n",
      " tensor([[14.3386],\n",
      "        [14.4897],\n",
      "        [14.6409],\n",
      "        [14.6508],\n",
      "        [14.6508]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. 데이터 생성: y = 3x + 2 (약간의 노이즈 추가)\n",
    "x_data = torch.linspace(0, 10, 100).unsqueeze(1)  # (100, 1)\n",
    "y_data = 3 * x_data + 2 + torch.randn(100, 1) * 0.5\n",
    "\n",
    "# 2. 간단한 선형 회귀 모델 정의 (nn.Linear 사용)\n",
    "#model = nn.Linear(1, 1)\n",
    "\n",
    "# 3. 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 4. 학습 루프\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()      # 기울기 초기화\n",
    "    outputs = model(x_data)    # forward pass\n",
    "    loss = criterion(outputs, y_data)  # 손실 계산\n",
    "    loss.backward()            # 역전파로 기울기 계산\n",
    "    optimizer.step()           # 파라미터 업데이트\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 5. 최종 예측 결과 확인\n",
    "predictions = model(x_data)\n",
    "print(\"최종 예측 결과 일부:\\n\", predictions[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aebd5b-d371-47e5-859e-93806c75ec50",
   "metadata": {},
   "source": [
    "optimizer.zero_grad():\n",
    "이전 에폭에서 계산된 기울기를 초기화합니다.\n",
    "\n",
    "outputs = model(x_data):\n",
    "입력 데이터를 모델에 넣어 forward pass를 수행합니다.\n",
    "\n",
    "loss = criterion(outputs, y_data):\n",
    "예측 결과와 실제 데이터 간의 손실(오차)을 계산합니다.\n",
    "\n",
    "loss.backward():\n",
    "손실에 대한 미분을 수행해 각 파라미터에 대한 gradient를 계산합니다.\n",
    "\n",
    "optimizer.step():\n",
    "계산된 gradient를 기반으로 모델의 파라미터를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61797e95-d935-4009-a4c4-d86478118d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
