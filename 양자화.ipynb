{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d1687c-dbc1-4c92-b912-c5bafceac649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 학습 시작 (5 에폭)...\n",
      "Epoch [1/5], Batch [100/938], Loss: 0.3509\n",
      "Epoch [1/5], Batch [200/938], Loss: 0.1738\n",
      "Epoch [1/5], Batch [300/938], Loss: 0.1296\n",
      "Epoch [1/5], Batch [400/938], Loss: 0.1133\n",
      "Epoch [1/5], Batch [500/938], Loss: 0.1113\n",
      "Epoch [1/5], Batch [600/938], Loss: 0.0841\n",
      "Epoch [1/5], Batch [700/938], Loss: 0.0895\n",
      "Epoch [1/5], Batch [800/938], Loss: 0.0664\n",
      "Epoch [1/5], Batch [900/938], Loss: 0.0725\n",
      "Epoch [2/5], Batch [100/938], Loss: 0.0689\n",
      "Epoch [2/5], Batch [200/938], Loss: 0.0623\n",
      "Epoch [2/5], Batch [300/938], Loss: 0.0605\n",
      "Epoch [2/5], Batch [400/938], Loss: 0.0690\n",
      "Epoch [2/5], Batch [500/938], Loss: 0.0629\n",
      "Epoch [2/5], Batch [600/938], Loss: 0.0615\n",
      "Epoch [2/5], Batch [700/938], Loss: 0.0672\n",
      "Epoch [2/5], Batch [800/938], Loss: 0.0469\n",
      "Epoch [2/5], Batch [900/938], Loss: 0.0571\n",
      "Epoch [3/5], Batch [100/938], Loss: 0.0405\n",
      "Epoch [3/5], Batch [200/938], Loss: 0.0498\n",
      "Epoch [3/5], Batch [300/938], Loss: 0.0368\n",
      "Epoch [3/5], Batch [400/938], Loss: 0.0371\n",
      "Epoch [3/5], Batch [500/938], Loss: 0.0470\n",
      "Epoch [3/5], Batch [600/938], Loss: 0.0577\n",
      "Epoch [3/5], Batch [700/938], Loss: 0.0585\n",
      "Epoch [3/5], Batch [800/938], Loss: 0.0387\n",
      "Epoch [3/5], Batch [900/938], Loss: 0.0445\n",
      "Epoch [4/5], Batch [100/938], Loss: 0.0376\n",
      "Epoch [4/5], Batch [200/938], Loss: 0.0369\n",
      "Epoch [4/5], Batch [300/938], Loss: 0.0433\n",
      "Epoch [4/5], Batch [400/938], Loss: 0.0433\n",
      "Epoch [4/5], Batch [500/938], Loss: 0.0342\n",
      "Epoch [4/5], Batch [600/938], Loss: 0.0332\n",
      "Epoch [4/5], Batch [700/938], Loss: 0.0425\n",
      "Epoch [4/5], Batch [800/938], Loss: 0.0345\n",
      "Epoch [4/5], Batch [900/938], Loss: 0.0374\n",
      "Epoch [5/5], Batch [100/938], Loss: 0.0327\n",
      "Epoch [5/5], Batch [200/938], Loss: 0.0388\n",
      "Epoch [5/5], Batch [300/938], Loss: 0.0419\n",
      "Epoch [5/5], Batch [400/938], Loss: 0.0330\n",
      "Epoch [5/5], Batch [500/938], Loss: 0.0349\n",
      "Epoch [5/5], Batch [600/938], Loss: 0.0336\n",
      "Epoch [5/5], Batch [700/938], Loss: 0.0278\n",
      "Epoch [5/5], Batch [800/938], Loss: 0.0445\n",
      "Epoch [5/5], Batch [900/938], Loss: 0.0316\n",
      "모델 학습 완료.\n",
      "\n",
      "--- FP32 모델 평가 ---\n",
      "Accuracy: 99.17%\n",
      "FP32 모델 크기: 42.71 MB\n",
      "\n",
      "--- Static Quantization Calibration 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\quantize.py:388: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calibration batch: 0/100\n",
      "  Calibration batch: 20/100\n",
      "  Calibration batch: 40/100\n",
      "  Calibration batch: 60/100\n",
      "  Calibration batch: 80/100\n",
      "Calibration 완료.\n",
      "\n",
      "--- Quantized Static 모델 평가 ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 167\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Quantized Static 모델 평가 ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# 양자화된 모델도 CPU에서 평가합니다.\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m quantized_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_model_static\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# 양자화된 모델 저장 (전체 모델 객체 저장)\u001b[39;00m\n\u001b[0;32m    170\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(quantized_model_static, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet18_mnist_quantized_static.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 97\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, test_loader)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m     96\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 97\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     99\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m, in \u001b[0;36mResNet18_MNIST.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m     42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.ao.quantization import get_default_qconfig, prepare, convert, QConfigMapping\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 1. 하이퍼파라미터 및 설정\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5 # 양자화 전에 모델을 어느 정도 학습시키는 것이 좋습니다.\n",
    "CALIBRATION_BATCHES = 100 # Calibration에 사용할 배치 수 (MNIST 학습 데이터셋에서)\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. ResNet 모델 수정 (MNIST 1채널 입력용)\n",
    "class ResNet18_MNIST(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18_MNIST, self).__init__()\n",
    "        # 사전 학습된 ResNet18을 불러오지만, weights는 로드하지 않습니다.\n",
    "        # ImageNet_V1 Weights를 로드하려면 conv1을 다시 초기화해야 합니다.\n",
    "        original_resnet = resnet18(weights=None)\n",
    "\n",
    "        # conv1 레이어를 1채널 입력에 맞게 수정\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = original_resnet.bn1\n",
    "        self.relu = original_resnet.relu\n",
    "        self.maxpool = original_resnet.maxpool\n",
    "\n",
    "        self.layer1 = original_resnet.layer1\n",
    "        self.layer2 = original_resnet.layer2\n",
    "        self.layer3 = original_resnet.layer3\n",
    "        self.layer4 = original_resnet.layer4\n",
    "\n",
    "        self.avgpool = original_resnet.avgpool\n",
    "        self.fc = nn.Linear(original_resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 3. 데이터 로더 준비 (MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # 이미지를 텐서로 변환\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST 평균, 표준편차로 정규화\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "calibration_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# 4. 모델 학습 (양자화 전에 모델을 학습시킵니다)\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    model.to(DEVICE)\n",
    "    print(f\"\\n모델 학습 시작 ({num_epochs} 에폭)...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99: # 100 배치마다 출력\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "    print(\"모델 학습 완료.\")\n",
    "\n",
    "# 5. 모델 평가 함수\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# 6. 모델 생성 및 초기 학습\n",
    "model_fp32 = ResNet18_MNIST(num_classes=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# GPU가 있다면 모델을 GPU로 이동\n",
    "model_fp32.to(DEVICE)\n",
    "\n",
    "# 학습 실행\n",
    "train_model(model_fp32, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "print(\"\\n--- FP32 모델 평가 ---\")\n",
    "fp32_accuracy = evaluate_model(model_fp32, test_loader)\n",
    "torch.save(model_fp32.state_dict(), \"resnet18_mnist_fp32.pth\")\n",
    "fp32_size = os.path.getsize(\"resnet18_mnist_fp32.pth\") / (1024 * 1024)\n",
    "print(f\"FP32 모델 크기: {fp32_size:.2f} MB\")\n",
    "\n",
    "# 7. Static Quantization 준비\n",
    "# 모델을 CPU로 옮깁니다. PyTorch의 PTQ는 현재 CPU에서 주로 지원됩니다.\n",
    "model_fp32.to(\"cpu\")\n",
    "model_fp32.eval() # 평가 모드 설정\n",
    "\n",
    "# 레이어 퓨징 (성능 향상)\n",
    "# ResNet의 각 Residual Block 내부에 Conv-BN-ReLU 패턴이 있습니다.\n",
    "# torchvision.models._utils.fuse_model 함수를 사용하거나 수동으로 퓨징할 수 있습니다.\n",
    "# 여기서는 간단하게 첫 번째 컨볼루션 레이어만 퓨징합니다.\n",
    "# 실제 ResNet의 모든 Conv-BN-ReLU를 퓨징하려면 더 복잡한 로직이 필요합니다.\n",
    "# https://pytorch.org/docs/stable/quantization.html#module-fusion\n",
    "fused_model = torch.ao.quantization.fuse_modules(model_fp32,\n",
    "                                               [['conv1', 'bn1', 'relu']],\n",
    "                                               inplace=False)\n",
    "\n",
    "# QConfig 설정 (CPU 최적화된 fbgemm 백엔드)\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig) # PyTorch 1.13+\n",
    "\n",
    "# 모델 준비 (양자화 가능한 모듈을 대체하고 Observer 삽입)\n",
    "#prepared_model = prepare(fused_model, qconfig_mapping, inplace=False)\n",
    "prepared_model = prepare(fused_model, qconfig_mapping)\n",
    "\n",
    "\n",
    "\n",
    "# 8. Calibration (보정)\n",
    "print(\"\\n--- Static Quantization Calibration 시작 ---\")\n",
    "# Calibration loader를 통해 모델에 데이터를 흘려보내 활성화 분포를 측정합니다.\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (inputs, labels) in enumerate(calibration_loader):\n",
    "        if batch_idx >= CALIBRATION_BATCHES:\n",
    "            break\n",
    "        prepared_model(inputs)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"  Calibration batch: {batch_idx}/{CALIBRATION_BATCHES}\")\n",
    "print(\"Calibration 완료.\")\n",
    "\n",
    "\n",
    "# 9. 양자화 적용 및 변환\n",
    "quantized_model_static = convert(prepared_model, inplace=False)\n",
    "quantized_model_static.eval()\n",
    "\n",
    "# 10. 양자화된 모델 평가\n",
    "print(\"\\n--- Quantized Static 모델 평가 ---\")\n",
    "# 양자화된 모델도 CPU에서 평가합니다.\n",
    "quantized_accuracy = evaluate_model(quantized_model_static, test_loader)\n",
    "\n",
    "# 양자화된 모델 저장 (전체 모델 객체 저장)\n",
    "torch.save(quantized_model_static, \"resnet18_mnist_quantized_static.pth\")\n",
    "quantized_size = os.path.getsize(\"resnet18_mnist_quantized_static.pth\") / (1024 * 1024)\n",
    "print(f\"Quantized Static 모델 크기: {quantized_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n--- 최종 결과 비교 ---\")\n",
    "print(f\"FP32 모델 정확도: {fp32_accuracy:.2f}% (크기: {fp32_size:.2f} MB)\")\n",
    "print(f\"Static Quantized 모델 정확도: {quantized_accuracy:.2f}% (크기: {quantized_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8841939-3f4d-4d46-8940-a3926f045d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.ao.quantization import get_default_qconfig, prepare, convert, QConfigMapping\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 1. 하이퍼파라미터 및 설정\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5 # 양자화 전에 모델을 어느 정도 학습시키는 것이 좋습니다.\n",
    "CALIBRATION_BATCHES = 100 # Calibration에 사용할 배치 수 (MNIST 학습 데이터셋에서)\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. ResNet 모델 수정 (MNIST 1채널 입력용)\n",
    "class ResNet18_MNIST(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18_MNIST, self).__init__()\n",
    "        # 사전 학습된 ResNet18을 불러오지만, weights는 로드하지 않습니다.\n",
    "        # ImageNet_V1 Weights를 로드하려면 conv1을 다시 초기화해야 합니다.\n",
    "        original_resnet = resnet18(weights=None)\n",
    "\n",
    "        # conv1 레이어를 1채널 입력에 맞게 수정\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = original_resnet.bn1\n",
    "        self.relu = original_resnet.relu\n",
    "        self.maxpool = original_resnet.maxpool\n",
    "\n",
    "        self.layer1 = original_resnet.layer1\n",
    "        self.layer2 = original_resnet.layer2\n",
    "        self.layer3 = original_resnet.layer3\n",
    "        self.layer4 = original_resnet.layer4\n",
    "\n",
    "        self.avgpool = original_resnet.avgpool\n",
    "        self.fc = nn.Linear(original_resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 3. 데이터 로더 준비 (MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # 이미지를 텐서로 변환\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST 평균, 표준편차로 정규화\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "calibration_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# 4. 모델 학습 (양자화 전에 모델을 학습시킵니다)\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    # model.to(DEVICE) # 학습 함수 내에서 DEVICE로 이동합니다.\n",
    "    print(f\"\\n모델 학습 시작 ({num_epochs} 에폭)...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99: # 100 배치마다 출력\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "    print(\"모델 학습 완료.\")\n",
    "\n",
    "#---\n",
    "\n",
    "### 5. 모델 평가 함수 (수정됨)\n",
    "\n",
    "def evaluate_model(model, test_loader, target_device=None): # target_device 인자 추가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if target_device is None:\n",
    "        # 모델의 첫 번째 파라미터가 있는 디바이스를 가져옵니다.\n",
    "        # ResNet18_MNIST는 항상 파라미터를 가지므로 안전합니다.\n",
    "        model_device = next(model.parameters()).device\n",
    "    else:\n",
    "        model_device = target_device\n",
    "\n",
    "    # 모델을 평가할 디바이스로 옮겨줍니다.\n",
    "    model.to(model_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            # 입력 데이터를 모델이 있는 디바이스로 옮깁니다.\n",
    "            inputs, labels = inputs.to(model_device), labels.to(model_device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b15f573-0138-4277-ab45-e9d52f5673cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 학습 시작 (5 에폭)...\n",
      "Epoch [1/5], Batch [100/938], Loss: 0.3462\n",
      "Epoch [1/5], Batch [200/938], Loss: 0.1681\n",
      "Epoch [1/5], Batch [300/938], Loss: 0.1192\n",
      "Epoch [1/5], Batch [400/938], Loss: 0.1210\n",
      "Epoch [1/5], Batch [500/938], Loss: 0.1058\n",
      "Epoch [1/5], Batch [600/938], Loss: 0.0961\n",
      "Epoch [1/5], Batch [700/938], Loss: 0.0918\n",
      "Epoch [1/5], Batch [800/938], Loss: 0.0804\n",
      "Epoch [1/5], Batch [900/938], Loss: 0.0753\n",
      "Epoch [2/5], Batch [100/938], Loss: 0.0660\n",
      "Epoch [2/5], Batch [200/938], Loss: 0.0577\n",
      "Epoch [2/5], Batch [300/938], Loss: 0.0756\n",
      "Epoch [2/5], Batch [400/938], Loss: 0.0576\n",
      "Epoch [2/5], Batch [500/938], Loss: 0.0707\n",
      "Epoch [2/5], Batch [600/938], Loss: 0.0524\n",
      "Epoch [2/5], Batch [700/938], Loss: 0.0619\n",
      "Epoch [2/5], Batch [800/938], Loss: 0.0667\n",
      "Epoch [2/5], Batch [900/938], Loss: 0.0488\n",
      "Epoch [3/5], Batch [100/938], Loss: 0.0495\n",
      "Epoch [3/5], Batch [200/938], Loss: 0.0423\n",
      "Epoch [3/5], Batch [300/938], Loss: 0.0441\n",
      "Epoch [3/5], Batch [400/938], Loss: 0.0421\n",
      "Epoch [3/5], Batch [500/938], Loss: 0.0441\n",
      "Epoch [3/5], Batch [600/938], Loss: 0.0391\n",
      "Epoch [3/5], Batch [700/938], Loss: 0.0556\n",
      "Epoch [3/5], Batch [800/938], Loss: 0.0535\n",
      "Epoch [3/5], Batch [900/938], Loss: 0.0380\n",
      "Epoch [4/5], Batch [100/938], Loss: 0.0286\n",
      "Epoch [4/5], Batch [200/938], Loss: 0.0443\n",
      "Epoch [4/5], Batch [300/938], Loss: 0.0369\n",
      "Epoch [4/5], Batch [400/938], Loss: 0.0356\n",
      "Epoch [4/5], Batch [500/938], Loss: 0.0356\n",
      "Epoch [4/5], Batch [600/938], Loss: 0.0475\n",
      "Epoch [4/5], Batch [700/938], Loss: 0.0409\n",
      "Epoch [4/5], Batch [800/938], Loss: 0.0385\n",
      "Epoch [4/5], Batch [900/938], Loss: 0.0243\n",
      "Epoch [5/5], Batch [100/938], Loss: 0.0280\n",
      "Epoch [5/5], Batch [200/938], Loss: 0.0382\n",
      "Epoch [5/5], Batch [300/938], Loss: 0.0346\n",
      "Epoch [5/5], Batch [400/938], Loss: 0.0371\n",
      "Epoch [5/5], Batch [500/938], Loss: 0.0419\n",
      "Epoch [5/5], Batch [600/938], Loss: 0.0359\n",
      "Epoch [5/5], Batch [700/938], Loss: 0.0365\n",
      "Epoch [5/5], Batch [800/938], Loss: 0.0356\n",
      "Epoch [5/5], Batch [900/938], Loss: 0.0357\n",
      "모델 학습 완료.\n",
      "\n",
      "--- FP32 모델 평가 ---\n",
      "Accuracy: 99.00%\n",
      "FP32 모델 크기: 42.71 MB\n",
      "\n",
      "--- Static Quantization Calibration 시작 ---\n",
      "  Calibration batch: 0/100\n",
      "  Calibration batch: 20/100\n",
      "  Calibration batch: 40/100\n",
      "  Calibration batch: 60/100\n",
      "  Calibration batch: 80/100\n",
      "Calibration 완료.\n",
      "\n",
      "--- Quantized Static 모델 평가 ---\n",
      "Accuracy: 99.00%\n",
      "Quantized Static 모델 크기: 42.72 MB\n",
      "\n",
      "--- 최종 결과 비교 ---\n",
      "FP32 모델 정확도: 99.00% (크기: 42.71 MB)\n",
      "Static Quantized 모델 정확도: 99.00% (크기: 42.72 MB)\n"
     ]
    }
   ],
   "source": [
    "# 6. 모델 생성 및 초기 학습\n",
    "model_fp32 = ResNet18_MNIST(num_classes=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# GPU가 있다면 모델을 GPU로 이동\n",
    "# 이 줄이 중요합니다! model_fp32 = model_fp32.to(DEVICE) 로 수정합니다.\n",
    "model_fp32 = model_fp32.to(DEVICE) \n",
    "\n",
    "# 학습 실행\n",
    "train_model(model_fp32, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "print(\"\\n--- FP32 모델 평가 ---\")\n",
    "# FP32 모델은 학습이 끝난 후 현재 DEVICE (GPU)에 있으므로, 해당 DEVICE에서 평가\n",
    "fp32_accuracy = evaluate_model(model_fp32, test_loader, target_device=DEVICE)\n",
    "torch.save(model_fp32.state_dict(), \"resnet18_mnist_fp32.pth\")\n",
    "fp32_size = os.path.getsize(\"resnet18_mnist_fp32.pth\") / (1024 * 1024)\n",
    "print(f\"FP32 모델 크기: {fp32_size:.2f} MB\")\n",
    "\n",
    "# 7. Static Quantization 준비\n",
    "# 모델을 CPU로 옮깁니다. PyTorch의 PTQ는 현재 CPU에서 주로 지원됩니다.\n",
    "model_fp32.to(\"cpu\")\n",
    "model_fp32.eval() # 평가 모드 설정\n",
    "\n",
    "# 레이어 퓨징 (성능 향상)\n",
    "fused_model = torch.ao.quantization.fuse_modules(model_fp32,\n",
    "                                               [['conv1', 'bn1', 'relu']],\n",
    "                                               inplace=False)\n",
    "\n",
    "# QConfig 설정 (CPU 최적화된 fbgemm 백엔드)\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig) # PyTorch 1.13+\n",
    "\n",
    "# 모델 준비 (양자화 가능한 모듈을 대체하고 Observer 삽입)\n",
    "prepared_model = prepare(fused_model, qconfig_mapping) # 'inplace=False' 인자 제거\n",
    "\n",
    "\n",
    "# 8. Calibration (보정)\n",
    "print(\"\\n--- Static Quantization Calibration 시작 ---\")\n",
    "# Calibration loader를 통해 모델에 데이터를 흘려보내 활성화 분포를 측정합니다.\n",
    "with torch.inference_mode():\n",
    "    # Calibration 시에도 모델이 CPU에 있으므로, 입력 데이터를 CPU로 옮겨줍니다.\n",
    "    for batch_idx, (inputs, labels) in enumerate(calibration_loader):\n",
    "        if batch_idx >= CALIBRATION_BATCHES:\n",
    "            break\n",
    "        inputs = inputs.to(\"cpu\") # Calibration 데이터는 CPU로!\n",
    "        prepared_model(inputs)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"  Calibration batch: {batch_idx}/{CALIBRATION_BATCHES}\")\n",
    "print(\"Calibration 완료.\")\n",
    "\n",
    "\n",
    "# 9. 양자화 적용 및 변환\n",
    "quantized_model_static = convert(prepared_model, inplace=False)\n",
    "quantized_model_static.eval()\n",
    "\n",
    "# 10. 양자화된 모델 평가 (수정됨)\n",
    "print(\"\\n--- Quantized Static 모델 평가 ---\")\n",
    "# 양자화된 모델은 CPU에 있으므로, CPU에서 평가하도록 명시합니다.\n",
    "quantized_accuracy = evaluate_model(quantized_model_static, test_loader, target_device=torch.device(\"cpu\"))\n",
    "\n",
    "# 양자화된 모델 저장 (전체 모델 객체 저장)\n",
    "torch.save(quantized_model_static, \"resnet18_mnist_quantized_static.pth\")\n",
    "quantized_size = os.path.getsize(\"resnet18_mnist_quantized_static.pth\") / (1024 * 1024)\n",
    "print(f\"Quantized Static 모델 크기: {quantized_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n--- 최종 결과 비교 ---\")\n",
    "print(f\"FP32 모델 정확도: {fp32_accuracy:.2f}% (크기: {fp32_size:.2f} MB)\")\n",
    "print(f\"Static Quantized 모델 정확도: {quantized_accuracy:.2f}% (크기: {quantized_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9f0b9-ceae-47ac-8f45-a197025f6dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3a674-7b96-40d8-a48b-2b3a73bdc39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49cb0b8c-ad4e-491c-a866-19fbc2f257ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 학습 시작 (5 에폭)...\n",
      "Epoch [1/5], Batch [100/938], Loss: 0.3756\n",
      "Epoch [1/5], Batch [200/938], Loss: 0.1647\n",
      "Epoch [1/5], Batch [300/938], Loss: 0.1524\n",
      "Epoch [1/5], Batch [400/938], Loss: 0.1093\n",
      "Epoch [1/5], Batch [500/938], Loss: 0.1072\n",
      "Epoch [1/5], Batch [600/938], Loss: 0.0920\n",
      "Epoch [1/5], Batch [700/938], Loss: 0.0789\n",
      "Epoch [1/5], Batch [800/938], Loss: 0.0892\n",
      "Epoch [1/5], Batch [900/938], Loss: 0.0991\n",
      "Epoch [2/5], Batch [100/938], Loss: 0.0457\n",
      "Epoch [2/5], Batch [200/938], Loss: 0.0668\n",
      "Epoch [2/5], Batch [300/938], Loss: 0.0616\n",
      "Epoch [2/5], Batch [400/938], Loss: 0.0651\n",
      "Epoch [2/5], Batch [500/938], Loss: 0.0617\n",
      "Epoch [2/5], Batch [600/938], Loss: 0.0526\n",
      "Epoch [2/5], Batch [700/938], Loss: 0.0585\n",
      "Epoch [2/5], Batch [800/938], Loss: 0.0664\n",
      "Epoch [2/5], Batch [900/938], Loss: 0.0589\n",
      "Epoch [3/5], Batch [100/938], Loss: 0.0413\n",
      "Epoch [3/5], Batch [200/938], Loss: 0.0415\n",
      "Epoch [3/5], Batch [300/938], Loss: 0.0409\n",
      "Epoch [3/5], Batch [400/938], Loss: 0.0430\n",
      "Epoch [3/5], Batch [500/938], Loss: 0.0557\n",
      "Epoch [3/5], Batch [600/938], Loss: 0.0497\n",
      "Epoch [3/5], Batch [700/938], Loss: 0.0481\n",
      "Epoch [3/5], Batch [800/938], Loss: 0.0461\n",
      "Epoch [3/5], Batch [900/938], Loss: 0.0396\n",
      "Epoch [4/5], Batch [100/938], Loss: 0.0389\n",
      "Epoch [4/5], Batch [200/938], Loss: 0.0408\n",
      "Epoch [4/5], Batch [300/938], Loss: 0.0432\n",
      "Epoch [4/5], Batch [400/938], Loss: 0.0432\n",
      "Epoch [4/5], Batch [500/938], Loss: 0.0326\n",
      "Epoch [4/5], Batch [600/938], Loss: 0.0409\n",
      "Epoch [4/5], Batch [700/938], Loss: 0.0465\n",
      "Epoch [4/5], Batch [800/938], Loss: 0.0372\n",
      "Epoch [4/5], Batch [900/938], Loss: 0.0471\n",
      "Epoch [5/5], Batch [100/938], Loss: 0.0231\n",
      "Epoch [5/5], Batch [200/938], Loss: 0.0318\n",
      "Epoch [5/5], Batch [300/938], Loss: 0.0375\n",
      "Epoch [5/5], Batch [400/938], Loss: 0.0355\n",
      "Epoch [5/5], Batch [500/938], Loss: 0.0342\n",
      "Epoch [5/5], Batch [600/938], Loss: 0.0400\n",
      "Epoch [5/5], Batch [700/938], Loss: 0.0283\n",
      "Epoch [5/5], Batch [800/938], Loss: 0.0306\n",
      "Epoch [5/5], Batch [900/938], Loss: 0.0341\n",
      "모델 학습 완료.\n",
      "\n",
      "--- FP32 모델 평가 ---\n",
      "Accuracy: 99.10%\n",
      "FP32 모델 크기: 42.71 MB\n",
      "FX Graph 퓨징 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FX Graph prepare 완료.\n",
      "\n",
      "--- Static Quantization Calibration 시작 ---\n",
      "  Calibration batch: 0/100\n",
      "  Calibration batch: 20/100\n",
      "  Calibration batch: 40/100\n",
      "  Calibration batch: 60/100\n",
      "  Calibration batch: 80/100\n",
      "Calibration 완료.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'layer1.0.conv1.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 198\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalibration 완료.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# 9. 양자화 적용 및 변환 (FX Graph 기반으로 수정)\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m quantized_model_static \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m quantized_model_static\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFX Graph convert 완료.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:615\u001b[0m, in \u001b[0;36mconvert_fx\u001b[1;34m(graph_module, convert_custom_config, _remove_qconfig, qconfig_mapping, backend_config, keep_original_weights)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Convert a calibrated or trained model to a quantized model\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m \n\u001b[0;32m    613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    614\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_api.quantize_fx.convert_fx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:541\u001b[0m, in \u001b[0;36m_convert_fx\u001b[1;34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[0m\n\u001b[0;32m    534\u001b[0m preserved_attr_names \u001b[38;5;241m=\u001b[39m convert_custom_config\u001b[38;5;241m.\u001b[39mpreserved_attributes\n\u001b[0;32m    535\u001b[0m preserved_attrs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    536\u001b[0m     attr: \u001b[38;5;28mgetattr\u001b[39m(graph_module, attr)\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m preserved_attr_names\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graph_module, attr)\n\u001b[0;32m    539\u001b[0m }\n\u001b[1;32m--> 541\u001b[0m quantized \u001b[38;5;241m=\u001b[39m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_standalone_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_decomposed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_decomposed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m attach_preserved_attrs_to_model(quantized, preserved_attrs)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantized\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\convert.py:1247\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;66;03m# TODO: maybe move this to quantize_fx.py\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_reference:\n\u001b[1;32m-> 1247\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mlower_to_fbgemm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_qconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_scope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;66;03m# TODO: this looks hacky, we want to check why we need this and see if we can\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;66;03m# remove this\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;66;03m# removes qconfig and activation_post_process modules\u001b[39;00m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _remove_qconfig_flag:\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\lower_to_fbgemm.py:19\u001b[0m, in \u001b[0;36mlower_to_fbgemm\u001b[1;34m(model, qconfig_map, node_name_to_scope, keep_original_weights)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlower_to_fbgemm\u001b[39m(\n\u001b[0;32m     11\u001b[0m     model: GraphModule,\n\u001b[0;32m     12\u001b[0m     qconfig_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, QConfigAny],\n\u001b[0;32m     13\u001b[0m     node_name_to_scope: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m]],\n\u001b[0;32m     14\u001b[0m     keep_original_weights: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lower a quantized reference model (with reference quantized operator patterns)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    to fbgemm\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lower_to_native_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_scope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\_lower_to_native_backend.py:1339\u001b[0m, in \u001b[0;36m_lower_to_native_backend\u001b[1;34m(model, qconfig_map, node_name_to_scope, keep_original_weights)\u001b[0m\n\u001b[0;32m   1337\u001b[0m _lower_static_weighted_ref_module(model, qconfig_map)\n\u001b[0;32m   1338\u001b[0m _lower_static_weighted_ref_module_with_two_inputs(model, qconfig_map)\n\u001b[1;32m-> 1339\u001b[0m \u001b[43m_lower_dynamic_weighted_ref_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m _lower_weight_only_weighted_ref_module(model)\n\u001b[0;32m   1341\u001b[0m _lower_static_weighted_ref_functional(model, qconfig_map)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\_lower_to_native_backend.py:809\u001b[0m, in \u001b[0;36m_lower_dynamic_weighted_ref_module\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    807\u001b[0m named_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(model\u001b[38;5;241m.\u001b[39mnamed_modules(remove_duplicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[43mnamed_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\n\u001b[0;32m    810\u001b[0m         DYNAMIC_LOWER_MODULE_MAP\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    811\u001b[0m     )\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(DYNAMIC_LOWER_FUSED_MODULE_MAP\u001b[38;5;241m.\u001b[39mkeys())):\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    813\u001b[0m     ref_node \u001b[38;5;241m=\u001b[39m n\n",
      "\u001b[1;31mKeyError\u001b[0m: 'layer1.0.conv1.1'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "# FX Graph 기반 양자화를 위한 모듈 임포트\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, fuse_fx\n",
    "from torch.ao.quantization import get_default_qconfig_mapping # QConfigMapping을 가져오는 더 간결한 방법\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 1. 하이퍼파라미터 및 설정\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5 # 양자화 전에 모델을 어느 정도 학습시키는 것이 좋습니다.\n",
    "CALIBRATION_BATCHES = 100 # Calibration에 사용할 배치 수 (MNIST 학습 데이터셋에서)\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. ResNet 모델 수정 (MNIST 1채널 입력용)\n",
    "class ResNet18_MNIST(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18_MNIST, self).__init__()\n",
    "        # 사전 학습된 ResNet18을 불러오지만, weights는 로드하지 않습니다.\n",
    "        original_resnet = resnet18(weights=None)\n",
    "\n",
    "        # conv1 레이어를 1채널 입력에 맞게 수정\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = original_resnet.bn1\n",
    "        self.relu = original_resnet.relu\n",
    "        self.maxpool = original_resnet.maxpool\n",
    "\n",
    "        self.layer1 = original_resnet.layer1\n",
    "        self.layer2 = original_resnet.layer2\n",
    "        self.layer3 = original_resnet.layer3\n",
    "        self.layer4 = original_resnet.layer4\n",
    "\n",
    "        self.avgpool = original_resnet.avgpool\n",
    "        self.fc = nn.Linear(original_resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 3. 데이터 로더 준비 (MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # 이미지를 텐서로 변환\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST 평균, 표준편차로 정규화\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "calibration_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# 4. 모델 학습 (양자화 전에 모델을 학습시킵니다)\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    # model.to(DEVICE) # 모델 이동은 호출하는 쪽에서 명시적으로 처리\n",
    "    print(f\"\\n모델 학습 시작 ({num_epochs} 에폭)...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99: # 100 배치마다 출력\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "    print(\"모델 학습 완료.\")\n",
    "\n",
    "# 5. 모델 평가 함수 (이전과 동일)\n",
    "def evaluate_model(model, test_loader, target_device=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if target_device is None:\n",
    "        model_device = next(model.parameters()).device\n",
    "    else:\n",
    "        model_device = target_device\n",
    "\n",
    "    model.to(model_device) # 모델을 평가할 디바이스로 옮겨줍니다.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(model_device), labels.to(model_device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "'''\n",
    "# 6. 모델 생성 및 초기 학습\n",
    "model_fp32 = ResNet18_MNIST(num_classes=10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# GPU가 있다면 모델을 GPU로 이동\n",
    "model_fp32 = model_fp32.to(DEVICE)\n",
    "\n",
    "# 학습 실행\n",
    "train_model(model_fp32, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "print(\"\\n--- FP32 모델 평가 ---\")\n",
    "fp32_accuracy = evaluate_model(model_fp32, test_loader, target_device=DEVICE)\n",
    "torch.save(model_fp32.state_dict(), \"resnet18_mnist_fp32.pth\")\n",
    "fp32_size = os.path.getsize(\"resnet18_mnist_fp32.pth\") / (1024 * 1024)\n",
    "print(f\"FP32 모델 크기: {fp32_size:.2f} MB\")\n",
    "'''\n",
    "# 6. 모델 생성 및 초기 학습\n",
    "# model_fp32 = ResNet18_MNIST(num_classes=10) # 이 부분 변경\n",
    "# model_fp32 = resnet18(weights=None) # torchvision의 기본 ResNet18 로드\n",
    "model_fp32 = resnet18(weights=None) # torchvision.models.resnet18에서 로드\n",
    "\n",
    "# conv1 레이어 수정 (1채널 입력에 맞게)\n",
    "model_fp32.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# fc 레이어 수정 (MNIST 10개 클래스에 맞게)\n",
    "model_fp32.fc = nn.Linear(model_fp32.fc.in_features, 10)\n",
    "\n",
    "# 양자화 친화적으로 만들기 위한 Stubs 삽입 (FX 양자화에는 불필요하지만 일반 양자화에 필요)\n",
    "# FX 양자화에서는 내부적으로 처리되지만, 명시적으로 추가하는 것은 나쁘지 않습니다.\n",
    "# model_fp32 = nn.Sequential(\n",
    "#     QuantStub(), # 입력 양자화를 위한 QuantStub\n",
    "#     model_fp32,\n",
    "#     DeQuantStub() # 출력 역양자화를 위한 DeQuantStub\n",
    "# )\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model_fp32 = model_fp32.to(DEVICE) # 모델을 GPU로 이동\n",
    "\n",
    "train_model(model_fp32, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "print(\"\\n--- FP32 모델 평가 ---\")\n",
    "fp32_accuracy = evaluate_model(model_fp32, test_loader, target_device=DEVICE)\n",
    "torch.save(model_fp32.state_dict(), \"resnet18_mnist_fp32.pth\")\n",
    "fp32_size = os.path.getsize(\"resnet18_mnist_fp32.pth\") / (1024 * 1024)\n",
    "print(f\"FP32 모델 크기: {fp32_size:.2f} MB\")\n",
    "\n",
    "# 7. Static Quantization 준비 (FX Graph 기반으로 수정)\n",
    "# 모델을 CPU로 옮깁니다. FX 양자화도 현재 CPU에서 주로 지원됩니다.\n",
    "model_fp32.to(\"cpu\")\n",
    "model_fp32.eval() # 평가 모드 설정\n",
    "\n",
    "# 7-1. FX 그래프 기반 모델 퓨징\n",
    "# torchvision.models.quantization.utils.fuse_model을 사용하거나,\n",
    "# fuse_fx를 사용하는 경우에도 모델 구조가 표준에 가까워야 함.\n",
    "# 여기서는 torchvision의 양자화 친화적 ResNet을 사용하므로 fuse_fx가 더 잘 작동할 가능성이 높음.\n",
    "# 모델을 직접 fuse_fx에 전달하면 됩니다.\n",
    "fused_model = fuse_fx(model_fp32)\n",
    "print(\"FX Graph 퓨징 완료.\")\n",
    "\n",
    "# 7-2. QConfig 설정\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n",
    "\n",
    "# 7-3. 모델 준비 (FX 그래프 기반)\n",
    "example_input = torch.randn(1, 1, 28, 28) # MNIST 입력 크기 (배치1, 채널1, 높이28, 너비28)\n",
    "prepared_model = prepare_fx(fused_model, qconfig_mapping, example_inputs=(example_input,))\n",
    "print(\"FX Graph prepare 완료.\")\n",
    "\n",
    "# 8. Calibration (보정) - 이전과 동일\n",
    "print(\"\\n--- Static Quantization Calibration 시작 ---\")\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (inputs, labels) in enumerate(calibration_loader):\n",
    "        if batch_idx >= CALIBRATION_BATCHES:\n",
    "            break\n",
    "        inputs = inputs.to(\"cpu\") # Calibration 데이터는 CPU로!\n",
    "        prepared_model(inputs)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"  Calibration batch: {batch_idx}/{CALIBRATION_BATCHES}\")\n",
    "print(\"Calibration 완료.\")\n",
    "\n",
    "\n",
    "# 9. 양자화 적용 및 변환 (FX Graph 기반으로 수정)\n",
    "quantized_model_static = convert_fx(prepared_model)\n",
    "quantized_model_static.eval()\n",
    "print(\"FX Graph convert 완료.\")\n",
    "\n",
    "# 10. 양자화된 모델 평가 (이전과 동일)\n",
    "print(\"\\n--- Quantized Static 모델 평가 ---\")\n",
    "quantized_accuracy = evaluate_model(quantized_model_static, test_loader, target_device=torch.device(\"cpu\"))\n",
    "\n",
    "# 양자화된 모델 저장 (전체 모델 객체 저장)\n",
    "torch.save(quantized_model_static, \"resnet18_mnist_quantized_static_fx.pth\") # 파일명 변경\n",
    "quantized_size = os.path.getsize(\"resnet18_mnist_quantized_static_fx.pth\") / (1024 * 1024)\n",
    "print(f\"Quantized Static (FX) 모델 크기: {quantized_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n--- 최종 결과 비교 ---\")\n",
    "print(f\"FP32 모델 정확도: {fp32_accuracy:.2f}% (크기: {fp32_size:.2f} MB)\")\n",
    "print(f\"Static Quantized (FX) 모델 정확도: {quantized_accuracy:.2f}% (크기: {quantized_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5cc8c28-c133-43ac-8e79-a7e90240450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 학습 시작 (5 에폭)...\n",
      "Epoch [1/5], Batch [100/938], Loss: 0.3427\n",
      "Epoch [1/5], Batch [200/938], Loss: 0.1869\n",
      "Epoch [1/5], Batch [300/938], Loss: 0.1222\n",
      "Epoch [1/5], Batch [400/938], Loss: 0.1132\n",
      "Epoch [1/5], Batch [500/938], Loss: 0.1099\n",
      "Epoch [1/5], Batch [600/938], Loss: 0.1094\n",
      "Epoch [1/5], Batch [700/938], Loss: 0.0952\n",
      "Epoch [1/5], Batch [800/938], Loss: 0.0876\n",
      "Epoch [1/5], Batch [900/938], Loss: 0.0817\n",
      "Epoch [2/5], Batch [100/938], Loss: 0.0549\n",
      "Epoch [2/5], Batch [200/938], Loss: 0.0543\n",
      "Epoch [2/5], Batch [300/938], Loss: 0.0600\n",
      "Epoch [2/5], Batch [400/938], Loss: 0.0717\n",
      "Epoch [2/5], Batch [500/938], Loss: 0.0525\n",
      "Epoch [2/5], Batch [600/938], Loss: 0.0608\n",
      "Epoch [2/5], Batch [700/938], Loss: 0.0527\n",
      "Epoch [2/5], Batch [800/938], Loss: 0.0618\n",
      "Epoch [2/5], Batch [900/938], Loss: 0.0591\n",
      "Epoch [3/5], Batch [100/938], Loss: 0.0363\n",
      "Epoch [3/5], Batch [200/938], Loss: 0.0546\n",
      "Epoch [3/5], Batch [300/938], Loss: 0.0416\n",
      "Epoch [3/5], Batch [400/938], Loss: 0.0377\n",
      "Epoch [3/5], Batch [500/938], Loss: 0.0448\n",
      "Epoch [3/5], Batch [600/938], Loss: 0.0481\n",
      "Epoch [3/5], Batch [700/938], Loss: 0.0586\n",
      "Epoch [3/5], Batch [800/938], Loss: 0.0508\n",
      "Epoch [3/5], Batch [900/938], Loss: 0.0508\n",
      "Epoch [4/5], Batch [100/938], Loss: 0.0382\n",
      "Epoch [4/5], Batch [200/938], Loss: 0.0347\n",
      "Epoch [4/5], Batch [300/938], Loss: 0.0291\n",
      "Epoch [4/5], Batch [400/938], Loss: 0.0411\n",
      "Epoch [4/5], Batch [500/938], Loss: 0.0402\n",
      "Epoch [4/5], Batch [600/938], Loss: 0.0326\n",
      "Epoch [4/5], Batch [700/938], Loss: 0.0445\n",
      "Epoch [4/5], Batch [800/938], Loss: 0.0469\n",
      "Epoch [4/5], Batch [900/938], Loss: 0.0334\n",
      "Epoch [5/5], Batch [100/938], Loss: 0.0300\n",
      "Epoch [5/5], Batch [200/938], Loss: 0.0331\n",
      "Epoch [5/5], Batch [300/938], Loss: 0.0305\n",
      "Epoch [5/5], Batch [400/938], Loss: 0.0363\n",
      "Epoch [5/5], Batch [500/938], Loss: 0.0401\n",
      "Epoch [5/5], Batch [600/938], Loss: 0.0335\n",
      "Epoch [5/5], Batch [700/938], Loss: 0.0310\n",
      "Epoch [5/5], Batch [800/938], Loss: 0.0391\n",
      "Epoch [5/5], Batch [900/938], Loss: 0.0373\n",
      "모델 학습 완료.\n",
      "\n",
      "--- FP32 모델 평가 ---\n",
      "Accuracy: 98.96%\n",
      "FP32 모델 크기: 42.71 MB\n",
      "FX Graph 퓨징 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FX Graph prepare 완료.\n",
      "\n",
      "--- Static Quantization Calibration 시작 ---\n",
      "  Calibration batch: 0/100\n",
      "  Calibration batch: 20/100\n",
      "  Calibration batch: 40/100\n",
      "  Calibration batch: 60/100\n",
      "  Calibration batch: 80/100\n",
      "Calibration 완료.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'layer1.0.conv1.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 146\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalibration 완료.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# 9. 양자화 적용 및 변환 (FX Graph 기반)\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# inplace=False를 명시적으로 추가\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m quantized_model_static \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m quantized_model_static\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFX Graph convert 완료.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:615\u001b[0m, in \u001b[0;36mconvert_fx\u001b[1;34m(graph_module, convert_custom_config, _remove_qconfig, qconfig_mapping, backend_config, keep_original_weights)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Convert a calibrated or trained model to a quantized model\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m \n\u001b[0;32m    613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    614\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_api.quantize_fx.convert_fx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:541\u001b[0m, in \u001b[0;36m_convert_fx\u001b[1;34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[0m\n\u001b[0;32m    534\u001b[0m preserved_attr_names \u001b[38;5;241m=\u001b[39m convert_custom_config\u001b[38;5;241m.\u001b[39mpreserved_attributes\n\u001b[0;32m    535\u001b[0m preserved_attrs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    536\u001b[0m     attr: \u001b[38;5;28mgetattr\u001b[39m(graph_module, attr)\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m preserved_attr_names\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graph_module, attr)\n\u001b[0;32m    539\u001b[0m }\n\u001b[1;32m--> 541\u001b[0m quantized \u001b[38;5;241m=\u001b[39m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_standalone_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_decomposed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_decomposed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m attach_preserved_attrs_to_model(quantized, preserved_attrs)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantized\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\convert.py:1247\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;66;03m# TODO: maybe move this to quantize_fx.py\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_reference:\n\u001b[1;32m-> 1247\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mlower_to_fbgemm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_qconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_scope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;66;03m# TODO: this looks hacky, we want to check why we need this and see if we can\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;66;03m# remove this\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;66;03m# removes qconfig and activation_post_process modules\u001b[39;00m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _remove_qconfig_flag:\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\lower_to_fbgemm.py:19\u001b[0m, in \u001b[0;36mlower_to_fbgemm\u001b[1;34m(model, qconfig_map, node_name_to_scope, keep_original_weights)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlower_to_fbgemm\u001b[39m(\n\u001b[0;32m     11\u001b[0m     model: GraphModule,\n\u001b[0;32m     12\u001b[0m     qconfig_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, QConfigAny],\n\u001b[0;32m     13\u001b[0m     node_name_to_scope: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m]],\n\u001b[0;32m     14\u001b[0m     keep_original_weights: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lower a quantized reference model (with reference quantized operator patterns)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    to fbgemm\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lower_to_native_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_scope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\_lower_to_native_backend.py:1339\u001b[0m, in \u001b[0;36m_lower_to_native_backend\u001b[1;34m(model, qconfig_map, node_name_to_scope, keep_original_weights)\u001b[0m\n\u001b[0;32m   1337\u001b[0m _lower_static_weighted_ref_module(model, qconfig_map)\n\u001b[0;32m   1338\u001b[0m _lower_static_weighted_ref_module_with_two_inputs(model, qconfig_map)\n\u001b[1;32m-> 1339\u001b[0m \u001b[43m_lower_dynamic_weighted_ref_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m _lower_weight_only_weighted_ref_module(model)\n\u001b[0;32m   1341\u001b[0m _lower_static_weighted_ref_functional(model, qconfig_map)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\_lower_to_native_backend.py:809\u001b[0m, in \u001b[0;36m_lower_dynamic_weighted_ref_module\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    807\u001b[0m named_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(model\u001b[38;5;241m.\u001b[39mnamed_modules(remove_duplicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[43mnamed_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\n\u001b[0;32m    810\u001b[0m         DYNAMIC_LOWER_MODULE_MAP\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    811\u001b[0m     )\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(DYNAMIC_LOWER_FUSED_MODULE_MAP\u001b[38;5;241m.\u001b[39mkeys())):\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    813\u001b[0m     ref_node \u001b[38;5;241m=\u001b[39m n\n",
      "\u001b[1;31mKeyError\u001b[0m: 'layer1.0.conv1.1'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18 # torchvision의 기본 resnet18 임포트\n",
    "# FX Graph 기반 양자화를 위한 모듈 임포트\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, fuse_fx\n",
    "from torch.ao.quantization import get_default_qconfig_mapping # QConfigMapping을 가져오는 간결한 방법\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 1. 하이퍼파라미터 및 설정\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5 # 양자화 전에 모델을 어느 정도 학습시키는 것이 좋습니다.\n",
    "CALIBRATION_BATCHES = 100 # Calibration에 사용할 배치 수 (MNIST 학습 데이터셋에서)\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. ResNet 모델 정의 (이제 torchvision의 기본 resnet18을 사용하고, conv1과 fc만 수정)\n",
    "# 이전의 ResNet18_MNIST 클래스는 이제 필요 없습니다.\n",
    "\n",
    "# 3. 데이터 로더 준비 (MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # 이미지를 텐서로 변환\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST 평균, 표준편차로 정규화\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "calibration_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# 4. 모델 학습 함수\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    print(f\"\\n모델 학습 시작 ({num_epochs} 에폭)...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99: # 100 배치마다 출력\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "    print(\"모델 학습 완료.\")\n",
    "\n",
    "# 5. 모델 평가 함수\n",
    "def evaluate_model(model, test_loader, target_device=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if target_device is None:\n",
    "        # 모델의 첫 번째 파라미터가 있는 디바이스를 가져옵니다.\n",
    "        model_device = next(model.parameters()).device\n",
    "    else:\n",
    "        model_device = target_device\n",
    "\n",
    "    model.to(model_device) # 모델을 평가할 디바이스로 옮겨줍니다.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(model_device), labels.to(model_device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# --- 메인 실행 로직 ---\n",
    "\n",
    "# 6. 모델 생성 및 초기 학습 (torchvision의 resnet18 사용)\n",
    "model_fp32 = resnet18(weights=None) # 사전 학습된 가중치 없이 로드\n",
    "\n",
    "# conv1 레이어 수정 (1채널 입력에 맞게)\n",
    "# ResNet은 원래 3채널을 받으므로, MNIST 흑백 이미지(1채널)에 맞춰 수정\n",
    "model_fp32.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "# fc 레이어 수정 (MNIST 10개 클래스에 맞게)\n",
    "model_fp32.fc = nn.Linear(model_fp32.fc.in_features, 10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# GPU가 있다면 모델을 GPU로 이동 (학습 전에 필수)\n",
    "model_fp32 = model_fp32.to(DEVICE)\n",
    "\n",
    "# 학습 실행\n",
    "train_model(model_fp32, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "print(\"\\n--- FP32 모델 평가 ---\")\n",
    "fp32_accuracy = evaluate_model(model_fp32, test_loader, target_device=DEVICE)\n",
    "torch.save(model_fp32.state_dict(), \"resnet18_mnist_fp32.pth\")\n",
    "fp32_size = os.path.getsize(\"resnet18_mnist_fp32.pth\") / (1024 * 1024)\n",
    "print(f\"FP32 모델 크기: {fp32_size:.2f} MB\")\n",
    "\n",
    "\n",
    "\n",
    "### Static Quantization (FX Graph 기반)\n",
    "\n",
    "# 7. Static Quantization 준비 (FX Graph 기반)\n",
    "# 모델을 CPU로 옮깁니다. FX 양자화는 주로 CPU에서 이루어집니다.\n",
    "model_fp32.to(\"cpu\")\n",
    "model_fp32.eval() # 평가 모드 설정\n",
    "\n",
    "# 7-1. FX 그래프 기반 모델 퓨징\n",
    "# 모든 Conv-BN-ReLU, Conv-ReLU 등의 패턴을 자동으로 퓨징합니다.\n",
    "fused_model = fuse_fx(model_fp32)\n",
    "print(\"FX Graph 퓨징 완료.\")\n",
    "\n",
    "# 7-2. QConfig 설정 (CPU 최적화된 fbgemm 백엔드)\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n",
    "\n",
    "# 7-3. 모델 준비 (FX 그래프 기반)\n",
    "# example_inputs가 필요합니다. 이는 모델의 계산 그래프를 트레이싱할 때 사용됩니다.\n",
    "# MNIST 입력 크기: (배치1, 채널1, 높이28, 너비28)\n",
    "example_input = torch.randn(1, 1, 28, 28)\n",
    "# inplace=False를 명시적으로 추가\n",
    "prepared_model = prepare_fx(fused_model, qconfig_mapping, example_inputs=(example_input,))\n",
    "print(\"FX Graph prepare 완료.\")\n",
    "\n",
    "# 8. Calibration (보정)\n",
    "print(\"\\n--- Static Quantization Calibration 시작 ---\")\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (inputs, labels) in enumerate(calibration_loader):\n",
    "        if batch_idx >= CALIBRATION_BATCHES:\n",
    "            break\n",
    "        inputs = inputs.to(\"cpu\") # Calibration 데이터는 CPU로!\n",
    "        prepared_model(inputs)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"  Calibration batch: {batch_idx}/{CALIBRATION_BATCHES}\")\n",
    "print(\"Calibration 완료.\")\n",
    "\n",
    "\n",
    "# 9. 양자화 적용 및 변환 (FX Graph 기반)\n",
    "# inplace=False를 명시적으로 추가\n",
    "quantized_model_static = convert_fx(prepared_model)\n",
    "quantized_model_static.eval()\n",
    "print(\"FX Graph convert 완료.\")\n",
    "\n",
    "# 10. 양자화된 모델 평가\n",
    "print(\"\\n--- Quantized Static 모델 평가 ---\")\n",
    "# 양자화된 모델은 CPU에 있으므로, CPU에서 평가하도록 명시합니다.\n",
    "quantized_accuracy = evaluate_model(quantized_model_static, test_loader, target_device=torch.device(\"cpu\"))\n",
    "\n",
    "# 양자화된 모델 저장 (전체 모델 객체 저장)\n",
    "torch.save(quantized_model_static, \"resnet18_mnist_quantized_static_fx.pth\") # 파일명 변경\n",
    "quantized_size = os.path.getsize(\"resnet18_mnist_quantized_static_fx.pth\") / (1024 * 1024)\n",
    "print(f\"Quantized Static (FX) 모델 크기: {quantized_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n--- 최종 결과 비교 ---\")\n",
    "print(f\"FP32 모델 정확도: {fp32_accuracy:.2f}% (크기: {fp32_size:.2f} MB)\")\n",
    "print(f\"Static Quantized (FX) 모델 정확도: {quantized_accuracy:.2f}% (크기: {quantized_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "840aa308-aa20-4d86-9224-23e980277a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 학습 시작 (5 에폭)...\n",
      "Epoch [1/5], Batch [100/938], Loss: 0.3681\n",
      "Epoch [1/5], Batch [200/938], Loss: 0.1724\n",
      "Epoch [1/5], Batch [300/938], Loss: 0.1211\n",
      "Epoch [1/5], Batch [400/938], Loss: 0.1261\n",
      "Epoch [1/5], Batch [500/938], Loss: 0.1104\n",
      "Epoch [1/5], Batch [600/938], Loss: 0.0953\n",
      "Epoch [1/5], Batch [700/938], Loss: 0.0849\n",
      "Epoch [1/5], Batch [800/938], Loss: 0.0874\n",
      "Epoch [1/5], Batch [900/938], Loss: 0.0819\n",
      "Epoch [2/5], Batch [100/938], Loss: 0.0721\n",
      "Epoch [2/5], Batch [200/938], Loss: 0.0570\n",
      "Epoch [2/5], Batch [300/938], Loss: 0.0464\n",
      "Epoch [2/5], Batch [400/938], Loss: 0.0660\n",
      "Epoch [2/5], Batch [500/938], Loss: 0.0629\n",
      "Epoch [2/5], Batch [600/938], Loss: 0.0541\n",
      "Epoch [2/5], Batch [700/938], Loss: 0.0558\n",
      "Epoch [2/5], Batch [800/938], Loss: 0.0592\n",
      "Epoch [2/5], Batch [900/938], Loss: 0.0594\n",
      "Epoch [3/5], Batch [100/938], Loss: 0.0450\n",
      "Epoch [3/5], Batch [200/938], Loss: 0.0474\n",
      "Epoch [3/5], Batch [300/938], Loss: 0.0434\n",
      "Epoch [3/5], Batch [400/938], Loss: 0.0430\n",
      "Epoch [3/5], Batch [500/938], Loss: 0.0474\n",
      "Epoch [3/5], Batch [600/938], Loss: 0.0552\n",
      "Epoch [3/5], Batch [700/938], Loss: 0.0527\n",
      "Epoch [3/5], Batch [800/938], Loss: 0.0447\n",
      "Epoch [3/5], Batch [900/938], Loss: 0.0538\n",
      "Epoch [4/5], Batch [100/938], Loss: 0.0425\n",
      "Epoch [4/5], Batch [200/938], Loss: 0.0390\n",
      "Epoch [4/5], Batch [300/938], Loss: 0.0313\n",
      "Epoch [4/5], Batch [400/938], Loss: 0.0378\n",
      "Epoch [4/5], Batch [500/938], Loss: 0.0458\n",
      "Epoch [4/5], Batch [600/938], Loss: 0.0438\n",
      "Epoch [4/5], Batch [700/938], Loss: 0.0358\n",
      "Epoch [4/5], Batch [800/938], Loss: 0.0335\n",
      "Epoch [4/5], Batch [900/938], Loss: 0.0271\n",
      "Epoch [5/5], Batch [100/938], Loss: 0.0334\n",
      "Epoch [5/5], Batch [200/938], Loss: 0.0254\n",
      "Epoch [5/5], Batch [300/938], Loss: 0.0374\n",
      "Epoch [5/5], Batch [400/938], Loss: 0.0314\n",
      "Epoch [5/5], Batch [500/938], Loss: 0.0353\n",
      "Epoch [5/5], Batch [600/938], Loss: 0.0353\n",
      "Epoch [5/5], Batch [700/938], Loss: 0.0303\n",
      "Epoch [5/5], Batch [800/938], Loss: 0.0394\n",
      "Epoch [5/5], Batch [900/938], Loss: 0.0290\n",
      "모델 학습 완료.\n",
      "\n",
      "--- FP32 모델 평가 ---\n",
      "Accuracy: 99.02%\n",
      "FP32 모델 크기: 42.71 MB\n",
      "FX Graph 퓨징 완료.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FX Graph prepare 완료.\n",
      "\n",
      "--- Static Quantization Calibration 시작 ---\n",
      "  Calibration batch: 0/100\n",
      "  Calibration batch: 20/100\n",
      "  Calibration batch: 40/100\n",
      "  Calibration batch: 60/100\n",
      "  Calibration batch: 80/100\n",
      "Calibration 완료.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'1.layer1.0.conv1.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 75\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalibration 완료.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# 9. 양자화 적용 및 변환\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Removed 'inplace=False' as it's not accepted by convert_fx\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m quantized_model_static \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m quantized_model_static\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFX Graph convert 완료.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:615\u001b[0m, in \u001b[0;36mconvert_fx\u001b[1;34m(graph_module, convert_custom_config, _remove_qconfig, qconfig_mapping, backend_config, keep_original_weights)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Convert a calibrated or trained model to a quantized model\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m \n\u001b[0;32m    613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    614\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_api.quantize_fx.convert_fx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 615\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_fx\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:541\u001b[0m, in \u001b[0;36m_convert_fx\u001b[1;34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[0m\n\u001b[0;32m    534\u001b[0m preserved_attr_names \u001b[38;5;241m=\u001b[39m convert_custom_config\u001b[38;5;241m.\u001b[39mpreserved_attributes\n\u001b[0;32m    535\u001b[0m preserved_attrs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    536\u001b[0m     attr: \u001b[38;5;28mgetattr\u001b[39m(graph_module, attr)\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m preserved_attr_names\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graph_module, attr)\n\u001b[0;32m    539\u001b[0m }\n\u001b[1;32m--> 541\u001b[0m quantized \u001b[38;5;241m=\u001b[39m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_standalone_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_remove_qconfig_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_remove_qconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqconfig_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_decomposed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_decomposed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_original_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m attach_preserved_attrs_to_model(quantized, preserved_attrs)\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quantized\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\convert.py:1247\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(model, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag, qconfig_mapping, backend_config, is_decomposed, keep_original_weights)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;66;03m# TODO: maybe move this to quantize_fx.py\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_reference:\n\u001b[1;32m-> 1247\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mlower_to_fbgemm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_qconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_scope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;66;03m# TODO: this looks hacky, we want to check why we need this and see if we can\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;66;03m# remove this\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;66;03m# removes qconfig and activation_post_process modules\u001b[39;00m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _remove_qconfig_flag:\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\lower_to_fbgemm.py:19\u001b[0m, in \u001b[0;36mlower_to_fbgemm\u001b[1;34m(model, qconfig_map, node_name_to_scope, keep_original_weights)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlower_to_fbgemm\u001b[39m(\n\u001b[0;32m     11\u001b[0m     model: GraphModule,\n\u001b[0;32m     12\u001b[0m     qconfig_map: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, QConfigAny],\n\u001b[0;32m     13\u001b[0m     node_name_to_scope: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m]],\n\u001b[0;32m     14\u001b[0m     keep_original_weights: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GraphModule:\n\u001b[0;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lower a quantized reference model (with reference quantized operator patterns)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    to fbgemm\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lower_to_native_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqconfig_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_name_to_scope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_original_weights\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\_lower_to_native_backend.py:1339\u001b[0m, in \u001b[0;36m_lower_to_native_backend\u001b[1;34m(model, qconfig_map, node_name_to_scope, keep_original_weights)\u001b[0m\n\u001b[0;32m   1337\u001b[0m _lower_static_weighted_ref_module(model, qconfig_map)\n\u001b[0;32m   1338\u001b[0m _lower_static_weighted_ref_module_with_two_inputs(model, qconfig_map)\n\u001b[1;32m-> 1339\u001b[0m \u001b[43m_lower_dynamic_weighted_ref_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1340\u001b[0m _lower_weight_only_weighted_ref_module(model)\n\u001b[0;32m   1341\u001b[0m _lower_static_weighted_ref_functional(model, qconfig_map)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\torch_env\\lib\\site-packages\\torch\\ao\\quantization\\fx\\_lower_to_native_backend.py:809\u001b[0m, in \u001b[0;36m_lower_dynamic_weighted_ref_module\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    807\u001b[0m named_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(model\u001b[38;5;241m.\u001b[39mnamed_modules(remove_duplicate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mnodes:\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39mop \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_module\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[43mnamed_modules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\n\u001b[0;32m    810\u001b[0m         DYNAMIC_LOWER_MODULE_MAP\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m    811\u001b[0m     )\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(DYNAMIC_LOWER_FUSED_MODULE_MAP\u001b[38;5;241m.\u001b[39mkeys())):\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    813\u001b[0m     ref_node \u001b[38;5;241m=\u001b[39m n\n",
      "\u001b[1;31mKeyError\u001b[0m: '1.layer1.0.conv1.1'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, fuse_fx\n",
    "from torch.ao.quantization import get_default_qconfig_mapping\n",
    "# Add QuantStub and DeQuantStub imports for explicit control\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ... (rest of the initial setup, hyperparameters, data loaders, train_model, evaluate_model) ...\n",
    "\n",
    "# --- Main execution logic ---\n",
    "\n",
    "# 6. 모델 생성 및 초기 학습\n",
    "model_fp32 = resnet18(weights=None)\n",
    "\n",
    "model_fp32.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model_fp32.fc = nn.Linear(model_fp32.fc.in_features, 10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model_fp32 = model_fp32.to(DEVICE)\n",
    "train_model(model_fp32, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "print(\"\\n--- FP32 모델 평가 ---\")\n",
    "fp32_accuracy = evaluate_model(model_fp32, test_loader, target_device=DEVICE)\n",
    "torch.save(model_fp32.state_dict(), \"resnet18_mnist_fp32.pth\")\n",
    "fp32_size = os.path.getsize(\"resnet18_mnist_fp32.pth\") / (1024 * 1024)\n",
    "print(f\"FP32 모델 크기: {fp32_size:.2f} MB\")\n",
    "\n",
    "# --- Static Quantization (FX Graph 기반) ---\n",
    "\n",
    "# 7. Static Quantization 준비\n",
    "model_fp32.to(\"cpu\")\n",
    "model_fp32.eval()\n",
    "\n",
    "# Explicitly insert QuantStub and DeQuantStub\n",
    "# Wrap the model within Sequential with QuantStub and DeQuantStub\n",
    "# This makes the input/output quantization explicit for the tracer\n",
    "model_to_quantize = nn.Sequential(\n",
    "    QuantStub(),\n",
    "    model_fp32,\n",
    "    DeQuantStub()\n",
    ")\n",
    "\n",
    "# Now apply fuse_fx on this wrapped model\n",
    "fused_model = fuse_fx(model_to_quantize) # Apply fuse_fx on the wrapped model\n",
    "print(\"FX Graph 퓨징 완료.\")\n",
    "\n",
    "qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n",
    "\n",
    "example_input = torch.randn(1, 1, 28, 28)\n",
    "# Prepare_fx on the fused model with example inputs\n",
    "prepared_model = prepare_fx(fused_model, qconfig_mapping, example_inputs=(example_input,))\n",
    "print(\"FX Graph prepare 완료.\")\n",
    "\n",
    "# 8. Calibration\n",
    "print(\"\\n--- Static Quantization Calibration 시작 ---\")\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (inputs, labels) in enumerate(calibration_loader):\n",
    "        if batch_idx >= CALIBRATION_BATCHES:\n",
    "            break\n",
    "        inputs = inputs.to(\"cpu\")\n",
    "        prepared_model(inputs)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"  Calibration batch: {batch_idx}/{CALIBRATION_BATCHES}\")\n",
    "print(\"Calibration 완료.\")\n",
    "\n",
    "# 9. 양자화 적용 및 변환\n",
    "# Removed 'inplace=False' as it's not accepted by convert_fx\n",
    "quantized_model_static = convert_fx(prepared_model)\n",
    "quantized_model_static.eval()\n",
    "print(\"FX Graph convert 완료.\")\n",
    "\n",
    "# ... (rest of evaluation and saving) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "84883cff-461a-4fc0-aedd-574debe566d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 학습 시작 (5 에폭)...\n",
      "Epoch [1/5], Batch [100/938], Loss: 0.3495\n",
      "Epoch [1/5], Batch [200/938], Loss: 0.1813\n",
      "Epoch [1/5], Batch [300/938], Loss: 0.1525\n",
      "Epoch [1/5], Batch [400/938], Loss: 0.1040\n",
      "Epoch [1/5], Batch [500/938], Loss: 0.0855\n",
      "Epoch [1/5], Batch [600/938], Loss: 0.1114\n",
      "Epoch [1/5], Batch [700/938], Loss: 0.0730\n",
      "Epoch [1/5], Batch [800/938], Loss: 0.0933\n",
      "Epoch [1/5], Batch [900/938], Loss: 0.0741\n",
      "Epoch [2/5], Batch [100/938], Loss: 0.0656\n",
      "Epoch [2/5], Batch [200/938], Loss: 0.0621\n",
      "Epoch [2/5], Batch [300/938], Loss: 0.0572\n",
      "Epoch [2/5], Batch [400/938], Loss: 0.0707\n",
      "Epoch [2/5], Batch [500/938], Loss: 0.0584\n",
      "Epoch [2/5], Batch [600/938], Loss: 0.0604\n",
      "Epoch [2/5], Batch [700/938], Loss: 0.0545\n",
      "Epoch [2/5], Batch [800/938], Loss: 0.0552\n",
      "Epoch [2/5], Batch [900/938], Loss: 0.0576\n",
      "Epoch [3/5], Batch [100/938], Loss: 0.0520\n",
      "Epoch [3/5], Batch [200/938], Loss: 0.0345\n",
      "Epoch [3/5], Batch [300/938], Loss: 0.0380\n",
      "Epoch [3/5], Batch [400/938], Loss: 0.0398\n",
      "Epoch [3/5], Batch [500/938], Loss: 0.0446\n",
      "Epoch [3/5], Batch [600/938], Loss: 0.0322\n",
      "Epoch [3/5], Batch [700/938], Loss: 0.0633\n",
      "Epoch [3/5], Batch [800/938], Loss: 0.0424\n",
      "Epoch [3/5], Batch [900/938], Loss: 0.0517\n",
      "Epoch [4/5], Batch [100/938], Loss: 0.0348\n",
      "Epoch [4/5], Batch [200/938], Loss: 0.0364\n",
      "Epoch [4/5], Batch [300/938], Loss: 0.0358\n",
      "Epoch [4/5], Batch [400/938], Loss: 0.0414\n",
      "Epoch [4/5], Batch [500/938], Loss: 0.0401\n",
      "Epoch [4/5], Batch [600/938], Loss: 0.0318\n",
      "Epoch [4/5], Batch [700/938], Loss: 0.0415\n",
      "Epoch [4/5], Batch [800/938], Loss: 0.0325\n",
      "Epoch [4/5], Batch [900/938], Loss: 0.0400\n",
      "Epoch [5/5], Batch [100/938], Loss: 0.0346\n",
      "Epoch [5/5], Batch [200/938], Loss: 0.0355\n",
      "Epoch [5/5], Batch [300/938], Loss: 0.0299\n",
      "Epoch [5/5], Batch [400/938], Loss: 0.0276\n",
      "Epoch [5/5], Batch [500/938], Loss: 0.0369\n",
      "Epoch [5/5], Batch [600/938], Loss: 0.0279\n",
      "Epoch [5/5], Batch [700/938], Loss: 0.0305\n",
      "Epoch [5/5], Batch [800/938], Loss: 0.0356\n",
      "Epoch [5/5], Batch [900/938], Loss: 0.0293\n",
      "모델 학습 완료.\n",
      "\n",
      "--- FP32 모델 평가 ---\n",
      "Accuracy: 98.83%\n",
      "FP32 모델 크기: 42.71 MB\n",
      "FX Graph 퓨징 완료.\n",
      "QConfig Mapping: QConfigMapping (\n",
      " global_qconfig\n",
      "  QConfig(activation=functools.partial(functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, qscheme=torch.per_tensor_symmetric, dtype=torch.qint8){}, weight=functools.partial(functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){}, qscheme=torch.per_channel_symmetric, dtype=torch.qint8){})\n",
      " object_type_qconfigs\n",
      "  reshape: QConfig(activation=<class 'torch.ao.quantization.observer.ReuseInputObserver'>, weight=<class 'torch.ao.quantization.observer.NoopObserver'>)\n",
      "  <class 'torch.nn.modules.conv.ConvTranspose1d'>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <class 'torch.nn.modules.conv.ConvTranspose2d'>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <class 'torch.nn.modules.conv.ConvTranspose3d'>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <built-in method conv_transpose1d of type object at 0x00007FFE750C6450>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <built-in method conv_transpose2d of type object at 0x00007FFE750C6450>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <built-in method conv_transpose3d of type object at 0x00007FFE750C6450>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <function layer_norm at 0x000001FE421D39A0>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=<class 'torch.ao.quantization.observer.PlaceholderObserver'>)\n",
      "  <class 'torch.nn.modules.normalization.LayerNorm'>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=<class 'torch.ao.quantization.observer.PlaceholderObserver'>)\n",
      "  <class 'torch.nn.modules.activation.PReLU'>: QConfig(activation=<class 'torch.ao.quantization.observer.HistogramObserver'>, weight=<class 'torch.ao.quantization.observer.MinMaxObserver'>)\n",
      "  <class 'torch.nn.modules.activation.Hardsigmoid'>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.00390625, zero_point=0, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <function hardsigmoid at 0x000001FE421D32E0>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.00390625, zero_point=0, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  hardsigmoid: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.00390625, zero_point=0, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  hardsigmoid_: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.00390625, zero_point=0, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <class 'torch.nn.modules.activation.Sigmoid'>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.00390625, zero_point=0, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <built-in method sigmoid of type object at 0x00007FFE750C6450>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.00390625, zero_point=0, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  sigmoid: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.00390625, zero_point=0, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  sigmoid_: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.00390625, zero_point=0, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <class 'torch.nn.modules.activation.Softmax'>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.00390625, zero_point=0, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <class 'torch.nn.modules.activation.Tanh'>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.0078125, zero_point=128, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  <built-in method tanh of type object at 0x00007FFE750C6450>: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.0078125, zero_point=128, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  tanh: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.0078125, zero_point=128, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "  tanh_: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.FixedQParamsObserver'>, scale=0.0078125, zero_point=128, dtype=torch.quint8, quant_min=0, quant_max=255){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      " module_name_regex_qconfigs\n",
      "  OrderedDict()\n",
      " module_name_qconfigs\n",
      "  OrderedDict()\n",
      " module_name_object_type_order_qconfigs\n",
      "  OrderedDict()\n",
      ")\n",
      "FX Graph prepare 완료.\n",
      "\n",
      "--- Static Quantization Calibration 시작 ---\n",
      "  Calibration batch: 0/100\n",
      "  Calibration batch: 20/100\n",
      "  Calibration batch: 40/100\n",
      "  Calibration batch: 60/100\n",
      "  Calibration batch: 80/100\n",
      "Calibration 완료.\n",
      "FX Graph convert 완료.\n",
      "\n",
      "--- Quantized Static 모델 평가 ---\n",
      "Accuracy: 98.83%\n",
      "Quantized Static (FX) 모델 크기: 42.65 MB\n",
      "\n",
      "--- 최종 결과 비교 ---\n",
      "FP32 모델 정확도: 98.83% (크기: 42.71 MB)\n",
      "Static Quantized (FX) 모델 정확도: 98.83% (크기: 42.65 MB)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, fuse_fx\n",
    "# QConfig와 Observer를 직접 임포트하여 제어\n",
    "from torch.ao.quantization import get_default_qconfig_mapping, QuantStub, DeQuantStub, default_per_channel_weight_observer, default_observer\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 1. 하이퍼파라미터 및 설정\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "CALIBRATION_BATCHES = 100\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 3. 데이터 로더 준비 (MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "calibration_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 4. 모델 학습 함수\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    print(f\"\\n모델 학습 시작 ({num_epochs} 에폭)...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "    print(\"모델 학습 완료.\")\n",
    "\n",
    "# 5. 모델 평가 함수\n",
    "def evaluate_model(model, test_loader, target_device=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if target_device is None:\n",
    "        model_device = next(model.parameters()).device\n",
    "    else:\n",
    "        model_device = target_device\n",
    "\n",
    "    model.to(model_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(model_device), labels.to(model_device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# --- 메인 실행 로직 ---\n",
    "\n",
    "# 6. 모델 생성 및 초기 학습 (torchvision의 resnet18 사용)\n",
    "model_fp32 = resnet18(weights=None)\n",
    "\n",
    "model_fp32.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model_fp32.fc = nn.Linear(model_fp32.fc.in_features, 10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model_fp32 = model_fp32.to(DEVICE)\n",
    "train_model(model_fp32, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "print(\"\\n--- FP32 모델 평가 ---\")\n",
    "fp32_accuracy = evaluate_model(model_fp32, test_loader, target_device=DEVICE)\n",
    "torch.save(model_fp32.state_dict(), \"resnet18_mnist_fp32.pth\")\n",
    "fp32_size = os.path.getsize(\"resnet18_mnist_fp32.pth\") / (1024 * 1024)\n",
    "print(f\"FP32 모델 크기: {fp32_size:.2f} MB\")\n",
    "\n",
    "# --- Static Quantization (FX Graph 기반) ---\n",
    "\n",
    "# 7. Static Quantization 준비\n",
    "model_fp32.to(\"cpu\")\n",
    "model_fp32.eval()\n",
    "\n",
    "# Explicitly insert QuantStub and DeQuantStub\n",
    "# Wrap the model within Sequential with QuantStub and DeQuantStub\n",
    "model_to_quantize = nn.Sequential(\n",
    "    QuantStub(),\n",
    "    model_fp32,\n",
    "    DeQuantStub()\n",
    ")\n",
    "\n",
    "# 7-1. FX 그래프 기반 모델 퓨징\n",
    "fused_model = fuse_fx(model_to_quantize)\n",
    "print(\"FX Graph 퓨징 완료.\")\n",
    "\n",
    "# 7-2. QConfig 설정 (직접 정의하여 Static Quantization 유도)\n",
    "# fbgemm 백엔드에서 정적 양자화를 위한 QConfig\n",
    "# default_observer: 활성화 값 (Activation)의 min/max를 측정\n",
    "# default_per_channel_weight_observer: 가중치(Weight)의 per-channel min/max를 측정\n",
    "'''qconfig = torch.ao.quantization.QConfig(\n",
    "    activation=default_observer.with_args(qscheme=torch.per_tensor_symmetric, dtype=torch.qint8),\n",
    "    weight=default_per_channel_weight_observer.with_args(qscheme=torch.per_channel_symmetric, dtype=torch.qint8)\n",
    ")\n",
    "'''\n",
    "\n",
    "qconfig = torch.ao.quantization.QConfig(\n",
    "    activation=default_observer.with_args(qscheme=torch.per_tensor_symmetric, dtype=torch.qint8), # 활성화를 8비트 정수로\n",
    "    weight=default_per_channel_weight_observer.with_args(qscheme=torch.per_channel_symmetric, dtype=torch.qint8) # 가중치를 8비트 정수로\n",
    ")\n",
    "# QConfigMapping을 직접 생성하고 글로벌 QConfig 설정\n",
    "qconfig_mapping = get_default_qconfig_mapping()\n",
    "qconfig_mapping.set_global(qconfig)\n",
    "\n",
    "print(f\"QConfig Mapping: {qconfig_mapping}\")\n",
    "\n",
    "\n",
    "# 7-3. 모델 준비 (FX 그래프 기반)\n",
    "example_input = torch.randn(1, 1, 28, 28)\n",
    "prepared_model = prepare_fx(fused_model, qconfig_mapping, example_inputs=(example_input,))\n",
    "print(\"FX Graph prepare 완료.\")\n",
    "\n",
    "# 8. Calibration\n",
    "print(\"\\n--- Static Quantization Calibration 시작 ---\")\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (inputs, labels) in enumerate(calibration_loader):\n",
    "        if batch_idx >= CALIBRATION_BATCHES:\n",
    "            break\n",
    "        inputs = inputs.to(\"cpu\")\n",
    "        prepared_model(inputs)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"  Calibration batch: {batch_idx}/{CALIBRATION_BATCHES}\")\n",
    "print(\"Calibration 완료.\")\n",
    "\n",
    "# 9. 양자화 적용 및 변환\n",
    "quantized_model_static = convert_fx(prepared_model)\n",
    "quantized_model_static.eval()\n",
    "print(\"FX Graph convert 완료.\")\n",
    "\n",
    "# 10. 양자화된 모델 평가\n",
    "print(\"\\n--- Quantized Static 모델 평가 ---\")\n",
    "quantized_accuracy = evaluate_model(quantized_model_static, test_loader, target_device=torch.device(\"cpu\"))\n",
    "\n",
    "torch.save(quantized_model_static, \"resnet18_mnist_quantized_static_fx.pth\")\n",
    "quantized_size = os.path.getsize(\"resnet18_mnist_quantized_static_fx.pth\") / (1024 * 1024)\n",
    "print(f\"Quantized Static (FX) 모델 크기: {quantized_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n--- 최종 결과 비교 ---\")\n",
    "print(f\"FP32 모델 정확도: {fp32_accuracy:.2f}% (크기: {fp32_size:.2f} MB)\")\n",
    "print(f\"Static Quantized (FX) 모델 정확도: {quantized_accuracy:.2f}% (크기: {quantized_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "008de2c0-bf43-4594-8303-82a651c33e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 양자화된 모델 가중치 타입 확인 ---\n",
      "모듈: 1.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer1.0.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer1.0.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer1.1.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer1.1.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer2.0.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer2.0.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer2.0.downsample.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer2.1.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer2.1.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer3.0.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer3.0.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer3.0.downsample.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer3.1.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer3.1.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer4.0.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer4.0.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer4.0.downsample.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer4.1.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: 1.layer4.1.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: 1.fc, 가중치 Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 양자화된 모델 가중치 타입 확인 ---\")\n",
    "for name, module in quantized_model_static.named_modules():\n",
    "    # Conv2d 또는 Linear 레이어인 경우에만 확인\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        if hasattr(module, 'weight') and module.weight is not None:\n",
    "            print(f\"모듈: {name}, 가중치 Dtype: {module.weight.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c9c1272-22bf-4aa7-bd93-6793d4b3d453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 학습 시작 (5 에폭)...\n",
      "Epoch [1/5], Batch [100/938], Loss: 0.3552\n",
      "Epoch [1/5], Batch [200/938], Loss: 0.1721\n",
      "Epoch [1/5], Batch [300/938], Loss: 0.1421\n",
      "Epoch [1/5], Batch [400/938], Loss: 0.1134\n",
      "Epoch [1/5], Batch [500/938], Loss: 0.1022\n",
      "Epoch [1/5], Batch [600/938], Loss: 0.0996\n",
      "Epoch [1/5], Batch [700/938], Loss: 0.0804\n",
      "Epoch [1/5], Batch [800/938], Loss: 0.0923\n",
      "Epoch [1/5], Batch [900/938], Loss: 0.0831\n",
      "Epoch [2/5], Batch [100/938], Loss: 0.0600\n",
      "Epoch [2/5], Batch [200/938], Loss: 0.0682\n",
      "Epoch [2/5], Batch [300/938], Loss: 0.0645\n",
      "Epoch [2/5], Batch [400/938], Loss: 0.0646\n",
      "Epoch [2/5], Batch [500/938], Loss: 0.0557\n",
      "Epoch [2/5], Batch [600/938], Loss: 0.0633\n",
      "Epoch [2/5], Batch [700/938], Loss: 0.0584\n",
      "Epoch [2/5], Batch [800/938], Loss: 0.0504\n",
      "Epoch [2/5], Batch [900/938], Loss: 0.0542\n",
      "Epoch [3/5], Batch [100/938], Loss: 0.0523\n",
      "Epoch [3/5], Batch [200/938], Loss: 0.0402\n",
      "Epoch [3/5], Batch [300/938], Loss: 0.0640\n",
      "Epoch [3/5], Batch [400/938], Loss: 0.0421\n",
      "Epoch [3/5], Batch [500/938], Loss: 0.0416\n",
      "Epoch [3/5], Batch [600/938], Loss: 0.0531\n",
      "Epoch [3/5], Batch [700/938], Loss: 0.0399\n",
      "Epoch [3/5], Batch [800/938], Loss: 0.0513\n",
      "Epoch [3/5], Batch [900/938], Loss: 0.0402\n",
      "Epoch [4/5], Batch [100/938], Loss: 0.0319\n",
      "Epoch [4/5], Batch [200/938], Loss: 0.0396\n",
      "Epoch [4/5], Batch [300/938], Loss: 0.0357\n",
      "Epoch [4/5], Batch [400/938], Loss: 0.0333\n",
      "Epoch [4/5], Batch [500/938], Loss: 0.0400\n",
      "Epoch [4/5], Batch [600/938], Loss: 0.0408\n",
      "Epoch [4/5], Batch [700/938], Loss: 0.0443\n",
      "Epoch [4/5], Batch [800/938], Loss: 0.0332\n",
      "Epoch [4/5], Batch [900/938], Loss: 0.0245\n",
      "Epoch [5/5], Batch [100/938], Loss: 0.0306\n",
      "Epoch [5/5], Batch [200/938], Loss: 0.0256\n",
      "Epoch [5/5], Batch [300/938], Loss: 0.0499\n",
      "Epoch [5/5], Batch [400/938], Loss: 0.0403\n",
      "Epoch [5/5], Batch [500/938], Loss: 0.0396\n",
      "Epoch [5/5], Batch [600/938], Loss: 0.0290\n",
      "Epoch [5/5], Batch [700/938], Loss: 0.0357\n",
      "Epoch [5/5], Batch [800/938], Loss: 0.0420\n",
      "Epoch [5/5], Batch [900/938], Loss: 0.0306\n",
      "모델 학습 완료.\n",
      "\n",
      "--- FP32 모델 평가 ---\n",
      "Accuracy: 98.95%\n",
      "FP32 모델 크기: 42.71 MB\n",
      "FX Graph prepare 완료.\n",
      "\n",
      "--- Static Quantization Calibration 시작 ---\n",
      "  Calibration batch: 0/100\n",
      "  Calibration batch: 20/100\n",
      "  Calibration batch: 40/100\n",
      "  Calibration batch: 60/100\n",
      "  Calibration batch: 80/100\n",
      "Calibration 완료.\n",
      "FX Graph convert 완료.\n",
      "\n",
      "--- Quantized Static 모델 평가 ---\n",
      "Accuracy: 98.95%\n",
      "Quantized Static (FX) 모델 크기: 42.65 MB\n",
      "\n",
      "--- 최종 결과 비교 ---\n",
      "FP32 모델 정확도: 98.95% (크기: 42.71 MB)\n",
      "Static Quantized (FX) 모델 정확도: 98.95% (크기: 42.65 MB)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "# torchvision.models.quantization에서 resnet18_quantized를 임포트\n",
    "from torchvision.models.quantization import resnet18 as resnet18_q\n",
    "from torch.ao.quantization import get_default_qconfig_mapping, QuantStub, DeQuantStub\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 1. 하이퍼파라미터 및 설정\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "CALIBRATION_BATCHES = 100\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 3. 데이터 로더 준비 (MNIST)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "calibration_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 4. 모델 학습 함수\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    print(f\"\\n모델 학습 시작 ({num_epochs} 에폭)...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {running_loss/100:.4f}\")\n",
    "                running_loss = 0.0\n",
    "    print(\"모델 학습 완료.\")\n",
    "\n",
    "# 5. 모델 평가 함수\n",
    "def evaluate_model(model, test_loader, target_device=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if target_device is None:\n",
    "        model_device = next(model.parameters()).device\n",
    "    else:\n",
    "        model_device = target_device\n",
    "\n",
    "    model.to(model_device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(model_device), labels.to(model_device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# --- 메인 실행 로직 ---\n",
    "\n",
    "# 6. 모델 생성 및 초기 학습 (torchvision.models.quantization의 resnet18_quantized 사용)\n",
    "# quantize=True로 설정하면 양자화 준비된 모델을 로드합니다.\n",
    "# 이 모델은 이미 fuse_modules와 QuantStub/DeQuantStub이 적용되어 있습니다.\n",
    "model_fp32 = resnet18_q(weights=None, quantize=False) # 일단 FP32 버전으로 로드\n",
    "\n",
    "# conv1 레이어 수정 (1채널 입력에 맞게)\n",
    "# 이 모델은 이미 양자화 친화적으로 퓨징되어 있으므로,\n",
    "# conv1 수정 후 수동 퓨징이나 stub 추가는 필요 없습니다.\n",
    "model_fp32.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "model_fp32.fc = nn.Linear(model_fp32.fc.in_features, 10)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_fp32.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model_fp32 = model_fp32.to(DEVICE)\n",
    "train_model(model_fp32, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "\n",
    "print(\"\\n--- FP32 모델 평가 ---\")\n",
    "fp32_accuracy = evaluate_model(model_fp32, test_loader, target_device=DEVICE)\n",
    "torch.save(model_fp32.state_dict(), \"resnet18_mnist_fp32.pth\")\n",
    "fp32_size = os.path.getsize(\"resnet18_mnist_fp32.pth\") / (1024 * 1024)\n",
    "print(f\"FP32 모델 크기: {fp32_size:.2f} MB\")\n",
    "\n",
    "# --- Static Quantization (torchvision.models.quantization 방식 사용) ---\n",
    "\n",
    "# 7. Static Quantization 준비\n",
    "model_fp32.to(\"cpu\") # 모델을 CPU로 옮깁니다.\n",
    "model_fp32.eval()    # 평가 모드 설정\n",
    "\n",
    "# torchvision의 양자화 친화적 모델은 QConfig와 옵저버를 이미 내장하고 있습니다.\n",
    "# 우리는 모델을 로드할 때 이미 (weights=None, quantize=False)로 로드했으므로,\n",
    "# 이제 qconfig_mapping을 설정하고 prepare_fx를 호출합니다.\n",
    "\n",
    "# QConfig 설정 (CPU 최적화된 fbgemm 백엔드, 8비트 정수 명시)\n",
    "qconfig = torch.ao.quantization.QConfig(\n",
    "    activation=torch.ao.quantization.default_observer.with_args(qscheme=torch.per_tensor_symmetric, dtype=torch.qint8),\n",
    "    weight=torch.ao.quantization.default_per_channel_weight_observer.with_args(qscheme=torch.per_channel_symmetric, dtype=torch.qint8)\n",
    ")\n",
    "qconfig_mapping = get_default_qconfig_mapping()\n",
    "qconfig_mapping.set_global(qconfig)\n",
    "\n",
    "# prepare_fx는 자동으로 torchvision 모델의 내부 모듈을 퓨징합니다.\n",
    "# example_inputs는 prepare_fx에 여전히 필요합니다.\n",
    "example_input = torch.randn(1, 1, 28, 28)\n",
    "prepared_model = torch.ao.quantization.quantize_fx.prepare_fx(\n",
    "    model_fp32, # 이제 model_fp32 자체를 넘겨줍니다. (Sequential로 감싸지 않습니다)\n",
    "    qconfig_mapping,\n",
    "    example_inputs=(example_input,),\n",
    "    # inplace=False는 prepare_fx에서는 허용될 수 있지만, convert_fx에서는 불필요합니다.\n",
    "    # 여기서는 명시적으로 True/False 대신 기본값(False)을 사용하겠습니다.\n",
    ")\n",
    "print(\"FX Graph prepare 완료.\")\n",
    "\n",
    "# 8. Calibration\n",
    "print(\"\\n--- Static Quantization Calibration 시작 ---\")\n",
    "with torch.inference_mode():\n",
    "    for batch_idx, (inputs, labels) in enumerate(calibration_loader):\n",
    "        if batch_idx >= CALIBRATION_BATCHES:\n",
    "            break\n",
    "        inputs = inputs.to(\"cpu\")\n",
    "        prepared_model(inputs)\n",
    "        if batch_idx % 20 == 0:\n",
    "            print(f\"  Calibration batch: {batch_idx}/{CALIBRATION_BATCHES}\")\n",
    "print(\"Calibration 완료.\")\n",
    "\n",
    "# 9. 양자화 적용 및 변환\n",
    "# convert_fx는 'inplace' 인수를 받지 않습니다.\n",
    "quantized_model_static = torch.ao.quantization.quantize_fx.convert_fx(prepared_model)\n",
    "quantized_model_static.eval()\n",
    "print(\"FX Graph convert 완료.\")\n",
    "\n",
    "# 10. 양자화된 모델 평가\n",
    "print(\"\\n--- Quantized Static 모델 평가 ---\")\n",
    "quantized_accuracy = evaluate_model(quantized_model_static, test_loader, target_device=torch.device(\"cpu\"))\n",
    "\n",
    "# 양자화된 모델 저장 (전체 모델 객체 저장)\n",
    "torch.save(quantized_model_static, \"resnet18_mnist_quantized_static_fx.pth\")\n",
    "quantized_size = os.path.getsize(\"resnet18_mnist_quantized_static_fx.pth\") / (1024 * 1024)\n",
    "print(f\"Quantized Static (FX) 모델 크기: {quantized_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n--- 최종 결과 비교 ---\")\n",
    "print(f\"FP32 모델 정확도: {fp32_accuracy:.2f}% (크기: {fp32_size:.2f} MB)\")\n",
    "print(f\"Static Quantized (FX) 모델 정확도: {quantized_accuracy:.2f}% (크기: {quantized_size:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ea525bc-ea70-452a-a2f9-00282e67f8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 양자화된 모델 가중치 타입 확인 ---\n",
      "모듈: conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer1.0.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer1.0.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: layer1.1.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer1.1.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: layer2.0.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer2.0.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: layer2.0.downsample.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer2.1.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer2.1.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: layer3.0.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer3.0.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: layer3.0.downsample.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer3.1.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer3.1.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: layer4.0.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer4.0.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: layer4.0.downsample.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer4.1.conv1.0, 가중치 Dtype: torch.float32\n",
      "모듈: layer4.1.conv2, 가중치 Dtype: torch.float32\n",
      "모듈: fc, 가중치 Dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 양자화된 모델 가중치 타입 확인 ---\")\n",
    "for name, module in quantized_model_static.named_modules():\n",
    "    # Conv2d 또는 Linear 레이어인 경우에만 확인\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        if hasattr(module, 'weight') and module.weight is not None:\n",
    "            print(f\"모듈: {name}, 가중치 Dtype: {module.weight.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4171b17-63fb-4da8-b033-2d17c75f359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu128\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (torch_env)",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
