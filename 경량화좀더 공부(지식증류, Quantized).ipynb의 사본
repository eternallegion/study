{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1I8lORmrMLaH2uH3WxDvzG_oj3ozjiZDG","timestamp":1747754172624}],"gpuType":"T4","authorship_tag":"ABX9TyNg0HL8oe7I7GmYv4nBbgGF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VfutcaMG1GCo"},"outputs":[],"source":["# 전체 수정된 QAT 파이프라인 코드\n","\n","import os\n","import functools\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms, models\n","\n","# quantization 관련\n","import torch.ao.quantization as quant\n","from torch.ao.quantization import (\n","    QuantStub,\n","    DeQuantStub,\n","    prepare_qat,\n","    convert,\n","    observer\n",")\n","# *** 수정된 부분: FloatFunctional import 경로 ***\n","from torch.nn.quantized import FloatFunctional\n","\n","from torchvision.models.resnet import BasicBlock\n","from tqdm.notebook import tqdm\n","\n","# --- 1. 하이퍼파라미터 & 디바이스 설정 ---\n","batch_size  = 64\n","lr          = 1e-4\n","num_epoch   = 5    # FP32 학습 에폭\n","qat_epoch   = 5    # QAT 학습 에폭\n","save_path   = \"./qat_resnet18_model.pth\"\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# INT8 변환에 사용할 백엔드\n","torch.backends.quantized.engine = 'fbgemm'\n","\n","# --- 2. 데이터 준비 ---\n","transform_train = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n","])\n","transform_test = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n","])\n","\n","train_dataset = datasets.CIFAR10('./data', train=True,  download=True, transform=transform_train)\n","test_dataset  = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n","\n","# 연습용으로 subset\n","train_subset = torch.utils.data.Subset(train_dataset, torch.randperm(len(train_dataset))[:5000])\n","test_subset  = torch.utils.data.Subset(test_dataset,  torch.randperm(len(test_dataset))[:1000])\n","\n","train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True,  num_workers=2)\n","test_loader  = DataLoader(test_subset,  batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n","\n","\n","# --- 3. 모델 정의 Helpers ---\n","def get_resnet18(num_classes=10, pretrained=True):\n","    \"\"\"ResNet18 불러와서 마지막 FC 교체\"\"\"\n","    weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n","    model = models.resnet18(weights=weights)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    return model\n","\n","class QBasicBlock(nn.Module):\n","    \"\"\"BasicBlock에서 skip connection에 FloatFunctional을 쓰도록 변경\"\"\"\n","    def __init__(self, orig: BasicBlock):\n","        super().__init__()\n","        self.conv1      = orig.conv1\n","        self.bn1        = orig.bn1\n","        self.relu       = orig.relu\n","        self.conv2      = orig.conv2\n","        self.bn2        = orig.bn2\n","        self.downsample = orig.downsample\n","        self.skip_add   = FloatFunctional()\n","\n","    def forward(self, x):\n","        identity = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out = self.skip_add.add(out, identity)\n","        out = self.relu(out)\n","        return out\n","\n","class QuantizedResNet18(nn.Module):\n","    \"\"\"QuantStub, DeQuantStub 래핑 + fuse + BasicBlock 교체\"\"\"\n","    def __init__(self, fp32_model: nn.Module):\n","        super().__init__()\n","        self.model   = fp32_model\n","        self.quant   = QuantStub()\n","        self.dequant = DeQuantStub()\n","\n","        self.model.eval()\n","        self._fuse_modules()\n","        self._replace_basic_blocks()\n","        self.model.train()\n","\n","    def _fuse_modules(self):\n","        quant.fuse_modules(self.model, [['conv1','bn1','relu']], inplace=True)\n","        for module in self.model.modules():\n","            if isinstance(module, BasicBlock):\n","                quant.fuse_modules(module, [['conv1','bn1','relu']], inplace=True)\n","                quant.fuse_modules(module, [['conv2','bn2']],   inplace=True)\n","                if module.downsample is not None:\n","                    quant.fuse_modules(module.downsample, ['0','1'], inplace=True)\n","\n","    def _replace_basic_blocks(self):\n","        for name, child in list(self.model.named_children()):\n","            if isinstance(child, nn.Sequential):\n","                new_seq = []\n","                for blk in child:\n","                    if isinstance(blk, BasicBlock):\n","                        new_seq.append(QBasicBlock(blk))\n","                    else:\n","                        new_seq.append(blk)\n","                setattr(self.model, name, nn.Sequential(*new_seq))\n","\n","    def forward(self, x):\n","        x = self.quant(x)\n","        x = self.model(x)\n","        x = self.dequant(x)\n","        return x\n","\n","\n","# --- 4. 학습/평가 루프 ---\n","def train_mode(model, loader, criterion, optimizer, epochs, device, name=\"Model\"):\n","    model.train()\n","    print(f\"\\n--- {name} Training ---\")\n","    for epoch in range(epochs):\n","        total_loss, correct, total = 0.0, 0, 0\n","        for data, target in tqdm(loader, desc=f\"{name} Epoch {epoch+1}/{epochs}\"):\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            out = model(data)\n","            loss = criterion(out, target)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            preds = out.argmax(1)\n","            correct   += (preds == target).sum().item()\n","            total     += target.size(0)\n","\n","        print(f\"[{name}] Epoch {epoch+1}: \"\n","              f\"Loss={total_loss/len(loader):.4f}, \"\n","              f\"Acc={100*correct/total:.2f}%\")\n","    print(f\"{name} training done.\")\n","\n","def eval_mode(model, loader, device, name=\"Model\"):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for data, target in tqdm(loader, desc=f\"Evaluating {name}\"):\n","            data, target = data.to(device), target.to(device)\n","            out = model(data)\n","            preds = out.argmax(1)\n","            correct += (preds == target).sum().item()\n","            total   += target.size(0)\n","    acc = 100 * correct / total\n","    print(f\"[{name}] Test Accuracy: {acc:.2f}%\")\n","    return acc\n","\n","\n","# --- 5. FP32 학습 & 저장 ---\n","print(\"\\n=== FP32 ResNet18 ===\")\n","fp32_model   = get_resnet18().to(device)\n","criterion    = nn.CrossEntropyLoss()\n","optimizer_fp = optim.Adam(fp32_model.parameters(), lr=lr)\n","\n","train_mode(fp32_model, train_loader, criterion, optimizer_fp, num_epoch, device, name=\"FP32\")\n","fp32_acc = eval_mode(fp32_model, test_loader, device, name=\"FP32\")\n","\n","fp32_path = save_path.replace(\".pth\",\"_fp32.pth\")\n","torch.save(fp32_model.state_dict(), fp32_path)\n","print(f\"Saved FP32 → {fp32_path}\")\n","\n","\n","# --- 6. QAT 준비 & 학습 ---\n","print(\"\\n=== QAT ResNet18 ===\")\n","qat_fp32 = get_resnet18(pretrained=False)\n","qat_fp32.load_state_dict(torch.load(fp32_path, map_location='cpu'))\n","\n","qat_model = QuantizedResNet18(qat_fp32).to('cpu')\n","print(\"Fused+wrapped for QAT.\")\n","\n","qat_model.qconfig = quant.QConfig(\n","    activation=functools.partial(observer.HistogramObserver, reduce_range=True),\n","    weight=functools.partial(\n","        observer.PerChannelMinMaxObserver,\n","        dtype=torch.qint8,\n","        qscheme=torch.per_channel_symmetric\n","    )\n",")\n","print(\"QConfig:\", qat_model.qconfig)\n","\n","prepare_qat(qat_model, inplace=True)\n","print(\"Prepared for QAT.\")\n","\n","qat_model.to(device)\n","optimizer_qat = optim.Adam(qat_model.parameters(), lr=lr * 0.1)\n","train_mode(qat_model, train_loader, criterion, optimizer_qat, qat_epoch, device, name=\"QAT\")\n","\n","\n","# --- 7. INT8 변환 & 평가 (CPU) ---\n","print(\"\\n--- Converting to INT8 ---\")\n","qat_model.to('cpu')\n","quantized_model = convert(qat_model, inplace=True)\n","print(\"Converted to INT8.\")\n","\n","quant_acc = eval_mode(quantized_model, test_loader, torch.device('cpu'), name=\"Quantized\")\n","\n","\n","# --- 8. 저장 & 크기 비교 ---\n","torch.save(quantized_model.state_dict(), save_path)\n","print(f\"Saved Quantized → {save_path}\")\n","\n","fp32_sz  = os.path.getsize(fp32_path)/(1024**2)\n","quant_sz = os.path.getsize(save_path)/(1024**2)\n","print(f\"FP32 size: {fp32_sz:.2f} MB\")\n","print(f\"Quant size: {quant_sz:.2f} MB\")\n","print(f\"Size reduction: {fp32_sz/quant_sz:.2f}×\")\n","print(f\"Acc FP32: {fp32_acc:.2f}%, Quant: {quant_acc:.2f}%\")\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader\n","from tqdm.notebook import tqdm\n","import torch.nn.functional as F # KL_divergence에 필요\n","\n","import os\n","\n","# --- 1. 하이퍼파라미터 및 장치 설정 ---\n","batch_size = 64\n","lr_student = 0.001 # 학생 모델 학습률\n","num_epoch = 10 # 선생님 모델 학습 에폭 (이미 학습된 모델 사용 시 생략 가능)\n","distill_epoch = 20 # 지식 증류 학습 에폭\n","\n","temperature = 3.0 # 지식 증류 온도 (T)\n","alpha = 0.5 # 하드 타겟 손실과 증류 손실 간의 가중치 (0~1 사이)\n","\n","num_classes = 10 # CIFAR-10 클래스 수\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# --- 2. 데이터 준비 ---\n","transform_train = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","])\n","\n","train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n","test_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n","\n","# 학습 속도를 위해 데이터셋 서브셋 사용 (선택 사항)\n","subset_size_train = 5000\n","subset_size_test = 1000\n","\n","train_subset_indices = torch.randperm(len(train_dataset))[:subset_size_train]\n","test_subset_indices = torch.randperm(len(test_dataset))[:subset_size_test]\n","\n","train_subset = torch.utils.data.Subset(train_dataset, train_subset_indices)\n","test_subset = torch.utils.data.Subset(test_dataset, test_subset_indices)\n","\n","train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n","test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","print(f\"Train batches (subset): {len(train_loader)}, Test batches (subset): {len(test_loader)}\")\n","\n","# --- 3. 모델 정의 ---\n","\n","# 선생님 모델 (ResNet34 또는 ResNet50 추천)\n","def get_teacher_model(num_classes=10, pretrained=True):\n","    # ResNet34를 선생님 모델로 사용 (ResNet50도 가능)\n","    model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1 if pretrained else None)\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    return model\n","\n","# 학생 모델 (ResNet18)\n","def get_student_model(num_classes=10, pretrained=False): # 학생 모델은 보통 처음부터 학습\n","    model = models.resnet18(weights=None) # pretrained=False로 시작\n","    model.fc = nn.Linear(model.fc.in_features, num_classes)\n","    return model\n","\n","print(\"Model definition functions defined.\")\n","\n","# --- 4. 학습 및 평가 함수 ---\n","def train_model(model, loader, criterion, optimizer, epochs, device, name=\"Model\"):\n","    model.train()\n","    print(f\"\\n--- {name} Training ---\")\n","    for epoch in range(epochs):\n","        total_loss, correct, total = 0.0, 0, 0\n","        for data, target in tqdm(loader, desc=f\"{name} Epoch {epoch+1}/{epochs}\"):\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad()\n","            out = model(data)\n","            loss = criterion(out, target)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            preds = out.argmax(1)\n","            correct += (preds == target).sum().item()\n","            total += target.size(0)\n","\n","        print(f\"[{name}] Epoch {epoch+1}: Loss={total_loss/len(loader):.4f}, Acc={100*correct/total:.2f}%\")\n","    print(f\"{name} training done.\")\n","\n","def evaluate_model(model, loader, device, name=\"Model\"):\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for data, target in tqdm(loader, desc=f\"Evaluating {name}\"):\n","            data, target = data.to(device), target.to(device)\n","            out = model(data)\n","            preds = out.argmax(1)\n","            correct += (preds == target).sum().item()\n","            total += target.size(0)\n","    acc = 100 * correct / total\n","    print(f\"[{name}] Test Accuracy: {acc:.2f}%\")\n","    return acc\n","\n","print(\"Training and evaluation functions defined.\")\n","\n","# --- 5. 선생님 모델 학습 및 평가 ---\n","# (참고: 실제 프로젝트에서는 이미 학습된 강력한 선생님 모델을 사용하거나,\n","# 훨씬 더 긴 에폭으로 선생님 모델을 먼저 충분히 학습시킵니다.)\n","print(\"\\n=== Teacher Model (ResNet34) Training ===\")\n","teacher_model = get_teacher_model(num_classes=num_classes, pretrained=True).to(device) # ImageNet 사전 학습 사용\n","criterion_teacher = nn.CrossEntropyLoss()\n","optimizer_teacher = optim.Adam(teacher_model.parameters(), lr=lr_student) # 선생님 학습률은 학생과 다를 수 있음\n","\n","# 선생님 모델 학습 (짧게만 돌림, 실제로는 더 길게)\n","print(\"Note: Teacher model training is brief for demonstration. For real use, train sufficiently.\")\n","train_model(teacher_model, train_loader, criterion_teacher, optimizer_teacher, num_epoch, device, name=\"Teacher\")\n","teacher_acc = evaluate_model(teacher_model, test_loader, device, name=\"Teacher\")\n","\n","# --- 6. 학생 모델 (ResNet18) 지식 증류 학습 ---\n","print(\"\\n=== Student Model (ResNet18) Knowledge Distillation Training ===\")\n","student_model = get_student_model(num_classes=num_classes, pretrained=False).to(device) # 학생 모델은 새로 시작\n","\n","criterion_hard_target = nn.CrossEntropyLoss() # 실제 레이블에 대한 손실\n","optimizer_student = optim.Adam(student_model.parameters(), lr=lr_student)\n","\n","student_model.train()\n","teacher_model.eval() # 선생님 모델은 평가 모드로 고정\n","\n","print(f\"\\n--- Distillation Training for {distill_epoch} Epochs ---\")\n","for epoch in range(distill_epoch):\n","    total_loss, correct, total = 0.0, 0, 0\n","    for data, target in tqdm(train_loader, desc=f\"Distill Epoch {epoch+1}/{distill_epoch}\"):\n","        data, target = data.to(device), target.to(device)\n","\n","        optimizer_student.zero_grad()\n","\n","        # 1. 선생님 모델의 로짓 얻기 (soft target)\n","        with torch.no_grad(): # 선생님 모델은 학습시키지 않으므로 no_grad\n","            teacher_logits = teacher_model(data)\n","\n","        # 2. 학생 모델의 로짓 얻기\n","        student_logits = student_model(data)\n","\n","        # 3. 하드 타겟 손실 계산 (CrossEntropyLoss)\n","        hard_target_loss = criterion_hard_target(student_logits, target)\n","\n","        # 4. 지식 증류 손실 계산 (KL Divergence)\n","        # 로짓에 온도를 적용하여 소프트맥스 취한 후 KL Divergence 계산\n","        distillation_loss = F.kl_div(\n","            F.log_softmax(student_logits / temperature, dim=1),\n","            F.softmax(teacher_logits / temperature, dim=1),\n","            reduction='batchmean'\n","        ) * (temperature * temperature) # 논문에서 제안된 스케일링\n","\n","        # 5. 전체 손실 = alpha * 하드 타겟 손실 + (1-alpha) * 증류 손실\n","        loss = alpha * hard_target_loss + (1 - alpha) * distillation_loss\n","\n","        loss.backward()\n","        optimizer_student.step()\n","\n","        total_loss += loss.item()\n","        preds = student_logits.argmax(1)\n","        correct += (preds == target).sum().item()\n","        total += target.size(0)\n","\n","    print(f\"[Student Distill] Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, Acc={100*correct/total:.2f}%\")\n","\n","print(\"Knowledge Distillation Training finished!\")\n","\n","# --- 7. 지식 증류된 학생 모델 평가 ---\n","print(\"\\n=== Evaluate Distilled Student Model (ResNet18) ===\")\n","distilled_student_acc = evaluate_model(student_model, test_loader, device, name=\"Distilled Student\")\n","\n","# --- 8. 증류 없이 학습된 학생 모델 (비교용) 학습 및 평가 ---\n","# 지식 증류 없이 ResNet18을 직접 학습시켜서 성능 비교\n","print(\"\\n=== Baseline Student Model (ResNet18) Training (without Distillation) ===\")\n","baseline_student_model = get_student_model(num_classes=num_classes, pretrained=False).to(device)\n","criterion_baseline = nn.CrossEntropyLoss()\n","optimizer_baseline = optim.Adam(baseline_student_model.parameters(), lr=lr_student)\n","\n","train_model(baseline_student_model, train_loader, criterion_baseline, optimizer_baseline, distill_epoch, device, name=\"Baseline Student\")\n","baseline_student_acc = evaluate_model(baseline_student_model, test_loader, device, name=\"Baseline Student\")\n","\n","\n","# --- 9. 최종 결과 요약 ---\n","print(\"\\n--- Summary of Results ---\")\n","print(f\"Teacher Model (ResNet34) Accuracy: {teacher_acc:.2f}%\")\n","print(f\"Baseline Student Model (ResNet18) Accuracy (No Distillation): {baseline_student_acc:.2f}%\")\n","print(f\"Distilled Student Model (ResNet18) Accuracy: {distilled_student_acc:.2f}%\")\n","\n","# 모델 크기 비교 (옵션)\n","def get_model_size_mb(model):\n","    torch.save(model.state_dict(), \"temp_model.pth\")\n","    size_mb = os.path.getsize(\"temp_model.pth\") / (1024 * 1024)\n","    os.remove(\"temp_model.pth\")\n","    return size_mb\n","\n","print(f\"\\n--- Model Size Comparison (MB) ---\")\n","print(f\"Teacher Model (ResNet34) Size: {get_model_size_mb(teacher_model):.2f} MB\")\n","print(f\"Student Model (ResNet18) Size: {get_model_size_mb(student_model):.2f} MB\")"],"metadata":{"id":"0EGxxUye1mG7"},"execution_count":null,"outputs":[]}]}