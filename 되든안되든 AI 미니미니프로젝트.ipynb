{
 "cells": [
  {
   "cell_type": "raw",
   "id": "23004856-f5bd-48ca-98dd-a7e979570990",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "1975593a-abdc-446d-8871-e84edf2618d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader as loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "aa90ea88-9372-4a28-b1a9-c9c747103c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.cv1=nn.Conv2d(3,32,kernel_size=3,stride=1, padding=1)\n",
    "        self.cv2=nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.fc1=nn.Linear(64*8*8,128)\n",
    "        self.fc2=nn.Linear(128,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.cv1(x))\n",
    "        x=self.pool(x)\n",
    "        x=F.relu(self.cv2(x))\n",
    "        x=self.pool(x)\n",
    "        x=x.view(-1,64*8*8)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c7f78e3f-dae1-4027-a9c8-242d9f20fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "# 사전 학습된 ResNet18 불러오기\n",
    "model = models.resnet18(pretrained=True)\n",
    "# 마지막 Fully Connected 레이어 수정 (ImageNet의 1000 클래스 → CIFAR-10의 10개 클래스)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "903d0696-971c-4983-a343-cfce9cad564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode(model, train, optim, criter, num_epoch):\n",
    "    model.train()\n",
    "    total_L=0.0\n",
    "    ave_loss=0.0\n",
    "    for epoch in range(num_epoch):\n",
    "        for data,target in train:\n",
    "            optim.zero_grad()\n",
    "            out=model(data)\n",
    "            loss=criter(out,target)\n",
    "            optim.step()\n",
    "            total_L+=loss.item()\n",
    "        avg_loss=loss/len(train)\n",
    "        print(f\"Epoch: {epoch+1}/{num_epoch}, Train Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "dea58ea0-647d-4037-af40-40943a5dc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr=trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    bs=trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "\n",
    "    trin_loader=loader(train_data,batch_size=bs,shuffle=True)\n",
    "    test_loader=loader(test_data,batch_size=bs,shuffle=True)\n",
    "\n",
    "    \n",
    "    model=simple_CNN()\n",
    "    criter=nn.CrossEntropyLoss()\n",
    "    optimy=optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "    train_mode(model,train_loader, criter, optimz, num_epoch)\n",
    "    test_acc=eval_mode(model, test_loader)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8c9d083c-b673-4f45-941b-8109fbaa7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mode(model,test_loader):\n",
    "    model.eval()\n",
    "    correct=0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            out=model(data)\n",
    "            pred=out.argmax(dim=1)\n",
    "            correct+=pred.eq(target).sum().item()\n",
    "            total+=data.size(0)\n",
    "\n",
    "    accuracy=100.0*correct/total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "d795eb7a-e29b-445b-b041-c86bfcb0ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "train_data=datasets.CIFAR10(root=\"./data\",train=True, download=True, transform=transform)\n",
    "test_data=datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader=loader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader=loader(test_data, batch_size=1000, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "9327d029-bc15-49f1-ad1a-806639794f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n",
      "////////////////////////////////\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(\"////////////////////////////////\")\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f1b3c46f-e015-46d3-aadb-0bbc9a20b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model()\n",
    "criter=nn.CrossEntropyLoss()\n",
    "optimy=optim.Adam(model.parameters(),lr=0.001)\n",
    "num_epoch=10\n",
    "\n",
    "train_mode(model, train_loader, optimy, criter, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "e0312adc-87e6-432d-8ed3-04ecc71bbc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train LR = 0.001, batch_size = 32\n",
      "Epoch: 1/10, Train Loss: 0.0015\n",
      "Epoch: 2/10, Train Loss: 0.0015\n",
      "Epoch: 3/10, Train Loss: 0.0015\n",
      "Epoch: 4/10, Train Loss: 0.0015\n",
      "Epoch: 5/10, Train Loss: 0.0015\n",
      "Epoch: 6/10, Train Loss: 0.0015\n",
      "Epoch: 7/10, Train Loss: 0.0015\n",
      "Epoch: 8/10, Train Loss: 0.0015\n",
      "Epoch: 9/10, Train Loss: 0.0015\n",
      "Epoch: 10/10, Train Loss: 0.0015\n",
      "ACCURACY: 10.04%\n",
      "\n",
      "Train LR = 0.001, batch_size = 64\n",
      "Epoch: 1/10, Train Loss: 0.0030\n",
      "Epoch: 2/10, Train Loss: 0.0029\n",
      "Epoch: 3/10, Train Loss: 0.0030\n",
      "Epoch: 4/10, Train Loss: 0.0029\n",
      "Epoch: 5/10, Train Loss: 0.0029\n",
      "Epoch: 6/10, Train Loss: 0.0029\n",
      "Epoch: 7/10, Train Loss: 0.0030\n",
      "Epoch: 8/10, Train Loss: 0.0029\n",
      "Epoch: 9/10, Train Loss: 0.0029\n",
      "Epoch: 10/10, Train Loss: 0.0029\n",
      "ACCURACY: 9.90%\n",
      "\n",
      "Train LR = 0.001, batch_size = 128\n",
      "Epoch: 1/10, Train Loss: 0.0059\n",
      "Epoch: 2/10, Train Loss: 0.0059\n",
      "Epoch: 3/10, Train Loss: 0.0059\n",
      "Epoch: 4/10, Train Loss: 0.0059\n",
      "Epoch: 5/10, Train Loss: 0.0059\n",
      "Epoch: 6/10, Train Loss: 0.0059\n",
      "Epoch: 7/10, Train Loss: 0.0059\n",
      "Epoch: 8/10, Train Loss: 0.0059\n",
      "Epoch: 9/10, Train Loss: 0.0059\n",
      "Epoch: 10/10, Train Loss: 0.0059\n",
      "ACCURACY: 6.41%\n",
      "\n",
      "Train LR = 0.005, batch_size = 32\n",
      "Epoch: 1/10, Train Loss: 0.0015\n",
      "Epoch: 2/10, Train Loss: 0.0015\n",
      "Epoch: 3/10, Train Loss: 0.0015\n",
      "Epoch: 4/10, Train Loss: 0.0015\n",
      "Epoch: 5/10, Train Loss: 0.0015\n",
      "Epoch: 6/10, Train Loss: 0.0015\n",
      "Epoch: 7/10, Train Loss: 0.0015\n",
      "Epoch: 8/10, Train Loss: 0.0014\n",
      "Epoch: 9/10, Train Loss: 0.0014\n",
      "Epoch: 10/10, Train Loss: 0.0015\n",
      "ACCURACY: 10.08%\n",
      "\n",
      "Train LR = 0.005, batch_size = 64\n",
      "Epoch: 1/10, Train Loss: 0.0030\n",
      "Epoch: 2/10, Train Loss: 0.0030\n",
      "Epoch: 3/10, Train Loss: 0.0029\n",
      "Epoch: 4/10, Train Loss: 0.0029\n",
      "Epoch: 5/10, Train Loss: 0.0029\n",
      "Epoch: 6/10, Train Loss: 0.0029\n",
      "Epoch: 7/10, Train Loss: 0.0030\n",
      "Epoch: 8/10, Train Loss: 0.0030\n",
      "Epoch: 9/10, Train Loss: 0.0029\n",
      "Epoch: 10/10, Train Loss: 0.0030\n",
      "ACCURACY: 13.04%\n",
      "\n",
      "Train LR = 0.005, batch_size = 128\n",
      "Epoch: 1/10, Train Loss: 0.0059\n",
      "Epoch: 2/10, Train Loss: 0.0059\n",
      "Epoch: 3/10, Train Loss: 0.0059\n",
      "Epoch: 4/10, Train Loss: 0.0059\n",
      "Epoch: 5/10, Train Loss: 0.0059\n",
      "Epoch: 6/10, Train Loss: 0.0059\n",
      "Epoch: 7/10, Train Loss: 0.0059\n",
      "Epoch: 8/10, Train Loss: 0.0059\n",
      "Epoch: 9/10, Train Loss: 0.0060\n",
      "Epoch: 10/10, Train Loss: 0.0059\n",
      "ACCURACY: 9.73%\n",
      "\n",
      "Train LR = 0.0001, batch_size = 32\n",
      "Epoch: 1/10, Train Loss: 0.0015\n",
      "Epoch: 2/10, Train Loss: 0.0015\n",
      "Epoch: 3/10, Train Loss: 0.0015\n",
      "Epoch: 4/10, Train Loss: 0.0015\n",
      "Epoch: 5/10, Train Loss: 0.0015\n",
      "Epoch: 6/10, Train Loss: 0.0015\n",
      "Epoch: 7/10, Train Loss: 0.0015\n",
      "Epoch: 8/10, Train Loss: 0.0015\n",
      "Epoch: 9/10, Train Loss: 0.0015\n",
      "Epoch: 10/10, Train Loss: 0.0015\n",
      "ACCURACY: 9.58%\n",
      "\n",
      "Train LR = 0.0001, batch_size = 64\n",
      "Epoch: 1/10, Train Loss: 0.0030\n",
      "Epoch: 2/10, Train Loss: 0.0029\n",
      "Epoch: 3/10, Train Loss: 0.0029\n",
      "Epoch: 4/10, Train Loss: 0.0030\n",
      "Epoch: 5/10, Train Loss: 0.0029\n",
      "Epoch: 6/10, Train Loss: 0.0029\n",
      "Epoch: 7/10, Train Loss: 0.0029\n",
      "Epoch: 8/10, Train Loss: 0.0030\n",
      "Epoch: 9/10, Train Loss: 0.0029\n",
      "Epoch: 10/10, Train Loss: 0.0029\n",
      "ACCURACY: 9.46%\n",
      "\n",
      "Train LR = 0.0001, batch_size = 128\n",
      "Epoch: 1/10, Train Loss: 0.0059\n",
      "Epoch: 2/10, Train Loss: 0.0059\n",
      "Epoch: 3/10, Train Loss: 0.0059\n",
      "Epoch: 4/10, Train Loss: 0.0059\n",
      "Epoch: 5/10, Train Loss: 0.0059\n",
      "Epoch: 6/10, Train Loss: 0.0059\n",
      "Epoch: 7/10, Train Loss: 0.0059\n",
      "Epoch: 8/10, Train Loss: 0.0059\n",
      "Epoch: 9/10, Train Loss: 0.0059\n",
      "Epoch: 10/10, Train Loss: 0.0058\n",
      "ACCURACY: 9.08%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#하이퍼파라미터 튜닝\n",
    "import itertools\n",
    "\n",
    "lrs = [0.001, 0.005, 0.0001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "result = {}\n",
    "\n",
    "for lr_val, bs in itertools.product(lrs, batch_sizes):\n",
    "    print(f\"Train LR = {lr_val}, batch_size = {bs}\")\n",
    "    \n",
    "    # 모델 재초기화: 각 실험마다 모델을 새롭게 초기화하는 것이 좋습니다.\n",
    "    model = Model()  # 혹은 simple_CNN() 등 초기 모델을 재생성합니다.\n",
    "\n",
    "    # DataLoader도 새롭게 구성합니다.\n",
    "    train_loader = loader(train_data, batch_size=bs, shuffle=True)\n",
    "    test_loader = loader(test_data, batch_size=1000, shuffle=True)\n",
    "    \n",
    "    # 옵티마이저 생성 시, 현재 lr_val 사용\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr_val)\n",
    "    \n",
    "    # 학습 함수 호출: 함수의 인자 이름과 전달하는 변수 이름이 일치해야 함\n",
    "    train_mode(model, train_loader, optimy, criter, num_epoch)\n",
    "    \n",
    "    acc = eval_mode(model, test_loader)\n",
    "    result[(lr_val, bs)] = acc\n",
    "    print(f\"ACCURACY: {acc:.2f}%\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "bd602348-0e95-491b-a95b-8891f4701f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 9.2100%\n"
     ]
    }
   ],
   "source": [
    "#train_mode(model, train_loader, criter, optimy, num_epoch)\n",
    "eval=eval_mode(model, test_loader)\n",
    "print(f\"test acc: {eval:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1061d2-8989-4c14-9b0d-ef48452a9b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da11c48f-b101-4c63-8cac-d40262cbd07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b36bd-030f-485c-a10f-157b51fecb92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219422f-ed60-49a9-93c2-47c7c1537317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b2065-b483-4a29-9bfa-be6fe119f27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "690fc459-a261-40f3-9353-bfe08d725c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mode(model, test):\n",
    "    model.eval()\n",
    "    total=0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for data,target in test:\n",
    "            out=model(data)\n",
    "            pred=out.argmax(dim=0)\n",
    "            correct+=pred.eq(target).sum().item()\n",
    "            total+=data.size(0)\n",
    "    accuracy=100.0*correct/total\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e29e6-0b76-4dc0-bae1-f0fe5f492118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "bb7b83e4-7046-416f-837f-dbe9fcb70fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImprovedCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (gap): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        # 첫 번째 Convolution + BatchNorm\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # 두 번째 Convolution + BatchNorm\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # 세 번째 Convolution + BatchNorm 추가\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # MaxPooling 레이어\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Global Average Pooling: 입력 feature map을 각 채널의 평균으로 만듭니다.\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 출력층: GAP 후의 feature의 차원은 128\n",
    "        self.fc = nn.Linear(128, 10)\n",
    "        \n",
    "        # Dropout (과적합 방지)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 3, 32, 32) CIFAR-10 이미지\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # → (batch_size, 32, 32, 32)\n",
    "        x = self.pool(x)                     # → (batch_size, 32, 16, 16)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))  # → (batch_size, 64, 16, 16)\n",
    "        x = self.pool(x)                     # → (batch_size, 64, 8, 8)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))  # → (batch_size, 128, 8, 8)\n",
    "        # Global Average Pooling: 각 채널 별로 8x8을 평균 → (batch_size, 128, 1, 1)\n",
    "        x = self.gap(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)            # flatten → (batch_size, 128)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)                       # → (batch_size, 10)\n",
    "        return x\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "model_improved = ImprovedCNN()\n",
    "print(model_improved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "55e11862-7b9d-46cb-9445-2e48da04f942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10, Train Loss: 0.0059\n",
      "Epoch: 2/10, Train Loss: 0.0059\n",
      "Epoch: 3/10, Train Loss: 0.0059\n",
      "Epoch: 4/10, Train Loss: 0.0059\n",
      "Epoch: 5/10, Train Loss: 0.0059\n",
      "Epoch: 6/10, Train Loss: 0.0059\n",
      "Epoch: 7/10, Train Loss: 0.0059\n",
      "Epoch: 8/10, Train Loss: 0.0059\n",
      "Epoch: 9/10, Train Loss: 0.0059\n",
      "Epoch: 10/10, Train Loss: 0.0059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0059, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criter=nn.CrossEntropyLoss()\n",
    "optimy=optim.Adam(model.parameters(),lr=0.001)\n",
    "num_epoch=10\n",
    "\n",
    "train_mode(model, train_loader, optimy, criter, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d286fd63-9882-4ea0-99bb-e4e8b92724e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc: 9.0800%\n"
     ]
    }
   ],
   "source": [
    "eval=eval_mode(model, test_loader)\n",
    "print(f\"test acc: {eval:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983f508-9608-4ec0-a352-e36628fad8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "모델 최적화 및 해석 (Explainable AI), 생성 모델 (Generative Models), 강화학습 (Reinforcement Learning), 딥러닝 배포 및 서비스화"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
