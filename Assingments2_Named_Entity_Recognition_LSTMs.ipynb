{"cells":[{"cell_type":"markdown","metadata":{"id":"A6czvz5VKO5M"},"source":["# Notebook for Programming in Problem 2\n"]},{"cell_type":"markdown","metadata":{"id":"5o8HI5JqTvU5"},"source":["## Learning Objectives\n","In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the provided dataset 'eng.train' and 'eng.val'."]},{"cell_type":"markdown","metadata":{"id":"ObrHyvWvTyGZ"},"source":["## Writing Code\n","Look for the keyword \"TODO\" and fill in your code in the empty space.\n","Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6YTnpgbFdMI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714225657046,"user_tz":-540,"elapsed":684,"user":{"displayName":"조재혁","userId":"14419522891410237672"}},"outputId":"658996fb-d864-453a-9450-116e945493f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Apr 27 13:47:36 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi # you may need to try reconnecting to get a T4 gpu"]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":77},"id":"HeVw7ykcexV6","executionInfo":{"status":"ok","timestamp":1714225749215,"user_tz":-540,"elapsed":38992,"user":{"displayName":"조재혁","userId":"14419522891410237672"}},"outputId":"b68cd78f-a854-4279-ac41-53cbec087fd3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-a0f06abe-3a28-4f2f-8655-72790801b7d6\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-a0f06abe-3a28-4f2f-8655-72790801b7d6\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving eng.train to eng.train\n"]}]},{"cell_type":"markdown","metadata":{"id":"tnYMKJlKNXYe"},"source":["## Installing PyTorch and Other Packages\n","\n","Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dRVuiP_JVdT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714225830537,"user_tz":-540,"elapsed":81326,"user":{"displayName":"조재혁","userId":"14419522891410237672"}},"outputId":"1ae73e55-0d25-4c81-94b9-e29e842a8980"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n","Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"]}],"source":["!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"]},{"cell_type":"markdown","metadata":{"id":"TPsFH637OpLy"},"source":["Test if our installation works:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c62StNb2NvKk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714225833854,"user_tz":-540,"elapsed":3324,"user":{"displayName":"조재혁","userId":"14419522891410237672"}},"outputId":"5aada3c3-b537-436e-e281-13bc42ac6498"},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch successfully installed!\n","Version: 2.2.1+cu121\n"]}],"source":["import torch\n","\n","# Multiply two matrices on GPU\n","a = torch.rand(100, 200).cuda()\n","b = torch.rand(200, 100).cuda()\n","c = torch.matmul(a, b)\n","\n","print(\"PyTorch successfully installed!\")\n","print(\"Version:\", torch.__version__)"]},{"cell_type":"markdown","metadata":{"id":"1qaC8sxcqkGX"},"source":["Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5Y2xB_uqqM9","executionInfo":{"status":"ok","timestamp":1714225848572,"user_tz":-540,"elapsed":14719,"user":{"displayName":"조재혁","userId":"14419522891410237672"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8e0308e-066e-4c73-9e23-23b8329dc563"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Collecting scikit-learn\n","  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n","Installing collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","Successfully installed scikit-learn-1.4.2\n"]}],"source":["!pip install -U scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"bhV4CYivRbt4"},"source":["Let's import all the packages at once:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjRM4cCFRh-d"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchtext.vocab import Vocab, vocab\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","import re\n","from collections import Counter\n","from typing import List, Tuple, Dict, Optional, Any"]},{"cell_type":"markdown","metadata":{"id":"Yn1bIPjAN-9V"},"source":["## Long Short Term Memory (LSTM)"]},{"cell_type":"markdown","metadata":{"id":"sJOKIneRTrTH"},"source":["### Data Loading\n","\n","We will use the provided datasets for named entity recognition ('eng.train' & 'eng.val'). Take a look at the first 50 lines:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWqz7kDxSqeb","executionInfo":{"status":"ok","timestamp":1714225850301,"user_tz":-540,"elapsed":10,"user":{"displayName":"조재혁","userId":"14419522891410237672"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e9370cba-6767-4fa7-8c02-62982b6bd275"},"outputs":[{"output_type":"stream","name":"stdout","text":["EU NNP I-NP ORG\n","rejects VBZ I-VP O\n","German JJ I-NP MISC\n","call NN I-NP O\n","to TO I-VP O\n","boycott VB I-VP O\n","British JJ I-NP MISC\n","lamb NN I-NP O\n",". . O O\n","\n","Peter NNP I-NP PER\n","Blackburn NNP I-NP PER\n","\n","BRUSSELS NNP I-NP LOC\n","1996-08-22 CD I-NP O\n","\n","The DT I-NP O\n","European NNP I-NP ORG\n","Commission NNP I-NP ORG\n","said VBD I-VP O\n","on IN I-PP O\n","Thursday NNP I-NP O\n","it PRP B-NP O\n","disagreed VBD I-VP O\n","with IN I-PP O\n","German JJ I-NP MISC\n","advice NN I-NP O\n","to TO I-PP O\n","consumers NNS I-NP O\n","to TO I-VP O\n","shun VB I-VP O\n","British JJ I-NP MISC\n","lamb NN I-NP O\n","until IN I-SBAR O\n","scientists NNS I-NP O\n","determine VBP I-VP O\n","whether IN I-SBAR O\n","mad JJ I-NP O\n","cow NN I-NP O\n","disease NN I-NP O\n","can MD I-VP O\n","be VB I-VP O\n","transmitted VBN I-VP O\n","to TO I-PP O\n","sheep NN I-NP O\n",". . O O\n","\n","Germany NNP I-NP LOC\n","'s POS B-NP O\n","representative NN I-NP O\n"]}],"source":["!cat eng.train | head -n 50"]},{"cell_type":"markdown","metadata":{"id":"YVt1a6nzWsiF"},"source":["Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnNfOBUYJvVW"},"outputs":[],"source":["# A sentence is a list of (word, tag) tuples.\n","# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n","Sentence = List[Tuple[str, str]]\n","\n","\n","def read_data_file(\n","    datapath: str,\n",") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n","    \"\"\"\n","    Read and preprocess input data from the file `datapath`.\n","    Example:\n","    ```\n","        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n","    ```\n","    Return values:\n","        `sentences`: a list of sentences, including words and NER tags\n","        `word_cnt`: a Counter object, the number of occurrences of each word\n","        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n","    \"\"\"\n","    sentences: List[Sentence] = []\n","    word_cnt: Dict[str, int] = Counter()\n","    tag_cnt: Dict[str, int] = Counter()\n","\n","    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n","        if \"DOCSTART\" in sentence_txt:\n","            # Ignore dummy sentences at the begining of each document.\n","            continue\n","        # Read a new sentence\n","        sentences.append([])\n","        for token in sentence_txt.split(\"\\n\"):\n","            w, _, _, t = token.split()\n","            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n","            w = re.sub(\"\\d\", \"0\", w)\n","            word_cnt[w] += 1\n","            tag_cnt[t] += 1\n","            sentences[-1].append((w, t))\n","\n","    return sentences, word_cnt, tag_cnt\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLMGYSZ7KxzP"},"outputs":[],"source":["# Some helper code\n","def get_device() -> torch.device:\n","    \"\"\"\n","    Use GPU when it is available; use CPU otherwise.\n","    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n","    \"\"\"\n","    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVHAOb7iMPwC"},"outputs":[],"source":["def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n","    \"\"\"\n","    Calculate various evaluation metrics such as accuracy and F1 score\n","    Parameters:\n","        `ground_truth`: the list of ground truth NER tags\n","        `predictions`: the list of predicted NER tags\n","    \"\"\"\n","    f1_scores = f1_score(ground_truth, predictions, average=None)\n","    return {\n","        \"accuracy\": accuracy_score(ground_truth, predictions),\n","        \"f1\": f1_scores,\n","        \"average f1\": np.mean(f1_scores),\n","        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n","    }"]},{"cell_type":"markdown","metadata":{"id":"7s830dhbnj1L"},"source":["## Long Short-term Memory (LSTM)\n","\n","Now we implement an one-layer LSTM for the NER task."]},{"cell_type":"markdown","metadata":{"id":"to7DnWNiY5ZS"},"source":["### Data Loading **(4 points)**\n","\n","We first implement the data loader. In the provided datasets, each data example is a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5oVgqE7JaJp"},"outputs":[],"source":["# 3 sentences with different lengths\n","sentence_1 = torch.tensor([6, 1, 2])\n","sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n","sentence_3 = torch.tensor([3, 4])\n","# Form a batch by padding 0\n","sentence_batch = torch.tensor([\n","    [6, 1, 2, 0, 0],\n","    [4, 2, 7, 7, 9],\n","    [3, 4, 0, 0, 0],\n","])"]},{"cell_type":"markdown","metadata":{"id":"udC0SMjkKaCN"},"source":["We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sACcGN4XYMgj"},"outputs":[],"source":["class SequenceDataset(Dataset):\n","    \"\"\"\n","    Each data example is a sentence, including its words and NER tags.\n","    \"\"\"\n","\n","    def __init__(\n","        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n","    ) -> None:\n","        \"\"\"\n","        Initialize the dataset by reading from datapath.\n","        \"\"\"\n","        super().__init__()\n","        self.sentences: List[Sentence] = []\n","        UNKNOWN = \"<UNKNOWN>\"\n","        PAD = \"<PAD>\"  # Special token used for padding\n","\n","        print(\"Loading data from %s\" % datapath)\n","        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n","        print(\"%d sentences loaded.\" % len(self.sentences))\n","\n","        if words_vocab is None:\n","            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n","            words_vocab.set_default_index(words_vocab[UNKNOWN])\n","\n","        self.words_vocab = words_vocab\n","\n","        self.unknown_idx = self.words_vocab[UNKNOWN]\n","        self.pad_idx = self.words_vocab[PAD]\n","\n","        if tags_vocab is None:\n","            tags_vocab = vocab(tag_cnt, specials=[])\n","        self.tags_vocab = tags_vocab\n","\n","    def __getitem__(self, idx: int) -> Sentence:\n","        \"\"\"\n","        Get the idx'th sentence in the dataset.\n","        \"\"\"\n","        return self.sentences[idx]\n","\n","    def __len__(self) -> int:\n","        \"\"\"\n","        Return the number of sentences in the dataset.\n","        \"\"\"\n","        # TODO: Implement this method\n","        # START HERE\n","        return len(self.sentences)\n","        #raise NotImplementedError\n","        # END\n","\n","    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n","        \"\"\"\n","        A customized function for batching a number of sentences together.\n","        Different sentences have different lengths. Let max_len be the longest length.\n","        When packing them into one tensor, we need to pad all sentences to max_len.\n","        Return values:\n","            `words`: a list in which each element itself is a list of words in a sentence\n","            `word_idxs`: a batch_size x max_len tensor.\n","                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n","            `tags`: a list in which each element itself is a list of tags in a sentence\n","            `tag_idxs`: a batch_size x max_len tensor\n","                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n","            `valid_mask`: a batch_size x max_len tensor\n","                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n","                        Otherwise, valid[i][j] is False.\n","        \"\"\"\n","        words: List[List[str]] = []\n","        tags: List[List[str]] = []\n","        max_len = -1  # length of the longest sentence\n","        for sent in sentences:\n","            words.append([])\n","            tags.append([])\n","            for w, t in sent:\n","                words[-1].append(w)\n","                tags[-1].append(t)\n","            max_len = max(max_len, len(words[-1]))\n","\n","        batch_size = len(sentences)\n","        word_idxs = torch.full(\n","            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n","        )\n","        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n","        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n","\n","        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n","        ## Caveat: There may be out-of-vocabulary words in validation data\n","        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n","        ## START HERE\n","        for i, sent in enumerate(sentences):\n","            for j, (word, tag) in enumerate(sent):\n","                if word in self.words_vocab:\n","                    word_index = self.words_vocab[word]\n","                else:\n","                    word_index = self.unknown_idx\n","\n","        tag_index = self.tags_vocab[tag]  # 태그 인덱스는 이미 존재한다고 가정\n","\n","        word_idxs[i, j] = word_index\n","        tag_idxs[i, j] = tag_index\n","        valid_mask[i, j] = True\n","\n","\n","        #raise NotImplementedError\n","\n","        # END\n","\n","        return {\n","            \"words\": words,\n","            \"word_idxs\": word_idxs,\n","            \"tags\": tags,\n","            \"tag_idxs\": tag_idxs,\n","            \"valid_mask\": valid_mask,\n","        }\n","\n","\n","def create_sequence_dataloaders(\n","    batch_size: int, shuffle: bool = True\n",") -> Tuple[DataLoader, DataLoader, Vocab]:\n","    \"\"\"\n","    Create the dataloaders for training and validaiton.\n","    \"\"\"\n","    ds_train = SequenceDataset(\"eng.train\")\n","    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n","    loader_train = DataLoader(\n","        ds_train,\n","        batch_size,\n","        shuffle,\n","        collate_fn=ds_train.form_batch,  # customized function for batching\n","        drop_last=True,\n","        pin_memory=True,\n","    )\n","    loader_val = DataLoader(\n","        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n","    )\n","    return loader_train, loader_val, ds_train"]},{"cell_type":"markdown","metadata":{"id":"E2EcVxYuYvGv"},"source":["Here is a simple sanity-check. Try to understand its output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TazmodGWYx2d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714225852940,"user_tz":-540,"elapsed":2642,"user":{"displayName":"조재혁","userId":"14419522891410237672"}},"outputId":"18c3345d-095c-44bf-a3ec-ffc50df23410"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data from eng.train\n","14041 sentences loaded.\n","Loading data from eng.val\n","3490 sentences loaded.\n","Iterating on the training data..\n","{'words': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '0000-00-00']], 'word_idxs': tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [ 0,  0,  0,  0,  0,  0,  0,  0,  0],\n","        [ 0, 14,  0,  0,  0,  0,  0,  0,  0]]), 'tags': [['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'], ['PER', 'PER'], ['LOC', 'O']], 'tag_idxs': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[False, False, False, False, False, False, False, False, False],\n","        [False, False, False, False, False, False, False, False, False],\n","        [False,  True, False, False, False, False, False, False, False]])}\n","Done!\n"]}],"source":["def check_sequence_dataloader() -> None:\n","    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n","    print(\"Iterating on the training data..\")\n","    for i, data_batch in enumerate(loader_train):\n","        if i == 0:\n","            print(data_batch)\n","    print(\"Done!\")\n","\n","\n","check_sequence_dataloader()"]},{"cell_type":"markdown","metadata":{"id":"ifk3i-obY8YB"},"source":["### Implement the Model **(8 points)**\n","\n","Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3V0NvQynZF8e"},"outputs":[],"source":["class LSTM(nn.Module):\n","    \"\"\"\n","    Long short-term memory for NER\n","    \"\"\"\n","\n","    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n","        \"\"\"\n","        Initialize an LSTM\n","        Parameters:\n","            `words_vocab`: vocabulary of words\n","            `tags_vocab`: vocabulary of tags\n","            `d_emb`: dimension of word embeddings (D)\n","            `d_hidden`: dimension of the hidden layer (H)\n","            `bidirectional`: true if LSTM should be bidirectional\n","        \"\"\"\n","        super().__init__()\n","        # TODO: Create the word embeddings (nn.Embedding),\n","        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n","        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n","        #       Note: Pay attention to the LSTM output shapes!\n","        # START HERE\n","        self.embedding = nn.Embedding(num_embeddings=len(words_vocab), embedding_dim=d_emb)\n","        self.lstm = nn.LSTM(input_size=d_emb, hidden_size=d_hidden, bidirectional=bidirectional, batch_first=True)\n","        multiplier = 2 if bidirectional else 1\n","        self.output_layer = nn.Linear(d_hidden * multiplier, len(tags_vocab))\n","\n","\n","        #raise NotImplementedError\n","\n","        # END\n","\n","    def forward(\n","        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n","    ) -> torch.Tensor:\n","        \"\"\"\n","        Given words in sentences, predict the logits of the NER tag.\n","        Parameters:\n","            `word_idxs`: a batch_size x max_len tensor\n","            `valid_mask`: a batch_size x max_len tensor\n","        Return values:\n","            `logits`: a batch_size x max_len x 5 tensor\n","        \"\"\"\n","        # TODO: Implement the forward pass\n","        # START HERE\n","        embeddings = self.embedding(word_idxs)\n","        lstm_output, _ = self.lstm(embeddings)\n","        logits = self.output_layer(lstm_output)\n","\n","        #raise NotImplementedError\n","\n","        # END\n","        return logits"]},{"cell_type":"markdown","metadata":{"id":"2BFTKaB4Zydx"},"source":["We do a sanity-check by loading a batch of data examples and pass it through the network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKg1ni4QZ6D1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714225854802,"user_tz":-540,"elapsed":1865,"user":{"displayName":"조재혁","userId":"14419522891410237672"}},"outputId":"734d4658-9d9c-4f2a-df61-f8b95bd4f3bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data from eng.train\n","14041 sentences loaded.\n","Loading data from eng.val\n","3490 sentences loaded.\n","LSTM(\n","  (embedding): Embedding(20102, 64)\n","  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n","  (output_layer): Linear(in_features=256, out_features=5, bias=True)\n",")\n","Input word_idxs shape: torch.Size([4, 20])\n","Input valid_mask shape: torch.Size([4, 20])\n","Output logits shape: torch.Size([4, 20, 5])\n"]}],"source":["def check_lstm() -> None:\n","    # Hyperparameters\n","    batch_size = 4\n","    d_emb = 64\n","    d_hidden = 128\n","    bidirectional = True\n","    # Create the dataloaders and the model\n","    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n","    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n","    device = get_device()\n","    model.to(device)\n","    print(model)\n","    # Get the first batch\n","    data_batch = next(iter(loader_train))\n","    # Move data to GPU\n","    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n","    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n","    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n","    # Calculate the model\n","    print(\"Input word_idxs shape:\", word_idxs.size())\n","    print(\"Input valid_mask shape:\", valid_mask.size())\n","    logits = model(word_idxs, valid_mask)\n","    print(\"Output logits shape:\", logits.size())\n","\n","\n","check_lstm()"]},{"cell_type":"markdown","metadata":{"id":"jddDYUiLY-hc"},"source":["### Training and Validation **(6 points)**\n","\n","Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hv_15mnXZ_dy"},"outputs":[],"source":["def train_lstm(\n","    model: nn.Module,\n","    loader: DataLoader,\n","    optimizer: optim.Optimizer,\n","    device: torch.device,\n","    silent: bool = False,  # whether to print the training loss\n",") -> Tuple[float, Dict[str, Any]]:\n","    \"\"\"\n","    Train the LSTM model.\n","    Return values:\n","        1. the average training loss\n","        2. training metrics such as accuracy and F1 score\n","    \"\"\"\n","    model.train()\n","    ground_truth = []\n","    predictions = []\n","    losses = []\n","    report_interval = 100\n","\n","    for i, data_batch in enumerate(loader):\n","        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n","        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n","        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n","\n","        # TODO:\n","        # 1. Perform the forward pass to calculate the model's output. Save it to the variable \"logits\".\n","        # 2. Calculate the loss using the output and the ground truth tags. Save it to the variable \"loss\".\n","        # 3. Perform the backward pass to calculate the gradient.\n","        # 4. Use the optimizer to update model parameters.\n","        # Caveat:\n","        # 1. You may need to call optimizer.zero_grad(). Figure out what it does!\n","        # 2. When calculating the loss, you should only consider positions where valid_mask == True\n","        # START HERE\n","\n","        # logits = ...\n","        # loss = ...\n","        optimizer.zero_grad()  # 루프 시작에서 그라디언트를 초기화\n","\n","        logits = model(word_idxs, valid_mask)  # 모델의 forward pass 실행\n","        loss_fn = nn.CrossEntropyLoss()  # Loss function 정의\n","\n","    # logits에서 valid_mask가 True인 위치만 손실을 계산합니다.\n","        active_logits = logits.view(-1, logits.shape[-1])[valid_mask.view(-1)]\n","        active_labels = tag_idxs.view(-1)[valid_mask.view(-1)]\n","        loss = loss_fn(active_logits, active_labels)\n","\n","        loss.backward()  # 그라디언트 계산\n","        optimizer.step()  # 파라미터 업데이트\n","\n","        losses.append(loss.item())\n","        #raise NotImplementedError\n","        # END\n","\n","       # optimizer.zero_grad()\n","       # loss.backward()\n","       # optimizer.step()\n","\n","       # losses.append(loss.item())\n","\n","        # we get (unmasked) predictions by getting argmax of logits along last dimension (You will need to define logits!)\n","        net_predictions = torch.argmax(logits, -1)\n","\n","        # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n","        tag_idxs_flat = tag_idxs.flatten()\n","        valid_mask_flat = valid_mask.flatten()\n","        net_predictions_flat = net_predictions.flatten()\n","\n","        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n","        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n","\n","        if not silent and i > 0 and i % report_interval == 0:\n","            print(\n","                \"\\t[%06d/%06d] Loss: %f\"\n","                % (i, len(loader), np.mean(losses[-report_interval:]))\n","            )\n","\n","    return np.mean(losses), eval_metrics(ground_truth, predictions)\n","\n","\n","def validate_lstm(\n","    model: nn.Module, loader: DataLoader, device: torch.device\n",") -> Tuple[float, Dict[str, Any]]:\n","    \"\"\"\n","    Validate the model.\n","    Return the validation loss and metrics.\n","    \"\"\"\n","    model.eval()\n","    ground_truth = []\n","    predictions = []\n","    losses = []\n","\n","    with torch.no_grad():\n","\n","        for data_batch in loader:\n","            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n","            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n","            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n","\n","            # TODO: Similar to what you did in train_lstm, but only step 1 and 2.\n","            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n","\n","            # START HERE\n","            logits = model(word_idxs, valid_mask)\n","            loss_fn = nn.CrossEntropyLoss()\n","            loss = loss_fn(logits[valid_mask], tag_idxs[valid_mask])\n","\n","            #raise NotImplementedError\n","            # END\n","\n","            losses.append(loss.item())\n","\n","            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n","            net_predictions = torch.argmax(logits, -1)\n","\n","            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n","            tag_idxs_flat = tag_idxs.flatten()\n","            valid_mask_flat = valid_mask.flatten()\n","            net_predictions_flat = net_predictions.flatten()\n","\n","            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n","            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n","\n","    return np.mean(losses), eval_metrics(ground_truth, predictions)\n","\n","\n","def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n","    \"\"\"\n","    Train and validate the LSTM model for a number of epochs.\n","    \"\"\"\n","    print(\"Hyperparameters:\", hyperparams)\n","    # Create the dataloaders\n","    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n","        hyperparams[\"batch_size\"]\n","    )\n","    # Create the model\n","    model = LSTM(\n","        ds_train.words_vocab,\n","        ds_train.tags_vocab,\n","        hyperparams[\"d_emb\"],\n","        hyperparams[\"d_hidden\"],\n","        hyperparams[\"bidirectional\"],\n","    )\n","    device = get_device()\n","    model.to(device)\n","    print(model)\n","    # Create the optimizer\n","    optimizer = optim.RMSprop(\n","        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n","    )\n","\n","    # Train and validate\n","    for i in range(hyperparams[\"num_epochs\"]):\n","        print(\"Epoch #%d\" % i)\n","\n","        print(\"Training..\")\n","        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n","        print(\"Training loss: \", loss_train)\n","        print(\"Training metrics:\")\n","        for k, v in metrics_train.items():\n","            print(\"\\t\", k, \": \", v)\n","\n","        print(\"Validating..\")\n","        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n","        print(\"Validation loss: \", loss_val)\n","        print(\"Validation metrics:\")\n","        for k, v in metrics_val.items():\n","            print(\"\\t\", k, \": \", v)\n","\n","    print(\"Done!\")"]},{"cell_type":"markdown","metadata":{"id":"rU9Nef7yal_M"},"source":["Run the experiment:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pFxQxlokai6Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714225887253,"user_tz":-540,"elapsed":32455,"user":{"displayName":"조재혁","userId":"14419522891410237672"}},"outputId":"ef187ab3-31a8-4970-f947-09c1f26dc832"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n","Loading data from eng.train\n","14041 sentences loaded.\n","Loading data from eng.val\n","3490 sentences loaded.\n","LSTM(\n","  (embedding): Embedding(20102, 64)\n","  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n","  (output_layer): Linear(in_features=256, out_features=5, bias=True)\n",")\n","Epoch #0\n","Training..\n","Training loss:  0.7123185556773495\n","Training metrics:\n","\t accuracy :  0.8888888888888888\n","\t f1 :  [0.         0.94117647]\n","\t average f1 :  0.47058823529411764\n","\t confusion matrix :  [[ 0  1]\n"," [ 2 24]]\n","Validating..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss:  0.002816062154514449\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #1\n","Training..\n","Training loss:  0.48893464070365383\n","Training metrics:\n","\t accuracy :  0.9259259259259259\n","\t f1 :  [0.         0.96153846 0.        ]\n","\t average f1 :  0.32051282051282054\n","\t confusion matrix :  [[ 0  1  0]\n"," [ 0 25  0]\n"," [ 0  1  0]]\n","Validating..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss:  0.04941164780341621\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #2\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.019813422262948868\n","Training metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[27]]\n","Validating..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss:  0.0017896266487826193\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #3\n","Training..\n","Training loss:  0.4520933657237307\n","Training metrics:\n","\t accuracy :  0.9629629629629629\n","\t f1 :  [0.98113208 0.        ]\n","\t average f1 :  0.49056603773584906\n","\t confusion matrix :  [[26  0]\n"," [ 1  0]]\n","Validating..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss:  0.03235872997902334\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #4\n","Training..\n","Training loss:  0.2520942707819308\n","Training metrics:\n","\t accuracy :  0.9259259259259259\n","\t f1 :  [0.96153846 0.        ]\n","\t average f1 :  0.4807692307692308\n","\t confusion matrix :  [[25  0]\n"," [ 2  0]]\n","Validating..\n","Validation loss:  0.013597824288548768\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #5\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.5503559703631455\n","Training metrics:\n","\t accuracy :  0.8518518518518519\n","\t f1 :  [0.   0.92]\n","\t average f1 :  0.46\n","\t confusion matrix :  [[ 0  3]\n"," [ 1 23]]\n","Validating..\n","Validation loss:  0.11522120769534792\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #6\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.03819554168041105\n","Training metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[27]]\n","Validating..\n","Validation loss:  0.019315345379124795\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #7\n","Training..\n","Training loss:  0.5739511761779862\n","Training metrics:\n","\t accuracy :  0.8518518518518519\n","\t f1 :  [0.92 0.   0.  ]\n","\t average f1 :  0.3066666666666667\n","\t confusion matrix :  [[23  0  0]\n"," [ 2  0  0]\n"," [ 2  0  0]]\n","Validating..\n","Validation loss:  0.3186390135171158\n","Validation metrics:\n","\t accuracy :  0.8571428571428571\n","\t f1 :  [0.92307692 0.        ]\n","\t average f1 :  0.46153846153846156\n","\t confusion matrix :  [[6 1]\n"," [0 0]]\n","Epoch #8\n","Training..\n","Training loss:  0.2835827549678032\n","Training metrics:\n","\t accuracy :  0.9259259259259259\n","\t f1 :  [0.96153846 0.         0.        ]\n","\t average f1 :  0.32051282051282054\n","\t confusion matrix :  [[25  1  0]\n"," [ 0  0  0]\n"," [ 1  0  0]]\n","Validating..\n","Validation loss:  0.11917173589712807\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #9\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.23535532042970536\n","Training metrics:\n","\t accuracy :  0.9259259259259259\n","\t f1 :  [0.         0.96153846 0.        ]\n","\t average f1 :  0.32051282051282054\n","\t confusion matrix :  [[ 0  1  0]\n"," [ 0 25  0]\n"," [ 0  1  0]]\n","Validating..\n","Validation loss:  0.06467152914098863\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #10\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.37165711213283764\n","Training metrics:\n","\t accuracy :  0.8888888888888888\n","\t f1 :  [0.         0.94117647 0.         0.        ]\n","\t average f1 :  0.23529411764705882\n","\t confusion matrix :  [[ 0  1  0  0]\n"," [ 0 24  0  0]\n"," [ 0  1  0  0]\n"," [ 0  1  0  0]]\n","Validating..\n","Validation loss:  0.04785650804738647\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #11\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.2613508609916877\n","Training metrics:\n","\t accuracy :  0.9259259259259259\n","\t f1 :  [0.96153846 0.        ]\n","\t average f1 :  0.4807692307692308\n","\t confusion matrix :  [[25  0]\n"," [ 2  0]]\n","Validating..\n","Validation loss:  0.1605704044257956\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #12\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.24266288394574076\n","Training metrics:\n","\t accuracy :  0.9259259259259259\n","\t f1 :  [0.         0.96153846 0.        ]\n","\t average f1 :  0.32051282051282054\n","\t confusion matrix :  [[ 0  1  0]\n"," [ 0 25  0]\n"," [ 0  1  0]]\n","Validating..\n","Validation loss:  0.10206735215615481\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #13\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.048072458697586425\n","Training metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[27]]\n","Validating..\n","Validation loss:  0.03445148276348066\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #14\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.016939781794024857\n","Training metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[27]]\n","Validating..\n","Validation loss:  0.005680834618812826\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Done!\n"]}],"source":["train_val_loop_lstm({\n","    \"bidirectional\": True,\n","    \"batch_size\": 512,\n","    \"d_emb\": 64,\n","    \"d_hidden\": 128,\n","    \"num_epochs\": 15,\n","    \"learning_rate\": 0.005,\n","    \"l2\": 1e-6,\n","})"]},{"cell_type":"markdown","metadata":{"id":"6vA-Yjqg7n0V"},"source":["We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7wNrdvJ98ARB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714225906072,"user_tz":-540,"elapsed":18842,"user":{"displayName":"조재혁","userId":"14419522891410237672"}},"outputId":"4a03bb77-6586-4f1b-b181-eafa9c542102"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n","Loading data from eng.train\n","14041 sentences loaded.\n","Loading data from eng.val\n","3490 sentences loaded.\n","LSTM(\n","  (embedding): Embedding(20102, 64)\n","  (lstm): LSTM(64, 128, batch_first=True)\n","  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",")\n","Epoch #0\n","Training..\n","Training loss:  0.059087134210804595\n","Training metrics:\n","\t accuracy :  0.9629629629629629\n","\t f1 :  [0.98113208 0.        ]\n","\t average f1 :  0.49056603773584906\n","\t confusion matrix :  [[26  1]\n"," [ 0  0]]\n","Validating..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss:  4.216505320593049e-05\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #1\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  3.2702539640552086e-05\n","Training metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[27]]\n","Validating..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss:  3.162397271288293e-05\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #2\n","Training..\n","Training loss:  0.7023946157592205\n","Training metrics:\n","\t accuracy :  0.8888888888888888\n","\t f1 :  [0.94117647 0.        ]\n","\t average f1 :  0.47058823529411764\n","\t confusion matrix :  [[24  1]\n"," [ 2  0]]\n","Validating..\n","Validation loss:  0.010129862864102637\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #3\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.41778207506501563\n","Training metrics:\n","\t accuracy :  0.9629629629629629\n","\t f1 :  [0.98113208 0.        ]\n","\t average f1 :  0.49056603773584906\n","\t confusion matrix :  [[26  0]\n"," [ 1  0]]\n","Validating..\n","Validation loss:  0.010919632789279734\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #4\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.48477579173804436\n","Training metrics:\n","\t accuracy :  0.8518518518518519\n","\t f1 :  [0.92 0.   0.  ]\n","\t average f1 :  0.3066666666666667\n","\t confusion matrix :  [[23  1  1]\n"," [ 1  0  0]\n"," [ 1  0  0]]\n","Validating..\n","Validation loss:  0.015364285758031266\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #5\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.007181624524053876\n","Training metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[27]]\n","Validating..\n","Validation loss:  0.005337836603367967\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #6\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.04380131339335262\n","Training metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[27]]\n","Validating..\n","Validation loss:  0.002145271365796881\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #7\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.0015579101961554476\n","Training metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[27]]\n","Validating..\n","Validation loss:  0.0013560026626302196\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #8\n","Training..\n","Training loss:  0.48228275112018715\n","Training metrics:\n","\t accuracy :  0.9259259259259259\n","\t f1 :  [0.96153846 0.         0.        ]\n","\t average f1 :  0.32051282051282054\n","\t confusion matrix :  [[25  0  0]\n"," [ 1  0  0]\n"," [ 1  0  0]]\n","Validating..\n","Validation loss:  0.1119755446644766\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #9\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.4267598062546717\n","Training metrics:\n","\t accuracy :  0.9259259259259259\n","\t f1 :  [0.96153846 0.         0.        ]\n","\t average f1 :  0.32051282051282054\n","\t confusion matrix :  [[25  0  0]\n"," [ 1  0  0]\n"," [ 1  0  0]]\n","Validating..\n","Validation loss:  0.04616397765598127\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #10\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.3278085263920258\n","Training metrics:\n","\t accuracy :  0.9259259259259259\n","\t f1 :  [0.96153846 0.         0.        ]\n","\t average f1 :  0.32051282051282054\n","\t confusion matrix :  [[25  0  0]\n"," [ 1  0  0]\n"," [ 1  0  0]]\n","Validating..\n","Validation loss:  0.12038524616842292\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #11\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.5002409415966313\n","Training metrics:\n","\t accuracy :  0.8888888888888888\n","\t f1 :  [0.         0.94117647 0.        ]\n","\t average f1 :  0.3137254901960784\n","\t confusion matrix :  [[ 0  2  0]\n"," [ 0 24  0]\n"," [ 0  1  0]]\n","Validating..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Validation loss:  0.2942335968837142\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #12\n","Training..\n","Training loss:  0.22048101459707445\n","Training metrics:\n","\t accuracy :  0.9629629629629629\n","\t f1 :  [0.98113208 0.        ]\n","\t average f1 :  0.49056603773584906\n","\t confusion matrix :  [[26  0]\n"," [ 1  0]]\n","Validating..\n","Validation loss:  0.07011459395289421\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #13\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.2440217368394197\n","Training metrics:\n","\t accuracy :  0.9259259259259259\n","\t f1 :  [0.96153846 0.         0.        ]\n","\t average f1 :  0.32051282051282054\n","\t confusion matrix :  [[25  0  0]\n"," [ 1  0  0]\n"," [ 1  0  0]]\n","Validating..\n","Validation loss:  0.20946933714107477\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Epoch #14\n","Training..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training loss:  0.14465959793866473\n","Training metrics:\n","\t accuracy :  0.9629629629629629\n","\t f1 :  [0.98113208 0.        ]\n","\t average f1 :  0.49056603773584906\n","\t confusion matrix :  [[26  0]\n"," [ 1  0]]\n","Validating..\n","Validation loss:  0.13117362706023933\n","Validation metrics:\n","\t accuracy :  1.0\n","\t f1 :  [1.]\n","\t average f1 :  1.0\n","\t confusion matrix :  [[7]]\n","Done!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n","  warnings.warn(\n"]}],"source":["## TODO: Re-run with unidirectional LSTMs\n","## Keep other hyperparameters fixed\n","train_val_loop_lstm({\n","    \"bidirectional\": False,\n","    \"batch_size\": 512,\n","    \"d_emb\": 64,\n","    \"d_hidden\": 128,\n","    \"num_epochs\": 15,\n","    \"learning_rate\": 0.005,\n","    \"l2\": 1e-6,\n","})\n","## END"]},{"cell_type":"markdown","metadata":{"id":"h8UChDyKaPBs"},"source":["### Questions **(2 points)**\n","\n","How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n","\n","**TODO: Please fill in your answer here**\n","\n","---answer----\n","bidirectional LSTMs은 한 시퀀스는 시작부터 끝, 다른 시퀀스는 끝부터 시작으로 작업을 합니다. 제일 큰 특징의 경우, 전 시점의 시퀀스에 대해서 앞뒤 정보를 모두 획득이 가능하다는 점입니다. 주로 전체맥락이 중요한경우에 많이 이용합니다.\n","장점은 데이터를 양방향으로 처리해서 단방향접근시 놓칠수 있는 패턴을 포착이 가능하단점이 있습니다. 이번 과제와 같이 전 시점의 패턴이 중요할경우, bidirectional LSTMs을 쓰는 것이 좋습니다. 단점으로는 계산적으로 단방향보다 비용과 시간이 많이드는(약 2배) 점입니다.\n","\n","반면 unidirectional LSTMs은 처음부터 끝 까지, 즉 사람이 글을 읽어 내려가는 방식과 같이 정방향으로만 작업이 가능하며, 과거와 현재의 데이터만 사용해서 예측을 실행합니다.\n","한계는 과거의 정보만 이용할수 있어서정확한 예측을 위해서 미래의 입력특징의 맥락이 중요할 경우, 성능이 안 좋다.\n","주로 과거의 데이터를 주로 쓰는 시계열데이터에 많이 씁니다.\n","\n","데이터의 전체적인 맥락이 중요할경우 bidirectional LSTMs을 쓰고, 실시간처리나 과거의 데이터가 중요한 경우 unidirectional LSTMs을 사용하는것이 효율적입니다.\n","코드로 구현시\n","uni -> nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=False)\n","bi -> nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n","uni는 bidirectional=False, bi는 bidirectional=True가 됩니다.\n","hidden_dim의경우 단방향이 1일때, 양방향은 2가 되며 2배의 차이가 납니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kMEWxkN_bpIT"},"outputs":[],"source":["\n","---answer---- bidirectional LSTMs은 한 시퀀스는 시작부터 끝, 다른 시퀀스는 끝부터 시작으로 작업을 합니다. 제일 큰 특징의 경우, 전 시점의 시퀀스에 대해서 앞뒤 정보를 모두 획득이 가능하다는 점입니다. 주로 전체맥락이 중요한경우에 많이 이용합니다.\n","장점은 데이터를 양방향으로 처리해서 단방향접근시 놓칠수 있는 패턴을 포착이 가능하단점이 있습니다. 이번 과제와 같이 전 시점의 패턴이 중요할경우, bidirectional LSTMs을 쓰는 것이 좋습니다. 단점으로는 계산적으로 단방향보다 비용과 시간이 많이드는(약 2배) 점입니다.\n","\n","반면 unidirectional LSTMs은 처음부터 끝 까지, 즉 사람이 글을 읽어 내려가는 방식과 같이 정방향으로만 작업이 가능하며, 과거와 현재의 데이터만 사용해서 예측을 실행합니다.\n","한계는 과거의 정보만 이용할수 있어서정확한 예측을 위해서 미래의 입력특징의 맥락이 중요할 경우, 성능이 안 좋다.\n","주로 과거의 데이터를 주로 쓰는 시계열데이터에 많이 씁니다.\n","\n","데이터의 전체적인 맥락이 중요할경우 bidirectional LSTMs을 쓰고, 실시간처리나 과거의 데이터가 중요한 경우 unidirectional LSTMs을 사용하는것이 효율적입니다.\n","코드로 구현시\n","uni -> nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=False)\n","bi -> nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n","uni는 bidirectional=False, bi는 bidirectional=True가 됩니다.\n","hidden_dim의경우 단방향이 1일때, 양방향은 2가 되며 2배의 차이가 납니다."]},{"cell_type":"code","source":[],"metadata":{"id":"Ieb2CQR4O1F5"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1jdOjJ6brRtdRuNa0gGLty0mYz7GHXHIC","timestamp":1714054729763}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}