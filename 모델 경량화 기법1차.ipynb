{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "139b8e59-9126-4a1d-884c-a5a3fdb19037",
   "metadata": {},
   "source": [
    "# 모델 경량화 기법 – Pruning & Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e7d67-bc7b-4db5-a328-9af65de0ef90",
   "metadata": {},
   "source": [
    "## Day 28: 모델 경량화 기법 – Pruning & Quantization\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Theory: 왜 경량화가 필요한가?\n",
    "\n",
    "* **실제 서비스 제약**\n",
    "\n",
    "  * 모바일·IoT 기기, 엣지 디바이스에서는 연산·메모리 예산이 제한적\n",
    "  * 모델 크기·추론 속도를 줄여야 사용자 경험(배터리·응답 속도)을 지킬 수 있음\n",
    "* **Pruning (가지치기)**\n",
    "\n",
    "  * **Unstructured Pruning**: 개별 가중치(파라미터) 중 작은 값(절댓값 기준)을 0으로 강제\n",
    "\n",
    "    * → 희소성(sparsity)을 높여, 저장 공간·메모리 접근량 절감\n",
    "  * **Structured Pruning**: 채널(channel)·필터(filter) 단위로 통째로 제거\n",
    "\n",
    "    * → 실제 연산량(Flops) 감소 효과가 뚜렷\n",
    "* **Quantization (양자화)**\n",
    "\n",
    "  * **Post-Training Quantization** (PTQ): 학습 후 `float32` → `int8` 등으로 변환\n",
    "  * **Quantization-Aware Training** (QAT): 양자화 오차를 학습 과정에 반영하여 성능 저하 최소화\n",
    "  * 숫자 표현 비트를 줄여 메모리 절감, 벡터 연산 가속\n",
    "\n",
    "#### 실행 결과 예상\n",
    "\n",
    "* **가지치기** 후: 약간의 정확도 감소(±1% 이내)\n",
    "* **양자화** 후: 정확도 큰 손실 없이(±2% 이내) 파라미터 크기 절반 이하로 감소\n",
    "\n",
    "---\n",
    "\n",
    "### 3) 심화 과제\n",
    "\n",
    "1. **Structured Pruning** (채널 단위) 해 보기\n",
    "\n",
    "   ```python\n",
    "   prune.ln_structured(model.fc1, name='weight', amount=0.2, n=2, dim=0)\n",
    "   ```\n",
    "2. **Post-Training Static Quantization**\n",
    "\n",
    "   * `quant.prepare` → **Calibration Data** 통과 → `quant.convert`\n",
    "3. **QAT**\n",
    "\n",
    "   * `quant.prepare_qat` → 학습 진행 → `quant.convert`\n",
    "4. **모델 비교 보고서**\n",
    "\n",
    "   * 원본 vs Pruned vs Quantized 성능·크기·추론 속도 비교\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "254360e3-a19d-48ff-9f6c-bd17e00b5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import prune as prune\n",
    "from torch import quantization as quant\n",
    "from torch.utils.data import DataLoader as loader\n",
    "from torchvision import datasets, transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed62a94-1b95-4fe0-a802-970ee0a3e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Models(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Models,self).__init__()\n",
    "        self.fc1=nn.Linear(28*28,256)\n",
    "        self.fc2=nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.view(x.size(0),-1)\n",
    "        x=torch.relu(self.fc1(x))\n",
    "        out=self.fc2(x)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e005ea0f-4c8c-463f-9546-b53b4e9d9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_data=datasets.MNIST('.',train=True, download=True,transform=transform)\n",
    "test_data=datasets.MNIST('.',train=False,download=True,transform=transform)\n",
    "\n",
    "trian_loader=loader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader=loader(test_data, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7629020-86d2-429c-b648-3dea209b4711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda mode\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda'if torch.cuda.is_available()else'cpu')\n",
    "print(device,'mode')\n",
    "model=Models().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c85111b2-75cc-4043-b53e-c21e00cb9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalmode(model):\n",
    "    model.eval()\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data,target=data.to(device), target.to(device)\n",
    "            out=model(data)\n",
    "            correct+=(out.argmax(1)==target).sum().item()\n",
    "        return correct/len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "235986f3-7ac7-47ac-a4bd-da96103832c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 모델 정확도: 0.1259\n"
     ]
    }
   ],
   "source": [
    "opt=optim.SGD(model.parameters(),lr=0.01)\n",
    "criter=nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "data,target=next(iter(trian_loader))\n",
    "data,target=data.to(device),target.to(device)\n",
    "opt.zero_grad()\n",
    "out=model(data)\n",
    "loss=criter(out,target)\n",
    "loss.backward()\n",
    "opt.step()\n",
    "print(\"원본 모델 정확도:\", evalmode(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc807757-79b2-4ba9-bf84-569811e40c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned 모델 정확도: 0.1468\n"
     ]
    }
   ],
   "source": [
    "# — 5) Unstructured Pruning 적용 (20% 희소화) —\n",
    "prune.random_unstructured(model.fc1,name='weight',amount=0.2)\n",
    "prune.random_unstructured(model.fc2,name='weight',amount=0.2)\n",
    "\n",
    "print(\"Pruned 모델 정확도:\", evalmode(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebc83336-9833-48bc-b45e-bd1d852fc6ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'weight_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''qmodel=quant.quantize_dynamic(\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m    model,{nn.Linear}, dtype=torch.qint8\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m).to(device)'''\u001b[39;00m\n\u001b[0;32m      4\u001b[0m qmodel \u001b[38;5;241m=\u001b[39m quant\u001b[38;5;241m.\u001b[39mquantize_dynamic(\n\u001b[0;32m      5\u001b[0m     model, {nn\u001b[38;5;241m.\u001b[39mLinear}, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mqint8\n\u001b[0;32m      6\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantized 모델 정확도:\u001b[39m\u001b[38;5;124m\"\u001b[39m, evalmode(qmodel))\n",
      "Cell \u001b[1;32mIn[17], line 7\u001b[0m, in \u001b[0;36mevalmode\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m      6\u001b[0m     data,target\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 7\u001b[0m     out\u001b[38;5;241m=\u001b[39mmodel(data)\n\u001b[0;32m      8\u001b[0m     correct\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m(out\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m==\u001b[39mtarget)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m correct\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m, in \u001b[0;36mModels.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m      8\u001b[0m     x\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m     x\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[0;32m     10\u001b[0m     out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1779\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1774\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1775\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward pre-hook must return None or a tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1776\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_kwargs_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1777\u001b[0m             )\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1779\u001b[0m     args_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args)\n\u001b[0;32m   1780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1781\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\utils\\prune.py:31\u001b[0m, in \u001b[0;36mBasePruningMethod.__call__\u001b[1;34m(self, module, inputs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, module, inputs):\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Multiply the mask into original tensor and store the result.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    Multiplies the mask (stored in ``module[name + '_mask']``)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m        inputs: not used.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_mask(module))\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\utils\\prune.py:70\u001b[0m, in \u001b[0;36mBasePruningMethod.apply_mask\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# to carry out the multiplication, the mask needs to have been computed,\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# so the pruning method must know what tensor it's operating on\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     69\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has to be pruned\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# this gets set in apply()\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_orig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m pruned_tensor \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39morig\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m*\u001b[39m orig\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'weight_mask'"
     ]
    }
   ],
   "source": [
    "'''qmodel=quant.quantize_dynamic(\n",
    "    model,{nn.Linear}, dtype=torch.qint8\n",
    ").to(device)'''\n",
    "qmodel = quant.quantize_dynamic(\n",
    "    model, {nn.Linear}, dtype=torch.qint8\n",
    ").to(device)\n",
    "print(\"Quantized 모델 정확도:\", evalmode(qmodel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e6288e3-8d2a-4f02-8911-7a960546b3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized 모델 정확도: 0.0949\n"
     ]
    }
   ],
   "source": [
    "# (1) model이 GPU에 올라가 있었다면 CPU로 꺼내고\n",
    "model = model.cpu()\n",
    "\n",
    "# (2) prune 훅을 제거하고\n",
    "prune.random_unstructured(model.fc1, 'weight', amount=0.2)\n",
    "prune.random_unstructured(model.fc2, 'weight', amount=0.2)\n",
    "prune.remove(model.fc1, 'weight')\n",
    "prune.remove(model.fc2, 'weight')\n",
    "\n",
    "# (3) 동적 양자화 역시 CPU에서만 적용\n",
    "qmodel = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")   # 이미 qmodel은 CPU 상의 모델\n",
    "\n",
    "# (4) 이제 평가 함수도 CPU 전용으로\n",
    "def eval_cpu(m):\n",
    "    m.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            # test_loader가 GPU가 아니라 CPU 데이터를 준다고 가정\n",
    "            out = m(x)  \n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "print(\"Quantized 모델 정확도:\", eval_cpu(qmodel))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "015f070e-8648-4481-a1a6-eb80b26d2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.quantized as qb\n",
    "qb.engine = 'fbgemm'   # x86 CPU에서 양자화용 연산을 수행할 백엔드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24844108-6934-4511-92af-fd8dcf5d5eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 파라미터 크기: 0.816 MB\n",
      "Quantized 파라미터 크기: 0.207 MB\n"
     ]
    }
   ],
   "source": [
    "# — 7) 크기 비교 —\n",
    "import io\n",
    "def size_of(m):\n",
    "    buf = io.BytesIO()\n",
    "    torch.save(m.state_dict(), buf)\n",
    "    return buf.getbuffer().nbytes / 1e6  # MB\n",
    "\n",
    "print(f\"원본 파라미터 크기: {size_of(model):.3f} MB\")\n",
    "print(f\"Quantized 파라미터 크기: {size_of(qmodel):.3f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd52a0a4-219a-4fa3-b269-fa3f2effcc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90634c5-9607-4ce2-af69-cda6316ea403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2d529-231f-4598-8b72-dc95377b826f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CUDA 12.4)",
   "language": "python",
   "name": "cuda124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
