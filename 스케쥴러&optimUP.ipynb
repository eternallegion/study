{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d94052-2d83-4775-8b01-d47ac6566072",
   "metadata": {},
   "source": [
    "# Learning Rate Scheduling\n",
    "# AdamW, RAdam, QHAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51e4bd3f-1186-4647-ba95-f302d3c76892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR#LambdaLR\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import DataLoader as loader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f37c4958-f938-4479-a12a-350ff42b67aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Compleat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225)\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data=datasets.CIFAR10(root='./data', download=True,transform=transform)\n",
    "test_data=datasets.CIFAR10(root='./data',download=True, transform=transform)\n",
    "\n",
    "train_loader=loader(train_data,batch_size=64,shuffle=True,num_workers=4)\n",
    "test_loader=loader(test_data, batch_size=128,shuffle=True, num_workers=4)\n",
    "\n",
    "print(\"Compleat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01bf362e-22b1-4840-aabb-fc20321ddfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=models.resnet18(pretrained=True)\n",
    "num_ftrs=model.fc.in_features\n",
    "model.fc=nn.Linear(num_ftrs,10) # CIFAR-10ì€ 10ê°œ í´ë˜ìŠ¤\n",
    "model=model.to(device:= torch.device('cuda'if torch.cuda.is_available()else'cpu'))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4728d579-f58e-419c-9c80-c5900df61d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimy=optim.AdamW(model.parameters(),lr=1e-3,weight_decay=1e-4)\n",
    "criter=nn.CrossEntropyLoss()\n",
    "schedule=CosineAnnealingLR(optimy,T_max=20,eta_min=1e-5)\n",
    "\n",
    "num_epoch=50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d9ffc3ff-d720-45b1-9967-ccb7e7ac9975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 â€” Train Loss: 0.0030\n",
      "Epoch 2/50 â€” Train Loss: 0.0071\n",
      "Epoch 3/50 â€” Train Loss: 0.0097\n",
      "Epoch 4/50 â€” Train Loss: 0.0070\n",
      "Epoch 5/50 â€” Train Loss: 0.0095\n",
      "Epoch 6/50 â€” Train Loss: 0.0116\n",
      "Epoch 7/50 â€” Train Loss: 0.0086\n",
      "Epoch 8/50 â€” Train Loss: 0.0078\n",
      "Epoch 9/50 â€” Train Loss: 0.0100\n",
      "Epoch 10/50 â€” Train Loss: 0.0087\n",
      "Epoch 11/50 â€” Train Loss: 0.0068\n",
      "Epoch 12/50 â€” Train Loss: 0.0099\n",
      "Epoch 13/50 â€” Train Loss: 0.0103\n",
      "Epoch 14/50 â€” Train Loss: 0.0085\n",
      "Epoch 15/50 â€” Train Loss: 0.0072\n",
      "Epoch 16/50 â€” Train Loss: 0.0080\n",
      "Epoch 17/50 â€” Train Loss: 0.0069\n",
      "Epoch 18/50 â€” Train Loss: 0.0094\n",
      "Epoch 19/50 â€” Train Loss: 0.0089\n",
      "Epoch 20/50 â€” Train Loss: 0.0067\n",
      "Epoch 21/50 â€” Train Loss: 0.0063\n",
      "Epoch 22/50 â€” Train Loss: 0.0077\n",
      "Epoch 23/50 â€” Train Loss: 0.0086\n",
      "Epoch 24/50 â€” Train Loss: 0.0084\n",
      "Epoch 25/50 â€” Train Loss: 0.0070\n",
      "Epoch 26/50 â€” Train Loss: 0.0060\n",
      "Epoch 27/50 â€” Train Loss: 0.0073\n",
      "Epoch 28/50 â€” Train Loss: 0.0066\n",
      "Epoch 29/50 â€” Train Loss: 0.0076\n",
      "Epoch 30/50 â€” Train Loss: 0.0066\n",
      "Epoch 31/50 â€” Train Loss: 0.0061\n",
      "Epoch 32/50 â€” Train Loss: 0.0056\n",
      "Epoch 33/50 â€” Train Loss: 0.0072\n",
      "Epoch 34/50 â€” Train Loss: 0.0047\n",
      "Epoch 35/50 â€” Train Loss: 0.0069\n",
      "Epoch 36/50 â€” Train Loss: 0.0035\n",
      "Epoch 37/50 â€” Train Loss: 0.0067\n",
      "Epoch 38/50 â€” Train Loss: 0.0071\n",
      "Epoch 39/50 â€” Train Loss: 0.0079\n",
      "Epoch 40/50 â€” Train Loss: 0.0052\n",
      "Epoch 41/50 â€” Train Loss: 0.0057\n",
      "Epoch 42/50 â€” Train Loss: 0.0061\n",
      "Epoch 43/50 â€” Train Loss: 0.0068\n",
      "Epoch 44/50 â€” Train Loss: 0.0055\n",
      "Epoch 45/50 â€” Train Loss: 0.0059\n",
      "Epoch 46/50 â€” Train Loss: 0.0044\n",
      "Epoch 47/50 â€” Train Loss: 0.0066\n",
      "Epoch 48/50 â€” Train Loss: 0.0054\n",
      "Epoch 49/50 â€” Train Loss: 0.0058\n",
      "Epoch 50/50 â€” Train Loss: 0.0049\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    total=0.0\n",
    "    for data,target in train_loader:\n",
    "        data=data.to(device)\n",
    "        target=target.to(device)\n",
    "        optimy.zero_grad()\n",
    "        out=model(data)\n",
    "        loss=criter(out, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n",
    "        optimy.step()\n",
    "        total+=loss.item()*data.size(0)\n",
    "    schedule.step()\n",
    "    train_loss=total/len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epoch} â€” Train Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5bb0b15b-cb1c-4cfa-b880-0454465a1fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW (Decoupled Weight Decay)\\\n",
    "#Decoupled Weight DecayëŠ” weight decay(ğœ†) í•­ì„ ì˜µí‹°ë§ˆì´ì € ì—…ë°ì´íŠ¸ì™€ ë¶„ë¦¬í•´,\n",
    "#í•™ìŠµë¥  ìŠ¤ì¼€ì¼ê³¼ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™ì‹œí‚µë‹ˆë‹¤\n",
    "lr=1e-3\n",
    "optimy=optim.AdamW(model.parameters(),lr=lr,weight_decay=1e-2)\n",
    "# Decoupled L2 í˜ë„í‹° :contentReference[oaicite:11]{index=11})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f35be848-4986-4940-a243-96edc61d9c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAdam (Rectified Adam)\n",
    "optimy=optim.RAdam(model.parameters(),lr=lr,betas=(0.9,0.999), eps=1e-8)\n",
    "#RAdamì€ ì´ˆê¸° ë¶„ì‚° ì¶”ì •ì˜ ë¶ˆì•ˆì •ì„ ì›Œë°ì—… í˜•íƒœë¡œ ë³´ì •í•œ ì˜µí‹°ë§ˆì´ì €ë¡œ,\n",
    "#ì´ˆë°˜ì—ëŠ” Momentum SGDì²˜ëŸ¼ ë™ì‘í•˜ë‹¤ê°€ ì´í›„ Adam ì—…ë°ì´íŠ¸ë¡œ ì „í™˜ë©ë‹ˆë‹¤ \n",
    "# :contentReference[oaicite:13]{index=13}\n",
    "#0.088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "45977120-bd79-4ae7-8722-feda89f1a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QHAdam (Quasi-Hyperbolic Adam)\n",
    "#QHAdamì€ Adamì˜ 1Â·2ì°¨ ëª¨ë©˜íŠ¸ ì¶”ì •ì— Quasi-Hyperbolic Momentumì„ ì ìš©í•´,\n",
    "#plain SGDì™€ momentum SGDì˜ ì¥ì ì„ ê°€ì¤‘ í‰ê· í•˜ë“¯ ê²°í•©í•©ë‹ˆë‹¤\n",
    "\n",
    "from qhoptim.pyt import QHAdam as qha\n",
    "optimy=qha(model.parameters(), lr=lr, nus=(0.7,1.0), betas=(0.995,0.999), weight_decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ea2bf-018a-482e-bc8d-17f6ab16cc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12b85da9-fced-470a-a8ce-6217a4f4e62d",
   "metadata": {},
   "source": [
    "30ì´ˆ ë™ì•ˆ ìƒê°í•¨\r\n",
    "\r\n",
    "\r\n",
    "ì•„ë˜ëŠ” **AdamW**, **RAdam**, **QHAdam** ì„¸ ì˜µí‹°ë§ˆì´ì €ì˜ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì˜ë¯¸í•˜ëŠ” ë°”ë¥¼ ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤. ê° íŒŒë¼ë¯¸í„°ê°€ ì–´ë–»ê²Œ ë™ì‘ì— ì˜í–¥ì„ ì£¼ëŠ”ì§€ ì´ë¡ ì ì¸ ì„¤ëª…ê³¼ í•¨ê»˜, PyTorch(ë˜ëŠ” qhoptim) êµ¬í˜„ ì‹œ ê¸°ë³¸ê°’ê³¼ ì¡°ì • íŒì„ ì œê³µí•˜ë©°, ê´€ë ¨ ë…¼ë¬¸Â·ë¬¸ì„œì—ì„œ ë°œì·Œí•œ ë‚´ìš©ì„ í’ë¶€í•œ ì¸ìš©ê³¼ í•¨ê»˜ ë‹´ì•˜ìŠµë‹ˆë‹¤.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 1. AdamWì˜ ì£¼ìš” íŒŒë¼ë¯¸í„°\r\n",
    "\r\n",
    "### 1.1 `weight_decay`\r\n",
    "\r\n",
    "AdamWì—ì„œì˜ weight decay(ğœ†)ëŠ” **íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸** ë‹¨ê³„ì—ì„œ ì§ì ‘ $\\theta \\leftarrow \\theta - \\eta\\,\\bigl(\\hat m_t / (\\sqrt{\\hat v_t} + \\epsilon)\\bigr) - \\eta\\,\\lambda\\,\\theta$ í˜•íƒœë¡œ ì ìš©ë˜ëŠ” **decoupled L2 ê·œì œ** ê³„ìˆ˜ì…ë‹ˆë‹¤.\r\n",
    "\r\n",
    "* ì „í†µì  L2 ì •ê·œí™”(Lossì— $\\tfrac{\\lambda}{2}\\|\\theta\\|^2$ ë¶€ê°€)ì™€ ë‹¬ë¦¬, í•™ìŠµë¥  ìŠ¤ì¼€ì¼ê³¼ ë¶„ë¦¬ë˜ì–´ **ì¼ê´€ëœ ì •ê·œí™” íš¨ê³¼**ë¥¼ ëƒ…ë‹ˆë‹¤ ([Medium][1], [Stack Overflow][2]).\r\n",
    "* ê¸°ë³¸ê°’ì€ `1e-2`ì´ë©°, ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ë ¤ë©´ `1e-4`âˆ¼`1e-2` ì‚¬ì´ì—ì„œ ì‹¤í—˜í•©ë‹ˆë‹¤ ([PyTorch][3]).\r\n",
    "\r\n",
    "### 1.2 `betas=(Î²â‚, Î²â‚‚)`\r\n",
    "\r\n",
    "* $\\beta_1$ (1ì°¨ ëª¨ë©˜íŠ¸ ê°ì‡ ): ê³¼ê±° ê·¸ë˜ë””ì–¸íŠ¸ì˜ **ì§€ìˆ˜ì´ë™í‰ê· **ì„ ê³„ì‚°í•˜ëŠ” ê³„ìˆ˜ (ê¸°ë³¸ê°’ `0.9`).\r\n",
    "* $\\beta_2$ (2ì°¨ ëª¨ë©˜íŠ¸ ê°ì‡ ): ê³¼ê±° ê·¸ë˜ë””ì–¸íŠ¸ ì œê³±ì˜ **ì§€ìˆ˜ì´ë™í‰ê· **ì„ ê³„ì‚°í•˜ëŠ” ê³„ìˆ˜ (ê¸°ë³¸ê°’ `0.999`).\r\n",
    "* ì´ ë‘ ê°’ì€ í•™ìŠµë¥ ì„ **íŒŒë¼ë¯¸í„°ë³„ë¡œ ì¡°ì •**í•˜ëŠ” í•µì‹¬ìœ¼ë¡œ, ëª¨ë©˜í…€ê³¼ RMSpropì˜ ì¥ì ì„ ê²°í•©í•©ë‹ˆë‹¤ ([PyTorch][3]).\r\n",
    "\r\n",
    "### 1.3 `eps`\r\n",
    "\r\n",
    "* ë¶„ëª¨ $\\sqrt{\\hat v_t} + \\epsilon$ ì— ë”í•´ì§€ëŠ” ì‘ì€ ìƒìˆ˜ë¡œ, **ìˆ˜ì¹˜ ë¶ˆì•ˆì •ì„±(NaN/Inf)** ë°©ì§€ë¥¼ ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.\r\n",
    "* ê¸°ë³¸ê°’ `1e-8`ì´ë©°, FP16/Mixed-Precision í™˜ê²½ì—ì„œ **ë” í° Îµ** (`1e-6` ë“±)ë¥¼ ì“°ë©´ ì•ˆì •ì„±ì´ ì¦ê°€í•©ë‹ˆë‹¤ ([PyTorch][3]).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 2. RAdamì˜ ì£¼ìš” íŒŒë¼ë¯¸í„°\r\n",
    "\r\n",
    "### 2.1 `betas=(Î²â‚, Î²â‚‚)`\r\n",
    "\r\n",
    "* Adamê³¼ ë™ì¼í•˜ê²Œ, 1ì°¨Â·2ì°¨ ëª¨ë©˜íŠ¸ ê°ì‡  ê³„ìˆ˜ì…ë‹ˆë‹¤ (ê¸°ë³¸ê°’ `(0.9, 0.999)`).\r\n",
    "* ì´ˆë°˜ ë¶„ì‚° ì¶”ì • ë¶ˆì•ˆì • ë¬¸ì œë¥¼ **ìë™ ì›Œë°ì—…**ìœ¼ë¡œ ì™„í™”í•œ RAdamì—ì„œë„ ë™ì¼í•œ ì—­í• ì„ í•©ë‹ˆë‹¤ ([PyTorch][4]).\r\n",
    "\r\n",
    "### 2.2 `eps`\r\n",
    "\r\n",
    "* AdamWì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë¶„ëª¨ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì‘ì€ ìƒìˆ˜ (`1e-8`).\r\n",
    "* RAdam êµ¬í˜„ì— ë”°ë¼ `1e-6`ì„ ê¸°ë³¸ìœ¼ë¡œ ì“°ëŠ” ê²½ìš°ë„ ìˆìœ¼ë¯€ë¡œ, **ë¬¸ì„œ í™•ì¸ í›„** ì¡°ì •í•˜ì„¸ìš” ([training-api.cerebras.ai][5]).\r\n",
    "\r\n",
    "### 2.3 `weight_decay`\r\n",
    "\r\n",
    "* L2 í˜ë„í‹° ê³„ìˆ˜ë¡œ, ê¸°ë³¸ê°’ `0`.\r\n",
    "* `decoupled_weight_decay=True`ë¡œ ì„¤ì •í•˜ë©´ **AdamW ë°©ì‹**ìœ¼ë¡œ weight decayë¥¼ ë¶„ë¦¬ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ([PyTorch][4]).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 3. QHAdamì˜ ì£¼ìš” íŒŒë¼ë¯¸í„°\r\n",
    "\r\n",
    "ì„¤ì¹˜: `pip install qhoptim` (Facebook Research ì œê³µ) ([facebookresearch.github.io][6]).\r\n",
    "\r\n",
    "### 3.1 `nus=(Î½â‚, Î½â‚‚)`\r\n",
    "\r\n",
    "* **Quasi-Hyperbolic Momentum (QHM)** ì—ì„œ ëª¨ë©˜í…€ê³¼ ë¶„ì‚° ê°ì‡ ë¥¼ ì¦‰ì‹œ ì¡°ì ˆí•˜ëŠ” íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.\r\n",
    "\r\n",
    "  * $\\nu_1$ (ì²« ë²ˆì§¸ QHM): **ëª¨ë©˜í…€** í•­ì˜ ì¦‰ê°ì  ë¹„ì¤‘ ì¡°ì ˆ (ì¶”ì²œê°’ `0.7`).\r\n",
    "  * $\\nu_2$ (ë‘ ë²ˆì§¸ QHM): **ë¶„ì‚°(2ì°¨ ëª¨ë©˜íŠ¸)** í•­ì˜ ì¦‰ê°ì  ë¹„ì¤‘ ì¡°ì ˆ (ì¶”ì²œê°’ `1.0`).\r\n",
    "* Î½ë¥¼ ì¡°ì ˆí•´ **SGDâ†”Momentum** ë˜ëŠ” **Adamâ†”RAdam** í˜•íƒœë¡œ ìœ ì—°í•˜ê²Œ ì „í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ([facebookresearch.github.io][7]).\r\n",
    "\r\n",
    "### 3.2 `betas=(Î²â‚, Î²â‚‚)`\r\n",
    "\r\n",
    "* QHAdamì—ì„œë„ **1ì°¨Â·2ì°¨ ëª¨ë©˜íŠ¸ ê°ì‡ ** ê³„ìˆ˜ë¡œ ì‘ìš©í•˜ë©°, ê¸°ë³¸ê°’ì€ `(0.9, 0.999)`.\r\n",
    "* Î½ì™€ ì¡°í•©í•´ **ì¦‰ê°ì„±**(plain gradient)ê³¼ **ì§€ì†ì„±**(momentum/RMS) ë¹„ì¤‘ì„ ì„¸ë°€í•˜ê²Œ ì¡°ì ˆí•©ë‹ˆë‹¤ ([facebookresearch.github.io][6]).\r\n",
    "\r\n",
    "### 3.3 `eps`\r\n",
    "\r\n",
    "* ë¶„ëª¨ ì•ˆì •í™” ìš© ìƒìˆ˜ (`1e-8`).\r\n",
    "* QHAdamë„ **ë¶„ì‚° ì¶”ì •** ë¶„ëª¨ì— ë”í•´ì ¸, ì‘ì€ ê·¸ë˜ë””ì–¸íŠ¸ì—ì„œë„ ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤ ([facebookresearch.github.io][6]).\r\n",
    "\r\n",
    "### 3.4 `weight_decay`\r\n",
    "\r\n",
    "* AdamWì™€ ê°™ì´ decoupled L2 ê·œì œ(ê¸°ë³¸ê°’ `0`).\r\n",
    "* `decouple_weight_decay=True`ë¡œ ì„¤ì •í•˜ë©´ **QHAdamW** í˜•íƒœë¡œ ë…ë¦½ ê·œì œë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ([facebookresearch.github.io][6]).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "ì´ìƒìœ¼ë¡œ, **weight\\_decay**, **betas**, **nus**, **eps** ë„¤ ê°€ì§€ íŒŒë¼ë¯¸í„°ê°€ ê° ì˜µí‹°ë§ˆì´ì €ì—ì„œ ì–´ë–»ê²Œ ë™ì‘í•˜ê³ , ê¸°ë³¸ê°’ê³¼ ì¡°ì • íŒì€ ë¬´ì—‡ì¸ì§€ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì‹¤ì œ ì‹¤í—˜ì—ì„œëŠ” **í•™ìŠµë¥ **ê³¼ í•¨ê»˜ ì´ë“¤ ê°’ì„ íŠœë‹í•´ ë³´ë©´ì„œ, ì•ˆì •ì„±ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ ê°„ ê· í˜•ì„ ë§ì¶”ì‹œëŠ” ê²ƒì„ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤.\r\n",
    "\r\n",
    "[1]: https://yassin01.medium.com/adam-vs-adamw-understanding-weight-decay-and-its-impact-on-model-performance-b7414f0af8a1?utm_source=chatgpt.com \"Adam vs. AdamW: Understanding Weight Decay and Its Impact on ...\"\r\n",
    "[2]: https://stackoverflow.com/questions/64621585/pytorch-optimizer-adamw-and-adam-with-weight-decay?utm_source=chatgpt.com \"PyTorch Optimizer: AdamW and Adam with weight decay\"\r\n",
    "[3]: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html?utm_source=chatgpt.com \"AdamW â€” PyTorch 2.7 documentation\"\r\n",
    "[4]: https://pytorch.org/docs/stable/generated/torch.optim.RAdam.html?utm_source=chatgpt.com \"RAdam â€” PyTorch 2.7 documentation\"\r\n",
    "[5]: https://training-api.cerebras.ai/en/latest/_modules/cerebras/pytorch/optim/RAdam.html?utm_source=chatgpt.com \"cerebras.pytorch.optim.RAdam\"\r\n",
    "[6]: https://facebookresearch.github.io/qhoptim/pyt.html?utm_source=chatgpt.com \"Quasi-hyperbolic optimizers for PyTorch â€” qhoptim 1.1.0 ...\"\r\n",
    "[7]: https://facebookresearch.github.io/qhoptim/?utm_source=chatgpt.com \"Quasi-hyperbolic optimization â€” qhoptim 1.1.0 documentation\"\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cff9b-97da-400a-88bb-85fdec4e3dde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f92ff6d-dd65-49ff-94d1-67f565bea151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a780e3c-f054-4e42-8512-d10fedf19a9d",
   "metadata": {},
   "source": [
    "\n",
    "\r\n",
    "\r\n",
    "AdamW, RAdam, QHAdam ì„¸ ì˜µí‹°ë§ˆì´ì €ì˜ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°(`weight_decay`, `betas`, `nus`, `eps`)ê°€ ëª¨ë¸ í•™ìŠµì— ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ìš”ì•½í•˜ë©´:\r\n",
    "\r\n",
    "* **`weight_decay`**: L2 ì •ê·œí™” ê°•ë„(AdamW/QHAdam) ë˜ëŠ” 0ì¼ ë•Œ ë¹„í™œì„±(ê¸°ë³¸ RAdam)ë¡œ, íŒŒë¼ë¯¸í„° í¬ê¸° ì œì–´ë¥¼ ì§ì ‘ ì ìš©í•©ë‹ˆë‹¤.\r\n",
    "* **`betas=(Î²â‚, Î²â‚‚)`**: 1ì°¨Â·2ì°¨ ëª¨ë©˜íŠ¸(ëª¨ë©˜í…€Â·ë¶„ì‚°) ì§€ìˆ˜ì´ë™í‰ê·  ê°ì‡ ìœ¨ë¡œ, ê³¼ê±° ê·¸ë˜ë””ì–¸íŠ¸ ì •ë³´ì˜ â€œê¸°ì–µ ê¹Šì´â€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\r\n",
    "* **`nus=(Î½â‚, Î½â‚‚)`**(QHAdam ì „ìš©): QHM ë°©ì‹ìœ¼ë¡œ 1Â·2ì°¨ ëª¨ë©˜íŠ¸ ì¦‰ì‹œ ë°˜ì˜ ë¹„ì¤‘ì„ ì¡°ì ˆí•´, ìˆœìˆ˜ SGDì™€ ëª¨ë©˜í…€ì˜ ì¤‘ê°„ ê±°ë™ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\r\n",
    "* **`eps`**: ë¶„ëª¨ ì•ˆì •ì„±ì„ ìœ„í•œ ì‘ì€ ìƒìˆ˜ë¡œ, ìˆ˜ì¹˜ì  NaN/Inf ë°©ì§€ì™€ FP16 í™˜ê²½ ëŒ€ì‘ì„ ë•ìŠµë‹ˆë‹¤.\r\n",
    "\r\n",
    "ì•„ë˜ì—ì„œ ì˜µí‹°ë§ˆì´ì €ë³„ë¡œ ê° íŒŒë¼ë¯¸í„°ê°€ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì˜ë¯¸ì¸ì§€, ê¸°ë³¸ê°’ê³¼ ì¡°ì • íŒì„ ì‚´í´ë³´ì„¸ìš”.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 1. AdamW\r\n",
    "\r\n",
    "### 1.1 `weight_decay`\r\n",
    "\r\n",
    "* **Decoupled Weight Decay**: íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì§ì „ì— ì§ì ‘\r\n",
    "  $\\theta \\leftarrow \\theta - \\eta\\,\\lambda\\,\\theta$ í˜•íƒœë¡œ L2 í˜ë„í‹°ë¥¼ ì ìš©í•©ë‹ˆë‹¤ ([PyTorch][1], [Papers with Code][2]).\r\n",
    "* ì „í†µì  L2 ì •ê·œí™”(Lossì— ì¶”ê°€)ì™€ ë‹¬ë¦¬, í•™ìŠµë¥  ìŠ¤ì¼€ì¼ê³¼ **ë…ë¦½ì ìœ¼ë¡œ** ë™ì‘í•´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ìš©ì´í•©ë‹ˆë‹¤ ([Origins AI][3]).\r\n",
    "* **ê¸°ë³¸ê°’**: `0.01`. ì¼ë°˜í™” ì„±ëŠ¥ì„ ìœ„í•´ `1e-4`âˆ¼`1e-2` ë²”ìœ„ì—ì„œ ì‹¤í—˜í•©ë‹ˆë‹¤ ([Learn R, Python & Data Science Online][4]).\r\n",
    "\r\n",
    "### 1.2 `betas=(Î²â‚, Î²â‚‚)`\r\n",
    "\r\n",
    "* $\\beta_1$: 1ì°¨ ëª¨ë©˜íŠ¸(ëª¨ë©˜í…€) EMA ê°ì‡ ìœ¨, ê¸°ë³¸ `0.9` (ê³¼ê±° ê¸°ìš¸ê¸°ë¥¼ 90% ìœ ì§€) ([Cross Validated][5]).\r\n",
    "* $\\beta_2$: 2ì°¨ ëª¨ë©˜íŠ¸(RMSprop) EMA ê°ì‡ ìœ¨, ê¸°ë³¸ `0.999` (ê³¼ê±° ì œê³± ê¸°ìš¸ê¸°ë¥¼ 99.9% ìœ ì§€) ([PyTorch Forums][6]).\r\n",
    "* ê°’ì´ í´ìˆ˜ë¡ ê³¼ê±° ì •ë³´ì— **ë” í° ë¹„ì¤‘**ì„ ë‘ë©°, ì‘ì„ìˆ˜ë¡ ìµœì‹  ê¸°ìš¸ê¸°ì— **ë” ë¯¼ê°**í•´ì§‘ë‹ˆë‹¤ ([Cross Validated][5]).\r\n",
    "\r\n",
    "### 1.3 `eps`\r\n",
    "\r\n",
    "* ë¶„ëª¨ $\\sqrt{\\hat v_t} + \\epsilon$ ì— ë”í•´ì§€ëŠ” ì‘ì€ ìƒìˆ˜ë¡œ, ìˆ˜ì¹˜ì  **ì•ˆì •ì„±**(NaN/Inf ë°©ì§€)ì— ê¸°ì—¬í•©ë‹ˆë‹¤ ([PyTorch][1]).\r\n",
    "* **ê¸°ë³¸ê°’**: `1e-8`. FP16/Mixed Precision í™˜ê²½ ì‹œ `1e-6` ë“±ìœ¼ë¡œ ëŠ˜ë¦¬ë©´ ì•ˆì •ì„±ì´ ë”ìš± í–¥ìƒë©ë‹ˆë‹¤ ([KDnuggets][7]).\r\n",
    "\r\n",
    "### 1.4 ì¡°í•© íŒ\r\n",
    "\r\n",
    "* **ìŠ¤ì¼€ì¤„ëŸ¬**: CosineAnnealingLR, StepLR ë“±ê³¼ í•¨ê»˜ ì‚¬ìš©í•  ë•Œ ìµœì ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ([Origins AI][3]).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 2. RAdam\r\n",
    "\r\n",
    "### 2.1 `betas=(Î²â‚, Î²â‚‚)`\r\n",
    "\r\n",
    "* Adamê³¼ ë™ì¼í•˜ê²Œ, 1ì°¨Â·2ì°¨ ëª¨ë©˜íŠ¸ EMA ê°ì‡ ìœ¨ì…ë‹ˆë‹¤ (ê¸°ë³¸ `(0.9, 0.999)`) .\r\n",
    "* **Warmup** íš¨ê³¼ë¥¼ ë‚´ì¥í•´, ì´ˆê¸° ë¶„ì‚° ì¶”ì • ë¶ˆì•ˆì • ë¬¸ì œë¥¼ ìë™ ë³´ì •í•©ë‹ˆë‹¤ ([GitHub][8]).\r\n",
    "\r\n",
    "### 2.2 `eps`\r\n",
    "\r\n",
    "* AdamWì™€ ë§ˆì°¬ê°€ì§€ë¡œ **ìˆ˜ì¹˜ ì•ˆì „ì„±**ì„ ìœ„í•œ ìƒìˆ˜ (`1e-8`), ì¼ë¶€ êµ¬í˜„ì—ì„œëŠ” `1e-6` ê¸°ë³¸ì¸ ê²½ìš°ë„ ìˆìŠµë‹ˆë‹¤ ([Facebook Research][9]).\r\n",
    "\r\n",
    "### 2.3 `weight_decay`\r\n",
    "\r\n",
    "* ê¸°ë³¸ê°’ `0`.\r\n",
    "* `decoupled_weight_decay=True` ë¡œ ì„¤ì •í•˜ë©´ **AdamW ìŠ¤íƒ€ì¼**ë¡œ L2 ê·œì œë¥¼ ë¶„ë¦¬ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ([PyTorch Forums][10]).\r\n",
    "\r\n",
    "### 2.4 í•µì‹¬ íš¨ê³¼\r\n",
    "\r\n",
    "* ì´ˆë°˜ ìŠ¤í…ì—ì„œëŠ” **ëª¨ë©˜í…€ SGD**ì™€ ìœ ì‚¬í•˜ê²Œ ë™ì‘í•˜ë‹¤ê°€, ì´í›„ **Adam** ì—…ë°ì´íŠ¸ë¡œ ì „í™˜í•´ ì•ˆì •ì  ìˆ˜ë ´ì„ ë•ìŠµë‹ˆë‹¤ ([GitHub][8]).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 3. QHAdam\r\n",
    "\r\n",
    "### 3.1 `nus=(Î½â‚, Î½â‚‚)`\r\n",
    "\r\n",
    "* Quasi-Hyperbolic Momentum(QHM) ê¸°ë°˜ ê°ì‡  íŒŒë¼ë¯¸í„°ë¡œ, í‰ë²”í•œ SGD ìŠ¤í…ê³¼ ëª¨ë©˜í…€ ìŠ¤í…ì„ **ê°€ì¤‘ í‰ê· **í•˜ë“¯ ê²°í•©í•©ë‹ˆë‹¤ ([Facebook Research][9]).\r\n",
    "\r\n",
    "  * $\\nu_1$: ëª¨ë©˜í…€ ì¦‰ì‹œ ë°˜ì˜ ë¹„ì¤‘ (ì¶”ì²œ `0.7`) ([OpenReview][11]).\r\n",
    "  * $\\nu_2$: ë¶„ì‚° ì¦‰ì‹œ ë°˜ì˜ ë¹„ì¤‘ (ì¶”ì²œ `1.0`) ([OpenReview][11]).\r\n",
    "\r\n",
    "### 3.2 `betas=(Î²â‚, Î²â‚‚)`\r\n",
    "\r\n",
    "* 1Â·2ì°¨ ëª¨ë©˜íŠ¸ ê°ì‡ ìœ¨ë¡œ, ê¸°ë³¸ `(0.9, 0.999)`.\r\n",
    "* QHM Î½ì™€ ê²°í•©í•´ **ìˆœìˆ˜ SGD**ì—ì„œ **ê°•í•œ ëª¨ë©˜í…€**ê¹Œì§€ ë‹¤ì–‘í•œ ì—…ë°ì´íŠ¸ ê±°ë™ì„ ì‹¤í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ([Papers with Code][12]).\r\n",
    "\r\n",
    "### 3.3 `eps`\r\n",
    "\r\n",
    "* ë¶„ëª¨ ì•ˆì •í™” ìƒìˆ˜ (`1e-8`), ê·¸ë˜ë””ì–¸íŠ¸ê°€ ë§¤ìš° ì‘ì„ ë•Œ ìˆ˜ì¹˜ì  NaNì„ ë°©ì§€í•©ë‹ˆë‹¤ ([Facebook Research][13]).\r\n",
    "\r\n",
    "### 3.4 `weight_decay`\r\n",
    "\r\n",
    "* ê¸°ë³¸ `0`.\r\n",
    "* `decouple_weight_decay=True` ë¡œ ì„¤ì •í•˜ë©´ **QHAdamW** í˜•íƒœë¡œ, AdamWì‹ decoupled L2 ê·œì œë¥¼ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ([GitHub][14]).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ì°¸ì¡° ë…¼ë¬¸Â·ë¬¸ì„œ\r\n",
    "\r\n",
    "1. Loshchilov & Hutter, â€œDecoupled Weight Decay Regularizationâ€ ([Papers with Code][2])\r\n",
    "2. Liu et al., â€œOn the Variance of the Adaptive Learning Rate and Beyondâ€ ([GitHub][8])\r\n",
    "3. Ma & Yarats, â€œQuasi-Hyperbolic Momentum and Adam for Deep Learningâ€ ([OpenReview][11])\r\n",
    "\r\n",
    "ìœ„ íŒŒë¼ë¯¸í„°ë“¤ì„ ì¡°í•©í•´, ë‹¤ì–‘í•œ í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìµœì í™” ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ê·¹ëŒ€í™”í•´ ë³´ì„¸ìš”!\r\n",
    "\r\n",
    "[1]: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html?utm_source=chatgpt.com \"AdamW â€” PyTorch 2.7 documentation\"\r\n",
    "[2]: https://paperswithcode.com/method/adamw?utm_source=chatgpt.com \"AdamW Explained - Papers With Code\"\r\n",
    "[3]: https://originshq.com/blog/decoupled-weight-decay-regularization-bye-bye-adam-optimizer/?utm_source=chatgpt.com \"Decoupled Weight Decay Regularization: Bye Bye Adam Optimizer\"\r\n",
    "[4]: https://www.datacamp.com/tutorial/adamw-optimizer-in-pytorch?utm_source=chatgpt.com \"AdamW Optimizer in PyTorch Tutorial - DataCamp\"\r\n",
    "[5]: https://stats.stackexchange.com/questions/265400/deep-learning-how-does-beta-1-and-beta-2-in-the-adam-optimizer-affect-its-lear?utm_source=chatgpt.com \"Deep Learning: How does beta_1 and beta_2 in the Adam ...\"\r\n",
    "[6]: https://discuss.pytorch.org/t/the-impact-of-beta-value-in-adam-optimizer/153757?utm_source=chatgpt.com \"The impact of Beta value in adam optimizer - PyTorch Forums\"\r\n",
    "[7]: https://www.kdnuggets.com/2022/12/tuning-adam-optimizer-parameters-pytorch.html?utm_source=chatgpt.com \"Tuning Adam Optimizer Parameters in PyTorch - KDnuggets\"\r\n",
    "[8]: https://github.com/facebookresearch/qhoptim?utm_source=chatgpt.com \"facebookresearch/qhoptim: Implementations of quasi-hyperbolic ...\"\r\n",
    "[9]: https://facebookresearch.github.io/qhoptim/?utm_source=chatgpt.com \"Quasi-hyperbolic optimization â€” qhoptim 1.1.0 documentation\"\r\n",
    "[10]: https://discuss.pytorch.org/t/difference-between-adam-and-adamw-in-pytorch/109173?utm_source=chatgpt.com \"Difference between Adam and AdamW in pytorch - nlp\"\r\n",
    "[11]: https://openreview.net/pdf/d2ea4b4ed5c48630b8c757bb107e9d470a5957ff.pdf?utm_source=chatgpt.com \"[PDF] quasi-hyperbolic momentum and adam for - OpenReview\"\r\n",
    "[12]: https://paperswithcode.com/method/qhadam?utm_source=chatgpt.com \"QHAdam Explained | Papers With Code\"\r\n",
    "[13]: https://facebookresearch.github.io/qhoptim/pyt.html?utm_source=chatgpt.com \"Quasi-hyperbolic optimizers for PyTorch â€” qhoptim 1.1.0 ...\"\r\n",
    "[14]: https://github.com/iprally/qhadamw-pytorch?utm_source=chatgpt.com \"QHAdamW: Optimizer combining QHAdam and AdamW - GitHub\"\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94274591-648a-4c9d-a6c2-526a693456e2",
   "metadata": {},
   "source": [
    "**Explaining L2 regularization and L2 norm**\r\n",
    "\r\n",
    "L2 ì •ê·œí™”ì™€ L2 ë…¸ë¦„ì€ ê´€ë ¨ì´ ìˆì§€ë§Œ ë™ì¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. L2 ë…¸ë¦„ì€ ë²¡í„°ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬, ì¦‰ ê° ê°’ì˜ ì œê³±í•©ì— ëŒ€í•œ ì œê³±ê·¼ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë°˜ë©´ L2 ì •ê·œí™”ëŠ” ê°€ì¤‘ì¹˜ì— ëŒ€í•´ L2 ë…¸ë¦„ì˜ ì œê³±ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì œì–´í•©ë‹ˆë‹¤. ì¦‰, L2 ì •ê·œí™”ëŠ” L2 ë…¸ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì§€ë§Œ, ì œì•½ì„ ì¶”ê°€í•˜ëŠ” ë°©ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤. ì´ ë‘˜ì€ ì—°ê²°ë˜ì–´ ìˆì§€ë§Œ ê°œë…ì ìœ¼ë¡œ ë‹¤ë¦…ë‹ˆë‹¤.\r\n",
    "\r\n",
    "\r\n",
    "6ì´ˆ ë™ì•ˆ ìƒê°í•¨\r\n",
    "\r\n",
    "\r\n",
    "L2 ì •ê·œí™”(L2 regularization)ì™€ L2 ë…¸ë¦„(L2 norm)ì€ ë°€ì ‘í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆì§€ë§Œ, **ë™ì¼í•œ ê°œë…ì€ ì•„ë‹™ë‹ˆë‹¤**.\r\n",
    "\r\n",
    "## 1. L2 ë…¸ë¦„ (Euclidean Norm)\r\n",
    "\r\n",
    "* ë²¡í„° $w = (w_1, w_2, \\dots, w_n)$ì— ëŒ€í•´\r\n",
    "\r\n",
    "  $$\r\n",
    "    \\|w\\|_2 = \\sqrt{\\,w_1^2 + w_2^2 + \\cdots + w_n^2\\,}\r\n",
    "  $$\r\n",
    "\r\n",
    "  ìœ¼ë¡œ ì •ì˜ë˜ëŠ” **ìœ í´ë¦¬ë“œ ê±°ë¦¬** í˜¹ì€ **2-ë…¸ë¦„**ì…ë‹ˆë‹¤ ([Kaggle][1]).\r\n",
    "* ê¸°í•˜í•™ì ìœ¼ë¡œ, ì›ì ì—ì„œ ë²¡í„°ê¹Œì§€ì˜ ì§ì„  ê±°ë¦¬ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\r\n",
    "\r\n",
    "## 2. L2 ì •ê·œí™” (L2 Regularization)\r\n",
    "\r\n",
    "* ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ **ê³¼ì í•©(overfitting)** ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì†ì‹¤ í•¨ìˆ˜ì— **L2 ë…¸ë¦„ì˜ ì œê³±**ì„ íŒ¨ë„í‹°ë¡œ ì¶”ê°€í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤ ([Built In][2]).\r\n",
    "* ì˜ˆë¥¼ ë“¤ì–´, íšŒê·€ ì†ì‹¤ $L_{\\text{data}}(w)$ì— ëŒ€í•´\r\n",
    "\r\n",
    "  $$\r\n",
    "    L(w) = L_{\\text{data}}(w) + \\lambda \\,\\|w\\|_2^2\r\n",
    "  $$\r\n",
    "\r\n",
    "  ì™€ ê°™ì´ ê°€ì¤‘ì¹˜ $w$ì˜ **ì œê³± L2 ë…¸ë¦„**ì„ ë”í•´ ì¤ë‹ˆë‹¤.\r\n",
    "* ì´ ì œê³± í•­ ë•Œë¬¸ì— **ridge regression(ëŠ¥í˜• íšŒê·€)** ë¼ê³ ë„ ë¶ˆë¦½ë‹ˆë‹¤.\r\n",
    "\r\n",
    "## 3. ì°¨ì´ì  ìš”ì•½\r\n",
    "\r\n",
    "| êµ¬ë¶„ | L2 ë…¸ë¦„ (`â€–wâ€–â‚‚`)        | L2 ì •ê·œí™” (`Î»â€–wâ€–â‚‚Â²`)      |\r\n",
    "| -- | --------------------- | ---------------------- |\r\n",
    "| ì •ì˜ | ë²¡í„° í¬ê¸°(ê±°ë¦¬)             | ì†ì‹¤ í•¨ìˆ˜ì— ë”í•˜ëŠ” íŒ¨ë„í‹° í•­       |\r\n",
    "| ìˆ˜ì‹ | $\\sqrt{\\sum_i w_i^2}$ | $\\lambda \\sum_i w_i^2$ |\r\n",
    "| ëª©ì  | ê±°ë¦¬Â·ê¸¸ì´ ì¸¡ì •              | íŒŒë¼ë¯¸í„° ì¶•ì†ŒÂ·ê³¼ì í•© ë°©ì§€         |\r\n",
    "\r\n",
    "* **L2 ì •ê·œí™”**ëŠ” **L2 ë…¸ë¦„**ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” â€œë…¸ë¦„ì˜ ì œê³±â€ì„ ì†ì‹¤ì— ë”í•˜ëŠ” **ì •ê·œí™” ê¸°ë²•**ì…ë‹ˆë‹¤ ([Analytics Vidhya][3]).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "ë”°ë¼ì„œ â€œL2 ì •ê·œí™”ê°€ L2 ë…¸ë¦„ì¸ê°€?â€ë¼ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì€:\r\n",
    "\r\n",
    "> **L2 ì •ê·œí™”ëŠ” L2 ë…¸ë¦„ì„ ì´ìš©í•´ íŒ¨ë„í‹° í•­ì„ ì¶”ê°€í•˜ëŠ” ê¸°ë²•ì´ë©°, L2 ë…¸ë¦„ ìì²´ì™€ëŠ” ëª©ì ê³¼ ìˆ˜ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤.**\r\n",
    "\r\n",
    "[1]: https://www.kaggle.com/code/residentmario/l1-norms-versus-l2-norms?utm_source=chatgpt.com \"L1 Norms versus L2 Norms - Kaggle\"\r\n",
    "[2]: https://builtin.com/data-science/l2-regularization?utm_source=chatgpt.com \"L1 and L2 Regularization Methods, Explained | Built In\"\r\n",
    "[3]: https://www.analyticsvidhya.com/blog/2024/01/vector-norms-in-machine-learning-decoding-l1-and-l2-norms/?utm_source=chatgpt.com \"Vector Norms in Machine Learning: Decoding L1 and L2 Norms\"\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac782e-310a-4ab5-958e-917fd062e9ec",
   "metadata": {},
   "source": [
    "\n",
    "\r\n",
    "ì‚¬ìš©ìëŠ” \"íšŒê·€ì†ì‹¤ê°’+ì†ì‹¤í•¨ìˆ˜\\*L2ë…¸ë¦„ ê°’ì´ L2 ì •ê·œí™”ì¸ê°€?\"ë¼ê³  ë¬»ê³  ìˆìŠµë‹ˆë‹¤. L2 ì •ê·œí™”ëŠ” ì†ì‹¤ í•¨ìˆ˜ì— L2 ë…¸ë¦„ ì œê³±ì— Î» ê°’ì„ ê³±í•œ í•­ì„ ë”í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ê·¸ë“¤ì€ íšŒê·€ì†ì‹¤ê°’ê³¼ L2ë…¸ë¦„ì„ ê³±í•œë‹¤ê³  ì–¸ê¸‰í–ˆì§€ë§Œ, ì˜¬ë°”ë¥¸ ì‹ì€ ì „ì²´ ì†ì‹¤ = íšŒê·€ì†ì‹¤ + Î» \\* ||w||2^2ì…ë‹ˆë‹¤. ê°„ë‹¨í•˜ê²Œ ë§í•˜ë©´ ë„¤, L2 ì •ê·œí™”ëŠ” ê°€ì¤‘ì¹˜ì˜ L2 ë…¸ë¦„ ì œê³±ì— Î»ë¥¼ ê³±í•´ ì†ì‹¤ í•¨ìˆ˜ì— ì¶”ê°€í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\r\n",
    "\r\n",
    "**ê²€í†  ì¤‘**\r\n",
    "\r\n",
    "ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆì§€ë§Œ, ê°€ì´ë“œë¼ì¸ì„ ë”°ë¼ ì°¸ê³ ë¬¸í—Œì„ ì œê³µí•´ì•¼ í•´ìš”. ì´ë²ˆì—ëŠ” ë™ì ì¸ ë‰´ìŠ¤ê°€ ì•„ë‹ˆë¼ ì¼ë°˜ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ê°œë…ì´ë‹ˆê¹Œ ì›¹ì„ ì‚¬ìš©í•  í•„ìš”ëŠ” ì—†ì„ ê²ƒ ê°™ë„¤ìš”. ê·¸ë ‡ì§€ë§Œ, ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê³ í’ˆì§ˆ ìë£Œì—ì„œ ìµœì†Œ 3ê°œì˜ ì¸ìš©ì„ í™•ë³´í•´ì•¼ í•˜ë¯€ë¡œ, \"L2 regularization formula\"ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”. ê·¸ë˜ë„ ì €ëŠ” ì´ ê°œë…ì— ëŒ€í•´ ì¶©ë¶„íˆ ìì‹ ì´ ìˆë„¤ìš”.\r\n",
    "\r\n",
    "\r\n",
    "7ì´ˆ ë™ì•ˆ ìƒê°í•¨\r\n",
    "\r\n",
    "\r\n",
    "ì•„ì£¼ ì •í™•í•©ë‹ˆë‹¤. L2 ì •ê·œí™”(L2 regularization)ëŠ” **ì›ë˜ì˜ ì†ì‹¤ í•¨ìˆ˜**(ì˜ˆ: íšŒê·€ ì†ì‹¤)ì— **L2 ë…¸ë¦„ì˜ ì œê³±**ì„ ê³±í•œ í•­ì„ ë”í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\r\n",
    "\r\n",
    "ì¦‰, ëª¨ë¸ íŒŒë¼ë¯¸í„° $w$ì— ëŒ€í•´\r\n",
    "\r\n",
    "$$\r\n",
    "\\text{Total Loss}\r\n",
    "=\\;L_{\\rm data}(w)\\;+\\;\\lambda\\,\\|w\\|_2^2\r\n",
    "$$\r\n",
    "\r\n",
    "ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ë°”ë¡œ L2 ì •ê·œí™”ì…ë‹ˆë‹¤.\r\n",
    "\r\n",
    "* ì—¬ê¸°ì„œ $L_{\\rm data}(w)$ëŠ” ì˜ˆì»¨ëŒ€ **MSE(Mean Squared Error)** ê°™ì€ íšŒê·€ ì†ì‹¤ì…ë‹ˆë‹¤.\r\n",
    "* $\\lambda>0$ëŠ” **ì •ê·œí™” ê³„ìˆ˜**(weight decay)ë¡œ, íŒ¨ë„í‹° ê°•ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤ ([Google for Developers][1], [Wikipedia][2]).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## ì™œ â€œì œê³±â€ ë…¸ë¦„ì¸ê°€?\r\n",
    "\r\n",
    "* L2 ë…¸ë¦„ $\\|w\\|_2$ëŠ” $\\sqrt{\\sum_i w_i^2}$ ì´ì§€ë§Œ, ìˆ˜ì‹ í¸ì˜ë¥¼ ìœ„í•´ **ë…¸ë¦„ì˜ ì œê³±** $\\|w\\|_2^2=\\sum_i w_i^2$ ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\r\n",
    "* ì œê³±ì„ ì“°ë©´ **ë¯¸ë¶„**ì´ ê°„ë‹¨í•´ì§€ê³ , ì—°ì†Â·ë³¼ë¡(convex) ìµœì í™”ê°€ ë³´ì¥ë©ë‹ˆë‹¤ ([Medium][3], [Stack Overflow][4]).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## ì •ë¦¬\r\n",
    "\r\n",
    "1. **ê¸°ë³¸ ì†ì‹¤**: íšŒê·€ ë¬¸ì œë¼ë©´ $L_{\\rm data}=\\mathrm{MSE}$.\r\n",
    "2. **ì •ê·œí™” í•­**: $\\lambda\\,\\|w\\|_2^2$.\r\n",
    "3. **ì´ ì†ì‹¤**:\r\n",
    "\r\n",
    "   $$\r\n",
    "     \\mathrm{MSE}(y,\\hat y)\r\n",
    "     \\;+\\;\\lambda\\,\\sum_i w_i^2.\r\n",
    "   $$\r\n",
    "\r\n",
    "ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ê³¼ë„í•˜ê²Œ í° ê°€ì¤‘ì¹˜ë¥¼ ê°–ì§€ ëª»í•˜ë„ë¡ **íŒ¨ë„í‹°**ë¥¼ ë¶€ì—¬í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ê²Œ ë©ë‹ˆë‹¤. ([Built In][5], [Medium][3])\r\n",
    "\r\n",
    "[1]: https://developers.google.com/machine-learning/crash-course/overfitting/regularization?utm_source=chatgpt.com \"Overfitting: L2 regularization | Machine Learning\"\r\n",
    "[2]: https://en.wikipedia.org/wiki/Regularization_%28mathematics%29?utm_source=chatgpt.com \"Regularization (mathematics)\"\r\n",
    "[3]: https://medium.com/%40alejandro.itoaramendia/l1-and-l2-regularization-part-1-a-complete-guide-51cf45bb4ade?utm_source=chatgpt.com \"L1 and L2 Regularization (Part 1): A Complete Guide - Medium\"\r\n",
    "[4]: https://stackoverflow.com/questions/58905671/compute-the-loss-of-l1-and-l2-regularization?utm_source=chatgpt.com \"Compute the Loss of L1 and L2 regularization - Stack Overflow\"\r\n",
    "[5]: https://builtin.com/data-science/l2-regularization?utm_source=chatgpt.com \"L1 and L2 Regularization Methods, Explained | Built In\"\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e78ba-820f-427b-8f7f-47455481610e",
   "metadata": {},
   "source": [
    "\n",
    "\r\n",
    "\r\n",
    "`Î»`(ëŒë‹¤)ëŠ” L2 ì •ê·œí™” í•­ì˜ **ê°•ë„**ë¥¼ ì¡°ì ˆí•˜ëŠ” **í•˜ì´í¼íŒŒë¼ë¯¸í„°**ì…ë‹ˆë‹¤.\r\n",
    "\r\n",
    "* **ì—­í• **\r\n",
    "\r\n",
    "  * ì›ë˜ ì†ì‹¤(ì˜ˆ: MSE)ì— ë”í•´ì§€ëŠ” `Î»â€–wâ€–â‚‚Â²` í•­ì˜ ê³„ìˆ˜ë¡œ,\r\n",
    "  * `Î»`ê°€ í¬ë©´ ê°€ì¤‘ì¹˜ `w`ë¥¼ ì‘ê²Œ ë§Œë“œëŠ” ì œì•½ì´ ê°•í•´ì ¸ **ëª¨ë¸ì´ ë‹¨ìˆœí•´ì§€ê³  ê³¼ì í•©ì´ ì¤„ì–´ë“­ë‹ˆë‹¤**.\r\n",
    "  * ë°˜ëŒ€ë¡œ `Î»`ê°€ ì‘ìœ¼ë©´ ì •ê·œí™” íš¨ê³¼ê°€ ì•½í•´ì ¸ **ëª¨ë¸ì´ ë°ì´í„°ì— ë” ì˜ ë§ìœ¼ë ¤ í•˜ë©°** ê³¼ì í•© ìœ„í—˜ì´ ì»¤ì§‘ë‹ˆë‹¤.\r\n",
    "\r\n",
    "* **ìˆ˜ì‹ ì˜ˆ**\r\n",
    "\r\n",
    "  $$\r\n",
    "    \\mathcal{L}(w)\r\n",
    "    = \\underbrace{\\tfrac1N\\sum_i (y_i - \\hat y_i)^2}_{\\displaystyle\\text{MSE}}\r\n",
    "    \\;+\\;\\lambda\\,\\sum_j w_j^2.\r\n",
    "  $$\r\n",
    "\r\n",
    "* **íŠœë‹ íŒ**\r\n",
    "\r\n",
    "  1. ë³´í†µ `10^{-4}`, `10^{-3}`, `10^{-2}` ì²˜ëŸ¼ **ë¡œê·¸ ìŠ¤ì¼€ì¼**ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.\r\n",
    "  2. ê²€ì¦ ë°ì´í„°ì—ì„œ ì†ì‹¤ í˜¹ì€ ì •í™•ë„ë¥¼ ê´€ì°°í•˜ë©°, **ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” Î»**ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\r\n",
    "  3. í•™ìŠµë¥ (`lr`)ê³¼ ìƒí˜¸ì‘ìš©í•˜ë¯€ë¡œ, ì¢…ì¢… **lrê³¼ Î»ë¥¼ í•¨ê»˜** ê·¸ë¦¬ë“œÂ·ëœë¤Â·ë² ì´ì§€ì•ˆ íƒìƒ‰í•©ë‹ˆë‹¤.\r\n",
    "\r\n",
    "ìš”ì•½í•˜ìë©´, `Î»`ëŠ” **ëª¨ë¸ ë³µì¡ë„**ì™€ **ì¼ë°˜í™” ì„±ëŠ¥** ê°„ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” í•µì‹¬ íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59910fc7-441d-4680-af3b-deebc6913d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6230c4f-13a7-4291-a8ea-3c860f70b16e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4da185-74da-4b4b-85ea-1c17d1cc1b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a9e85ac-0bb9-4c37-adac-5ae1abeebb5f",
   "metadata": {},
   "source": [
    "# ìŠ¤ì¼€ì¤„ëŸ¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbb245-9afb-46fb-9ca1-5edc064d3ad9",
   "metadata": {},
   "source": [
    "1. LambdaLR\n",
    "ë™ì‘: ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ lr_lambda(epoch)ê°€ ë°˜í™˜í•˜ëŠ” ê°’ìœ¼ë¡œ ë§¤ ì—í­ë§ˆë‹¤ í•™ìŠµë¥ ì„ ê³±í•´ ì¡°ì •í•©ë‹ˆë‹¤ \n",
    "pytorch-argus.readthedocs.io.\n",
    "\n",
    "ìš©ë„: ì•„ì£¼ ìœ ì—°í•œ ì»¤ìŠ¤í…€ ìŠ¤ì¼€ì¤„ë§ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bdfbba44-c278-410a-b3c5-182674d1b81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "scheduler=LambdaLR(optimy,lr_lambda=lambda t:0.95**t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b11dd7-8e91-403d-b1fb-38532cf9dd38",
   "metadata": {},
   "source": [
    "2. MultiplicativeLR\n",
    "ë™ì‘: lr_lambda(epoch) í•¨ìˆ˜ë¥¼ ë§¤ ì—í­ë§ˆë‹¤ í˜¸ì¶œí•´, í˜„ì¬ í•™ìŠµë¥ ì— ê·¸ ê²°ê³¼ë¥¼ ê³±í•©ë‹ˆë‹¤ \n",
    "PyTorch\n",
    "\n",
    "ìš©ë„: ì§€ìˆ˜ì (Exponential) ê°ì†Œ ì™¸ì— ì„ì˜ ê³¡ì„  í˜•íƒœê°€ í•„ìš”í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "535f4dd6-a0f9-4c29-b31f-977c4120a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import MultiplicativeLR\n",
    "scheduler=MultiplicativeLR(optimy, lr_lambda=lambda t:0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cee116-097e-4f89-ae9d-9803ad657666",
   "metadata": {},
   "source": [
    "3. StepLR\n",
    "ë™ì‘: ë§¤ step_size ì—í­ë§ˆë‹¤ í•™ìŠµë¥ ì„ gammaë§Œí¼ ê³±í•´ ê°ì†Œì‹œí‚µë‹ˆë‹¤ \n",
    "PyTorch\n",
    ".\n",
    "\n",
    "ìš©ë„: ì¼ì • ì£¼ê¸°ë¡œ ê°„í—ì  ëŒ€í­ ê°ì†Œê°€ í•„ìš”í•  ë•Œ ì í•©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a4239bff-bdb7-417e-a848-e787b38210be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler=StepLR(optimy,step_size=10,gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b6538-e8ea-4a74-a0c4-98772e163a3d",
   "metadata": {},
   "source": [
    "4. MultiStepLR\n",
    "ë™ì‘: milestones ë¦¬ìŠ¤íŠ¸ì— ì§€ì •ëœ ì—í­ì—ì„œ í•™ìŠµë¥ ì„ gammaë§Œí¼ ê°ì†Œì‹œí‚µë‹ˆë‹¤ \n",
    "pytorch-argus.readthedocs.io\n",
    ".\n",
    "\n",
    "ìš©ë„: ë¹„ê· ë“±í•œ ì—í­ ê°„ê²©ìœ¼ë¡œ ìŠ¤ì¼€ì¤„ë§í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5b5a49ce-7b94-4974-86c0-7d6d716fb26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "scheduler=MultiStepLR(optimy, milestones=[30, 80], gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568edbc-bcfe-4ae1-9f98-42bc4f7080dc",
   "metadata": {},
   "source": [
    "5. ExponentialLR\n",
    "ë™ì‘: ë§¤ ì—í­ë§ˆë‹¤ í•™ìŠµë¥ ì„ gammaë§Œí¼ ì§€ìˆ˜ì ìœ¼ë¡œ ê°ì†Œì‹œí‚µë‹ˆë‹¤ \n",
    "pytorch-argus.readthedocs.io\n",
    ".\n",
    "\n",
    "ìš©ë„: ì™„ë§Œí•˜ì§€ë§Œ ì§€ì†ì ì¸ í•™ìŠµë¥  ê°ì†Œê°€ í•„ìš”í•  ë•Œ ì í•©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "96960577-2665-447e-af31-c074f0cc2427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "scheduler=ExponentialLR(optimy, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb651a-df8f-4344-bc67-d6502bde35ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5d66e16-9684-4016-9c8d-2de70e625d75",
   "metadata": {},
   "source": [
    "# ì£¼ê¸°ì  ë° ì„±ëŠ¥ ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d50b7-28f5-430f-ac49-5d0738b5783c",
   "metadata": {},
   "source": [
    "6. CosineAnnealingLR\n",
    "ë™ì‘: ì½”ì‚¬ì¸ ê³¡ì„ ì„ ë”°ë¼ í•™ìŠµë¥ ì„ eta_minì—ì„œ eta_max ì‚¬ì´ë¡œ ì¡°ì ˆí•©ë‹ˆë‹¤ \n",
    "PyTorch\n",
    ".\n",
    "\n",
    "ìš©ë„: ë¶€ë“œëŸ¬ìš´ ì£¼ê¸°ì  ê°ì†Œê°€ í•„ìš”í•œ ëŒ€ê·œëª¨ í•™ìŠµì— ìœ ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f92ea97e-9970-4cbe-b64f-64c31bb08c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler=CosineAnnealingLR(optimy, T_max=50, eta_min=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a8a8c-1732-4451-8ac7-8986701351fb",
   "metadata": {},
   "source": [
    "7. CosineAnnealingWarmRestarts\n",
    "ë™ì‘: CosineAnnealingì„ ë°˜ë³µ(restart)í•´ í•™ìŠµë¥ ì„ ì—¬ëŸ¬ ë²ˆ ë¶€í™œì‹œí‚¤ë©° ê°ì†Œì‹œí‚µë‹ˆë‹¤ \n",
    "PyTorch\n",
    ".\n",
    "\n",
    "ìš©ë„: í•œ ë²ˆì˜ ì£¼ê¸°ê°€ ì•„ë‹Œ ì—¬ëŸ¬ ì£¼ê¸°ë¡œ í•™ìŠµë¥ ì„ ì¬ì¦í­í•˜ê³ ì í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a9fd59d0-c78d-4783-8ce7-76907c8f83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "scheduler=CosineAnnealingWarmRestarts(optimy, T_0=10,T_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d63a851-1639-437f-a3b1-a51518899748",
   "metadata": {},
   "source": [
    "8. ReduceLROnPlateau\n",
    "ë™ì‘: ê²€ì¦(loss ë˜ëŠ” accuracy)ì´ patience ì—í­ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµë¥ ì„ factorë§Œí¼ ê°ì†Œì‹œí‚µë‹ˆë‹¤ \n",
    "GeeksforGeeks\n",
    ".\n",
    "\n",
    "ìš©ë„: í•™ìŠµ ì •ì²´ êµ¬ê°„ì— ìë™ ë°˜ì‘í•´ LRì„ ì¤„ì´ë ¤ í•  ë•Œ ì í•©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cd2e6bad-0a57-4942-a60d-b4da75f3f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler=ReduceLROnPlateau(optimy, mode='min',factor=0.5,patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df763ed-a781-481f-9a99-42477014296f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "816d715a-9e3a-4f74-8b36-d637cff0786c",
   "metadata": {},
   "source": [
    "# ë°°ì¹˜ ë‹¨ìœ„ ìŠ¤ì¼€ì¤„ëŸ¬ ë° ì²´ì´ë‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b12e9-77f4-4926-bc6f-f60087a2523d",
   "metadata": {},
   "source": [
    "9. CyclicLR\n",
    "ë™ì‘: í•™ìŠµë¥ ì„ â€œìµœì €â†’ìµœê³ â†’ìµœì €â€ ì‚¬ì´ì—ì„œ ì‚¼ê°í˜•(triangle) ëª¨ì–‘ìœ¼ë¡œ ë°˜ë³µ ë³€í™”ì‹œí‚µë‹ˆë‹¤ \n",
    "PyTorch\n",
    ".\n",
    "\n",
    "ìš©ë„: ë¹ ë¥¸ ìˆ˜ë ´ê³¼ íƒˆì¶œì„ ìœ„í•´ ì£¼ê¸°ì ìœ¼ë¡œ LRì„ ë³€ë™ì‹œí‚¬ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "dd12d332-8d95-49b2-ba92-d817e718d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "scheduler=CyclicLR(optimy, base_lr=1e-5,max_lr=1e-3,step_size_up=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4df2dc-f6e0-4b54-addf-1b2058ba2008",
   "metadata": {},
   "source": [
    "10. OneCycleLR\n",
    "ë™ì‘: í•™ìŠµë¥ ì„ â€œì´ˆê¸°â†’ìµœê³ â†’ì´ˆê¸°ë³´ë‹¤ ë‚®ì€ ìµœì €â€ë¡œ ë°°ì¹˜ ë‹¨ìœ„(batch)ì—ì„œ ì¡°ì ˆí•©ë‹ˆë‹¤ \n",
    "PyTorch\n",
    ".\n",
    "\n",
    "ìš©ë„: Super-convergence ê¸°ë²•ì„ í™œìš©í•´ ë§¤ìš° ë¹ ë¥¸ í•™ìŠµì´ í•„ìš”í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "358be6d7-761b-4a62-9f48-1736da6386b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "scheduler=OneCycleLR(optimy,max_lr=1e-3,total_steps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d2dfe-b53c-4104-8a9a-2dc31d7a6664",
   "metadata": {},
   "source": [
    "11. ChainedScheduler\n",
    "ë™ì‘: ì—¬ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•´ step()ì„ í•œ ë²ˆì— í˜¸ì¶œí•  ìˆ˜ ìˆê²Œ í•´ ì¤ë‹ˆë‹¤ \n",
    "PyTorch\n",
    ".\n",
    "\n",
    "ìš©ë„: Warmup + CosineAnnealing ê°™ì´ ë‹¤ë‹¨ê³„ ìŠ¤ì¼€ì¤„ë§ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "36c4bc5f-c7f9-4ea0-995b-b8226d6752db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer=optim.AdamW(model.parameters(), lr=lr,weight_decay=1e-2)\n",
    "total_warmup_steps=10\n",
    "# total_warmup_steps ë§Œí¼ 0 â†’ 1 ë¹„ìœ¨ë¡œ ì¦ê°€\n",
    "def warmup_lambda(step):\n",
    "    return min((step + 1) / total_warmup_steps, 1.0)\n",
    "\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f18ea148-47d9-43b6-b078-4b2c4bdf888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "total_epochs=3\n",
    "minimum_lr=9\n",
    "cosine_scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=total_epochs,  # ì½”ì‚¬ì¸ í•œ ì£¼ê¸°ë‹¹ ì—í­ ìˆ˜\n",
    "    eta_min=minimum_lr   # ìµœì € í•™ìŠµë¥ \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "76206a11-936a-4f00-8ae6-a687077dbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ChainedScheduler\n",
    "scheduler = ChainedScheduler([warmup_scheduler, cosine_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "945a6355-4fcb-4193-9e83-37ccc1701c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ChainedScheduler\n",
    "\n",
    "# ì‚¬ìš©ìê°€ ì •ì˜í•œ ì—¬ëŸ¬ ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "warmup_scheduler   = LambdaLR(optimy,lr_lambda=lambda t:0.95**t)\n",
    "cosine_scheduler   = ExponentialLR(optimy, gamma=0.95)\n",
    "plateau_scheduler  = MultiplicativeLR(optimy, lr_lambda=lambda t:0.9)\n",
    "\n",
    "# ì›í•˜ëŠ” ìˆœì„œì™€ ê°œìˆ˜ë¡œ ë¬¶ì–´ í•œ ì„¸íŠ¸ë¡œ ë™ì‘ì‹œí‚¤ê¸°\n",
    "scheduler = ChainedScheduler([\n",
    "    warmup_scheduler,\n",
    "    cosine_scheduler,\n",
    "    plateau_scheduler\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad34a1-47bb-4dcc-b35f-f6664f5554b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31511508-667f-4a7b-a45b-1d3b9e811c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1b01d-7711-4315-9a43-b2746c599acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b948c-09ea-45ab-b49e-edd61b663e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76944b-3515-49d2-9cea-02a100bfc104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566758d1-31dc-4c8f-bf85-2629aec9b134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72aafca-c30f-4f9f-823f-5e1b144dac4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5e179-fbbf-459d-8681-f01578a709e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CUDA 12.4)",
   "language": "python",
   "name": "cuda124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
