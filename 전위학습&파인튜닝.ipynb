{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47277be-d2b1-4636-9af0-5504c679ac97",
   "metadata": {},
   "source": [
    "# Transfer Learning 개념 이해:\n",
    "\n",
    "사전 학습된 모델(Pre-trained Model)을 활용하여, 기존에 대규모 데이터셋으로 학습된 모델의 가중치를 재사용하고, 이를 우리의 특정 문제에 맞게 파인튜닝(Fine-tuning)하는 개념을 이해합니다.\n",
    "\n",
    "# Pre-trained 모델 활용:\n",
    "\n",
    "torchvision에서 제공하는 Pre-trained 모델(예: ResNet, VGG 등)을 불러와서, 우리 문제(예: MNIST, CIFAR-10 등)에 맞게 마지막 레이어를 수정하는 방법을 학습합니다.\n",
    "\n",
    "# 파인튜닝 전략:\n",
    "\n",
    "모델의 일부 레이어를 고정(Frozen)하고, 마지막 몇 개 레이어만 학습하도록 설정하는 방법, 그리고 전체 네트워크를 미세 조정하는 방법을 배웁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba9a05b-a699-4431-921a-e2e259b783da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [02:13<00:00, 1274319.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\JH\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\JH/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:08<00:00, 5.42MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Pre-trained ResNet18을 CIFAR-10에 맞게 파인튜닝\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader as loader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform_train=transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_data=datasets.CIFAR10(root=\"./data\",train=True, download=True,transform=transform_train)\n",
    "test_data=datasets.CIFAR10(root=\"./data\",train=True,download=True, transform=transform_test)\n",
    "\n",
    "train_loader=loader(train_data,batch_size=64,shuffle=True)\n",
    "test_loader=loader(test_data, batch_size=1000, shuffle=True)\n",
    "\n",
    "model=models.resnet18(pretrained=True)# 2. Pre-trained 모델 불러오기\n",
    "\n",
    "# 3. Transfer Learning 적용: 마지막 FC 레이어 수정 (CIFAR-10은 10개 클래스)\n",
    "num_ftrs=model.fc.in_features# 기존 fc layer의 입력 차원\n",
    "model.fc=nn.Linear(num_ftrs,10)# CIFAR-10에 맞게 출력 클래스 수 수정\n",
    "\n",
    "# 4. 손실 함수 및 옵티마이저 설정\n",
    "criter=nn.CrossEntropyLoss()\n",
    "optimy=optim.Adam(model.parameters(),lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f209dc0-bdec-4dde-878e-d81a8e2e301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode(model, loader, criter, optimy, num_epoch=10):\n",
    "    model.train()\n",
    "    history=[]\n",
    "    for epoch in range(num_epoch):\n",
    "        total=0.0\n",
    "        for data, target in loader:\n",
    "            optimy.zero_grad()\n",
    "            out=model(data)\n",
    "            loss=criter(out,target)\n",
    "            loss.backward()\n",
    "            optimy.step()\n",
    "            total+=loss.item()\n",
    "        avg_loss=total/len(loader)\n",
    "        history.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epoch}, Training Loss: {avg_loss:.4f}\")\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84092d79-9076-4eba-9e0a-14998ad11477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mode(model, loader):\n",
    "    model.eval()\n",
    "    correct=0.0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            out=model(data)\n",
    "            pred=out.argmax(dim=1)\n",
    "            correct+=pred.eq(target).sum().items()\n",
    "            total+=data.size(0)\n",
    "    acc=100.0*correct/total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f5a2976-3ce8-446e-813f-58be0e2109f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train\u001b[38;5;241m=\u001b[39mtrain_mode(model, train_loader, criter, optimy, num_epoch)\n\u001b[0;32m      3\u001b[0m test_acc\u001b[38;5;241m=\u001b[39meval_mode(model, test_loader)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 10\u001b[0m, in \u001b[0;36mtrain_mode\u001b[1;34m(model, loader, criter, optimy, num_epoch)\u001b[0m\n\u001b[0;32m      8\u001b[0m out\u001b[38;5;241m=\u001b[39mmodel(data)\n\u001b[0;32m      9\u001b[0m loss\u001b[38;5;241m=\u001b[39mcriter(out,target)\n\u001b[1;32m---> 10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     11\u001b[0m optimy\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     12\u001b[0m total\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epoch=10\n",
    "train=train_mode(model, train_loader, criter, optimy, num_epoch)\n",
    "test_acc=eval_mode(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1306d3-5094-4f9f-b06a-25045b2c3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,num_epoch+1),train,marker='o')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c648e9da-d3e1-47ab-ad87-d1224908bfb7",
   "metadata": {},
   "source": [
    "Transfer Learning과 Fine-tuning 적용 과정\n",
    "모델 불러오기:\n",
    "사전 학습된 모델(예: ResNet, VGG 등)을 불러옵니다.\n",
    "\n",
    "레이어 고정 여부 결정:\n",
    "\n",
    "Feature Extraction: 전체 모델의 가중치를 고정시키고, 마지막 레이어만 학습합니다.\n",
    "\n",
    "Fine-tuning: 일부 레이어는 고정하고, 나머지 레이어는 미세 조정합니다.\n",
    "\n",
    "출력 레이어 수정:\n",
    "새로운 데이터셋의 클래스 수에 맞게 마지막 레이어를 수정합니다.\n",
    "\n",
    "학습률 및 옵티마이저 설정:\n",
    "미세 조정을 위해 학습률을 낮게 설정하는 것이 일반적입니다.\n",
    "\n",
    "모델 학습:\n",
    "새 데이터셋으로 모델을 학습시킵니다.\n",
    "\n",
    "평가 및 개선:\n",
    "모델 성능을 평가하고, 필요한 경우 추가적인 튜닝(예: 데이터 증강, 정규화) 등을 진행합니다.\n",
    "==============================================================================================\n",
    "Transfer Learning은 사전 학습된 모델의 일반화된 특징을 활용하는 방법입니다.\n",
    "\n",
    "Fine-tuning은 사전 학습된 모델의 일부 또는 전체를 새로운 데이터셋에 맞게 미세하게 조정하는 방법입니다.\n",
    "\n",
    "두 접근법은 데이터의 양과 문제의 특성에 따라 선택하며, 보통 학습률을 낮추고, 필요한 부분의 가중치를 업데이트하여 최적의 성능을 도출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13901b3d-1b80-4146-95ff-910e7d4254cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "310988aa-2363-4db5-a317-591bfb778818",
   "metadata": {},
   "source": [
    "Transfer Learning (전이 학습):\n",
    "\n",
    "개념:\n",
    "대규모 데이터셋(예: ImageNet)으로 미리 학습된 모델의 가중치를 가져와서, 새로운 작업에 활용하는 것입니다.\n",
    "\n",
    "적용 방식:\n",
    "보통 사전 학습된 모델의 대부분(특히 초기 또는 중간 레이어)은 그대로 사용하고, 마지막 출력 레이어(또는 일부 레이어)를 새로운 문제에 맞게 교체합니다.\n",
    "\n",
    "목적:\n",
    "적은 양의 데이터로도 좋은 성능을 낼 수 있도록, 일반적인 특징 추출 능력을 재사용하는 데 중점을 둡니다.\n",
    "================================================================================================\n",
    "Fine-tuning (미세 조정):\n",
    "\n",
    "개념:\n",
    "Transfer Learning의 한 형태로, 사전 학습된 모델의 가중치를 가져온 후, 새로운 데이터셋에 맞게 모델 전체 또는 일부를 추가로 학습(조정)시키는 방법입니다.\n",
    "\n",
    "적용 방식:\n",
    "\n",
    "Feature Extraction: 대부분의 레이어를 고정(freeze)하고, 마지막 몇 개 레이어만 새 데이터에 맞게 학습하는 경우\n",
    "\n",
    "미세 조정: 사전 학습된 모델의 일부 또는 전체 가중치를 낮은 학습률로 업데이트하여 새로운 데이터셋에 맞게 최적화하는 경우\n",
    "\n",
    "목적:\n",
    "모델이 새로운 작업에 더 잘 적응하도록, 사전 학습된 가중치를 조금씩 변경해 최적의 성능을 도출하는 데 중점을 둡니다.\n",
    "====================================================================================================================================================================================================\n",
    "요약:\n",
    "\n",
    "Transfer Learning은 사전 학습된 모델을 그대로 사용하거나 마지막 레이어만 교체하는 방식으로, 모델의 일반적인 특징 추출 능력을 재사용하는 것입니다.\n",
    "\n",
    "Fine-tuning은 Transfer Learning 이후, 모델의 일부 또는 전체 가중치를 미세하게 조정하여 새로운 데이터셋에 맞게 최적화하는 과정입니다.\n",
    "\n",
    "즉, Fine-tuning은 Transfer Learning의 한 단계로 볼 수 있으며, 전이 학습으로 모델을 가져온 후 추가로 미세 조정하는 과정을 통해 모델 성능을 한층 더 향상시키는 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6572a-e364-4a8b-8600-4d5f3e715b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c8effa0-9e98-4fd3-8ab8-314bef1a30c6",
   "metadata": {},
   "source": [
    "# 허깅페이스 ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04443f-4edf-4cc2-852b-465f9395945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 필요한 라이브러리 불러오기\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1. 데이터셋 불러오기: 여기서는 GLUE의 MRPC 작업을 사용합니다.\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# 2. 토크나이저 로드: 사전 학습된 BERT 토크나이저 사용\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 3. 데이터셋 토큰화 함수 정의\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"],\n",
    "                     truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# 4. 데이터셋 토큰화 및 포맷 설정\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# 5. 사전 학습된 모델 불러오기 및 출력 레이어 수정 (num_labels 설정)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# 6. 학습 파라미터 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 7. Trainer 객체 생성 (모델, 학습 인자, 데이터셋 포함)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "# 8. 모델 파인튜닝: Trainer를 이용해 학습 실행\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94830741-b83c-4806-8fc3-772e7ce11354",
   "metadata": {},
   "source": [
    "단계별 설명\n",
    "데이터셋 불러오기:\n",
    "\n",
    "허깅페이스의 datasets 라이브러리를 사용하여 GLUE 데이터셋의 MRPC 작업을 불러옵니다.\n",
    "\n",
    "MRPC는 문장 쌍이 유사한지 아닌지 분류하는 작업입니다.\n",
    "\n",
    "토크나이저 로드:\n",
    "\n",
    "사전 학습된 \"bert-base-uncased\" 모델에 맞는 토크나이저를 불러옵니다.\n",
    "\n",
    "이 토크나이저는 입력 문장을 모델이 이해할 수 있는 토큰 시퀀스로 변환합니다.\n",
    "\n",
    "데이터셋 토큰화:\n",
    "\n",
    "두 문장을 동시에 토큰화하여, 모델의 입력으로 사용할 수 있는 input_ids와 attention_mask를 생성합니다.\n",
    "\n",
    "padding=\"max_length\"와 max_length=128을 통해 입력 길이를 일정하게 맞춥니다.\n",
    "\n",
    "데이터셋 포맷 설정:\n",
    "\n",
    "tokenized_datasets의 라벨 컬럼 이름을 \"labels\"로 변경하고, PyTorch 텐서 형식으로 설정합니다.\n",
    "\n",
    "사전 학습된 모델 불러오기 및 수정:\n",
    "\n",
    "BertForSequenceClassification 클래스는 BERT 기반의 분류 모델로, 사전 학습된 가중치를 불러오고 마지막 분류 레이어를 작업에 맞게 설정할 수 있습니다.\n",
    "\n",
    "num_labels=2로 설정하여 이진 분류 문제에 맞게 출력 레이어를 조정합니다.\n",
    "\n",
    "학습 파라미터 설정:\n",
    "\n",
    "TrainingArguments를 통해 학습 출력 디렉토리, 평가 전략, 학습률, 배치 크기, 에폭 수, 정규화(weight decay) 등의 하이퍼파라미터를 설정합니다.\n",
    "\n",
    "Trainer 객체 생성:\n",
    "\n",
    "Trainer는 허깅페이스에서 제공하는 고수준 API로, 모델 학습, 평가, 저장 등의 작업을 쉽게 처리할 수 있게 도와줍니다.\n",
    "\n",
    "학습 데이터셋과 평가 데이터셋을 함께 전달합니다.\n",
    "\n",
    "모델 파인튜닝:\n",
    "\n",
    "trainer.train()을 호출하면, 설정된 하이퍼파라미터에 따라 모델을 파인튜닝합니다.\n",
    "\n",
    "요약\n",
    "전이학습:\n",
    "\n",
    "사전 학습된 BERT 모델을 불러와서, 이미 학습된 언어 이해 능력을 새로운 분류 작업(MRPC)에 재사용합니다.\n",
    "\n",
    "파인튜닝:\n",
    "\n",
    "불러온 모델의 마지막 레이어를 새로운 작업에 맞게 수정하고, 전체 모델(또는 일부 레이어)을 새로운 데이터셋에 맞게 미세 조정합니다.\n",
    "\n",
    "허깅페이스 Trainer API:\n",
    "\n",
    "Trainer API를 사용하면, 모델 학습, 평가, 저장 등을 간단하게 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d59be1d-a8da-4cac-8a53-767544597b55",
   "metadata": {},
   "source": [
    "1. 전이 학습(Transfer Learning)의 기본 아이디어\n",
    "사전 학습된 모델 활용:\n",
    "대규모 텍스트 데이터셋(ImageNet의 이미지 모델에 해당하는, 예: BERT, RoBERTa 등)으로 미리 학습된 모델은 언어의 일반적인 문맥과 패턴을 이미 학습했습니다.\n",
    "\n",
    "출력 레이어 교체:\n",
    "원래 모델은 일반적인 작업(예: 마스킹된 단어 예측, 다음 문장 예측)으로 학습되었으므로, 새로운 작업(예: 문장 분류)의 경우 마지막 출력 레이어(분류기)를 데이터셋의 라벨 수에 맞게 수정합니다.\n",
    "\n",
    "파인튜닝:\n",
    "전이 학습 단계에서 변경된 출력 레이어를 포함해, 전체 모델이나 일부 레이어의 가중치를 새로운 작업에 맞게 미세하게 조정하는 과정입니다.\n",
    "\n",
    "Feature Extraction: 모델의 대부분을 고정하고 마지막 레이어만 학습하는 방식\n",
    "\n",
    "Fine-tuning: 전체 모델 또는 일부 층의 파라미터를 낮은 학습률로 업데이트하는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a810c4d6-ae63-42c0-87c2-43cb45b1aff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# DataLoader 생성: 배치 크기 등 설정\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m eval_loader \u001b[38;5;241m=\u001b[39m DataLoader(eval_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 옵티마이저 설정 (Fine-tuning 시 보통 낮은 학습률 사용)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader 생성: 배치 크기 등 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# 옵티마이저 설정 (Fine-tuning 시 보통 낮은 학습률 사용)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                        attention_mask=batch[\"attention_mask\"],\n",
    "                        labels=batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "#전통적인 핛급"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648bd370-9863-47f0-9c50-9527bfa7decd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71838fd9-0eb7-436a-b47f-c30c8bd50e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e4af04-1944-4b25-a82a-e6f224d45e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad49a53-b0b2-4b05-9cb2-3955fcc3eb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea07d1-65a2-453f-a2bf-63cc18a8b0f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f415edc-ca7c-4621-a97b-7a08c23d5774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 데이터 전처리 및 로딩 (CIFAR-10 예시)\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # 데이터 증강: 좌우 반전\n",
    "    transforms.RandomCrop(32, padding=4), # 데이터 증강: 랜덤 크롭\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# 2. Pre-trained 모델 불러오기: ResNet18 예시\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# 3. Transfer Learning 적용: 마지막 FC 레이어 수정 (CIFAR-10은 10개 클래스)\n",
    "num_ftrs = model.fc.in_features  # 기존 fc layer의 입력 차원\n",
    "model.fc = nn.Linear(num_ftrs, 10)  # CIFAR-10에 맞게 출력 클래스 수 수정\n",
    "\n",
    "# (옵션) 미세 조정을 위해, 앞의 일부 레이어를 고정할 수도 있습니다.\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.fc.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "# 4. 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. 간단한 학습 루프\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    train_loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
    "    return train_loss_history\n",
    "\n",
    "# 6. 평가 함수\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            preds = output.argmax(dim=1)\n",
    "            correct += preds.eq(target).sum().item()\n",
    "            total += data.size(0)\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# 7. 모델 학습 및 평가\n",
    "num_epochs = 5\n",
    "train_loss_history = train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "test_accuracy = evaluate_model(model, test_loader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# 8. 결과 시각화 (Training Loss)\n",
    "plt.plot(range(1, num_epochs+1), train_loss_history, marker='o')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Training Loss\")\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cc658-4576-43df-94b7-7de7b0729f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
