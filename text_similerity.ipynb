{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eca9294",
   "metadata": {},
   "source": [
    "# back of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "149193fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9}\n",
      "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#back of words\n",
    "\n",
    "'''\n",
    "Bag of Words란?\n",
    "Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법입니다. \n",
    "Bag of Words를 직역하면 단어들의 가방이라는 의미입니다. 단어들이 들어있는 가방을 상상해봅시다.\n",
    "갖고있는 어떤 텍스트 문서에 있는 단어들을 가방에다가 전부 넣습니다. 그 후에는 이 가방을 흔들어 단어들을 섞습니다.\n",
    "만약, 해당 문서 내에서 특정 단어가 N번 등장했다면, 이 가방에는 그 특정 단어가 N개 있게됩니다.\n",
    "또한 가방을 흔들어서 단어를 섞었기 때문에 더 이상 단어의 순서는 중요하지 않습니다.\n",
    "\n",
    "BoW를 만드는 과정을 이렇게 두 가지 과정으로 생각해보겠습니다.\n",
    "\n",
    "(1) 각 단어에 고유한 정수 인덱스를 부여합니다.  # 단어 집합 생성.\n",
    "(2) 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.  \n",
    "'''\n",
    "\n",
    "#문서1 : 정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\n",
    "\n",
    "from  konlpy.tag import Okt\n",
    "okt=Okt()\n",
    "\n",
    "def back_ward (docu):\n",
    "    dock=docu.replace('.',' ')\n",
    "    token=okt.morphs(dock)\n",
    "    word_index={}\n",
    "    bow=[]\n",
    "    \n",
    "    \n",
    "    for word in token:  \n",
    "        if word not in word_index.keys():\n",
    "            word_index[word]=len(word_index)        \n",
    "            bow.insert(len(word_index) - 1,1)\n",
    "        else:\n",
    "            index=word_index.get(word)\n",
    "            bow[index]=bow[index]+1\n",
    "            \n",
    "    return word_index, bow\n",
    "\n",
    "\n",
    "dock1=\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "vocab, bow=back_ward(dock1)\n",
    "\n",
    "print(vocab)\n",
    "print(bow)\n",
    "\n",
    "'''\n",
    "문서1에 각 단어에 대해서 인덱스를 부여한 결과는 첫번째 출력 결과입니다. 문서1의 BoW는 두번째 출력 결과입니다. \n",
    "두번째 출력 결과를 보면, 인덱스 4에 해당하는 물가상승률은 두 번 언급되었기 때문에 인덱스 4에 해당하는 값이 2입니다. \n",
    "인덱스는 0부터 시작됨에 주의합니다. \n",
    "다시 말해 물가상승률은 BoW에서 다섯번째 값입니다.\n",
    "만약, 한국어에서 불용어에 해당되는 조사들 또한 제거한다면 더 정제된 BoW를 만들 수도 있습니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "789d0e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'소비자': 0, '는': 1, '주로': 2, '소비': 3, '하는': 4, '상품': 5, '을': 6, '기준': 7, '으로': 8, '물가상승률': 9, '느낀다': 10}\n",
      "[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]\n",
      "{'정부': 0, '가': 1, '발표': 2, '하는': 3, '물가상승률': 4, '과': 5, '소비자': 6, '느끼는': 7, '은': 8, '다르다': 9, '는': 10, '주로': 11, '소비': 12, '상품': 13, '을': 14, '기준': 15, '으로': 16, '느낀다': 17}\n",
      "[1, 2, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#문서2 : 소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\n",
    "\n",
    "dock2='소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.'\n",
    "vocab, bow = back_ward(dock2)\n",
    "\n",
    "print(vocab)\n",
    "print(bow)  #'을' 이2번\n",
    "\n",
    "#=======================================================================\n",
    "\n",
    "dock3=dock1+dock2\n",
    "vocab,bow=back_ward(dock3) #1문서, 2문서를 더해서 뽑은 것\n",
    "print(vocab)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac94817",
   "metadata": {},
   "source": [
    "BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이므로 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 쓰입니다. 즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰입니다. 가령, '달리기', '체력', '근력'과 같은 단어가 자주 등장하면 해당 문서를 체육 관련 문서로 분류할 수 있을 것이며, '미분', '방정식', '부등식'과 같은 단어가 자주 등장한다면 수학 관련 문서로 분류할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89f4df5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n",
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "#CountVectorizer 클래스로 BoW 만들기\n",
    "#사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원합니다. \n",
    "#이를 이용하면 영어에 대해서는 손쉽게 BoW를 만들 수 있습니다.\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vec=CountVectorizer()\n",
    "\n",
    "print(vec.fit_transform(corpus).toarray())\n",
    "print(vec.vocabulary_) #순서는 코퍼스와 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6349c",
   "metadata": {},
   "source": [
    "예제 문장에서 you와 love는 두 번씩 언급되었으므로 각각 인덱스 2와 인덱스 4에서 2의 값을 가지며, 그 외의 값에서는 1의 값을 가지는 것을 볼 수 있습니다. 또한 알파벳 I는 BoW를 만드는 과정에서 사라졌는데, 이는 CountVectorizer가 기본적으로 길이가 2이상인 문자에 대해서만 토큰으로 인식하기 때문입니다. 정제(Cleaning) 챕터에서 언급했듯이, 영어에서는 길이가 짧은 문자를 제거하는 것 또한 전처리 작업으로 고려되기도 합니다.\n",
    "\n",
    "주의할 것은 CountVectorizer는 단지 띄어쓰기만을 기준으로 단어를 자르는 낮은 수준의 토큰화를 진행하고 BoW를 만든다는 점입니다. 이는 영어의 경우 띄어쓰기만으로 토큰화가 수행되기 때문에 문제가 없지만 한국어에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않음을 의미합니다.\n",
    "\n",
    "예를 들어, 앞서 BoW를 만드는데 사용했던 '정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.' 라는 문장을 CountVectorizer를 사용하여 BoW로 만들 경우, CountVectorizer는 '물가상승률'이라는 단어를 인식하지 못 합니다. CountVectorizer는 띄어쓰기를 기준으로 분리한 뒤에 '물가상승률과'와 '물가상승률은' 으로 조사를 포함해서 하나의 단어로 판단하기 때문에 서로 다른 두 단어로 인식합니다. 그리고 '물가상승률과'와 '물가상승률은'이 각자 다른 인덱스에서 1이라는 빈도의 값을 갖게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18deb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00557d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "#불용어를 제거한 BoW 만들기\n",
    "'''\n",
    " BoW를 사용한다는 것은 그 문서에서 각 단어가 얼마나 자주 등장했는지를 보겠다는 것입니다.\n",
    " 그리고 각 단어에 대한 빈도수를 수치화 하겠다는 것은 결국 텍스트 내에서 어떤 단어들이 중요한지를 보고싶다는 의미를 함축하고 있습니다.\n",
    " 그렇다면 BoW를 만들때 불용어를 제거하는 일은 자연어 처리의 정확도를 높이기 위해서 선택할 수 있는 전처리 기법입니다.\n",
    "\n",
    "영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, \n",
    "불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원하고 있습니다.\n",
    "'''\n",
    "#--사용자가 직접 정의한 불용어 사용--\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vec=CountVectorizer(stop_words=['the','a','an','is','not'])\n",
    "\n",
    "print(vec.fit_transform(text).toarray())\n",
    "\n",
    "print(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19a358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d1af20c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]]\n",
      "{'family': 0, 'important': 1, 'thing': 2}\n"
     ]
    }
   ],
   "source": [
    "# --CountVectorizer에서 제공하는 자체 불용어 사용--\n",
    "\n",
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vec=CountVectorizer(stop_words='english')\n",
    "print(vec.fit_transform(text).toarray())\n",
    "print(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2f6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78eca18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "#NLTK에서 지원하는 불용어 사용\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_word=stopwords.words('english')\n",
    "\n",
    "vec=CountVectorizer(stop_words=stop_word)\n",
    "\n",
    "print(vec.fit_transform(text).toarray())  #-->tf값\n",
    "print(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe70c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79e22af7",
   "metadata": {},
   "source": [
    "문서의 벡터화: 문서 단어 행렬(Document-Term Matrix, DTM)\n",
    "서로 다른 문서들의 BoW들을 결합한 표현 방법인 문서 단어 행렬(Document-Term Matrix, DTM) 표현 방법을 배워보겠습니다. 이하 DTM이라고 명명합니다. 행과 열을 반대로 선택하면 TDM이라고 부르기도 합니다. 이렇게 하면 서로 다른 문서들을 비교할 수 있게 됩니다.\n",
    "\n",
    "1. 문서 단어 행렬(Document-Term Matrix, DTM)의 표기법\n",
    "문서 단어 행렬(Document-Term Matrix, DTM)이란 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것을 말합니다. 쉽게 생각하면 각 문서에 대한 BoW를 하나의 행렬로 만든 것으로 생각할 수 있으며, BoW와 다른 표현 방법이 아니라 BoW 표현을 다수의 문서에 대해서 행렬로 표현하고 부르는 용어입니다. 예를 들어서 이렇게 4개의 문서가 있다고 합시다.\n",
    "\n",
    "문서1 : 먹고 싶은 사과\n",
    "문서2 : 먹고 싶은 바나나\n",
    "문서3 : 길고 노란 바나나 바나나\n",
    "문서4 : 저는 과일이 좋아요\n",
    "\n",
    "띄어쓰기 단위 토큰화를 수행한다고 가정하고, 문서 단어 행렬로 표현하면 다음과 같습니다.\n",
    "\n",
    " \t과일이\t길고\t노란\t먹고\t바나나\t사과\t싶은\t저는\t좋아요\n",
    "문서1\t 0\t  0\t   0\t 1\t    0\t  1 \t1\t 0\t    0\n",
    "문서2  0\t  0\t   0\t 1\t    1\t  0\t    1\t 0\t    0\n",
    "문서3\t 0\t  1\t   1\t 0\t    2\t  0\t    0\t 0 \t    0\n",
    "문서4\t 1\t  0    0\t 0\t    0\t  0\t    0\t 1\t    1\n",
    "\n",
    "각 문서에서 등장한 단어의 빈도를 행렬의 값으로 표기합니다. 문서 단어 행렬은 문서들을 서로 비교할 수 있도록 수치화할 수 있다는 점에서 의의를 갖습니다. 만약 필요에 따라서는 형태소 분석기로 단어 토큰화를 수행하고, 불용어에 해당되는 조사들 또한 제거하여 더 정제된 DTM을 만들 수도 있을 것입니다.\n",
    "\n",
    "2. 문서 단어 행렬(Document-Term Matrix)의 한계\n",
    "DTM은 매우 간단하고 구현하기도 쉽지만, 본질적으로 가지는 몇 가지 한계들이 있습니다.\n",
    "\n",
    "1) 희소 표현(Sparse representation)\n",
    "원-핫 벡터는 단어 집합의 크기가 벡터의 차원이 되고 대부분의 값이 0이 되는 벡터입니다. 원-핫 벡터는 공간적 낭비와 계산 리소스를 증가시킬 수 있다는 점에서 단점을 가집니다. DTM도 마찬가지입니다. **DTM에서의 각 행을 문서 벡터라고 해봅시다. 각 문서 벡터의 차원은 원-핫 벡터와 마찬가지로 전체 단어 집합의 크기를 가집니다.** 만약 가지고 있는 전체 코퍼스가 방대한 데이터라면 문서 벡터의 차원은 수만 이상의 차원을 가질 수도 있습니다. 또한 많은 문서 벡터가 대부분의 값이 0을 가질 수도 있습니다. 당장 위에서 예로 들었던 문서 단어 행렬의 모든 행이 0이 아닌 값보다 0의 값이 더 많은 것을 볼 수 있습니다.\n",
    "\n",
    "원-핫 벡터나 DTM과 같은 대부분의 값이 0인 표현을 희소 벡터(sparse vector) 또는 희소 행렬(sparse matrix)라고 부르는데, 희소 벡터는 많은 양의 저장 공간과 높은 계산 복잡도를 요구합니다. 이러한 이유로 전처리를 통해 단어 집합의 크기를 줄이는 일은 BoW 표현을 사용하는 모델에서 중요할 수 있습니다. 앞서 배운 텍스트 전처리 방법을 사용하여 구두점, 빈도수가 낮은 단어, 불용어를 제거하고, 어간이나 표제어 추출을 통해 단어를 정규화하여 단어 집합의 크기를 줄일 수 있습니다.\n",
    "\n",
    "2) 단순 빈도 수 기반 접근\n",
    "여러 문서에 등장하는 모든 단어에 대해서 빈도 표기를 하는 이런 방법은 때로는 한계를 가지기도 합니다. 예를 들어 영어에 대해서 DTM을 만들었을 때, 불용어인 the는 어떤 문서이든 자주 등장할 수 밖에 없습니다. 그런데 유사한 문서인지 비교하고 싶은 문서1, 문서2, 문서3에서 동일하게 the가 빈도수가 높다고 해서 이 문서들이 유사한 문서라고 판단해서는 안 됩니다.\n",
    "\n",
    "각 문서에는 중요한 단어와 불필요한 단어들이 혼재되어 있습니다. 앞서 불용어(stopwords)와 같은 단어들은 빈도수가 높더라도 자연어 처리에 있어 의미를 갖지 못하는 단어라고 언급한 바 있습니다. 그렇다면 DTM에 불용어와 중요한 단어에 대해서 가중치를 줄 수 있는 방법은 없을까요? 이러한 아이디어를 적용한 TF-IDF를 이어서 학습해봅시다. 사이킷런의 CountVectorizer를 사용하여 DTM을 만드는 실습 또한 TF-IDF를 설명하면서 진행하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ac146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f48677f",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "이번에는 DTM 내에 있는 각 단어에 대한 중요도를 계산할 수 있는 TF-IDF 가중치에 대해서 알아보겠습니다. TF-IDF를 사용하면, 기존의 DTM을 사용하는 것보다 보다 많은 정보를 고려하여 문서들을 비교할 수 있습니다. TF-IDF가 DTM보다 항상 좋은 성능을 보장하는 것은 아니지만, 많은 경우에서 DTM보다 더 좋은 성능을 얻을 수 있습니다.\n",
    "\n",
    "1. TF-IDF(단어 빈도-역 문서 빈도, Term Frequency-Inverse Document Frequency)\n",
    "TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다. 우선 DTM을 만든 후, TF-IDF 가중치를 부여합니다.\n",
    "\n",
    "**TF-IDF는 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 쓰일 수 있습니다.**\n",
    "\n",
    "TF-IDF는 TF와 IDF를 곱한 값을 의미하는데 이를 식으로 표현해보겠습니다. 문서를 d, 단어를 t, 문서의 총 개수를 n이라고 표현할 때 TF, DF, IDF는 각각 다음과 같이 정의할 수 있습니다.\n",
    "\n",
    "(1) tf(d,t) : 특정 문서 d에서의 특정 단어 t의 등장 횟수.\n",
    "생소한 글자때문에 어려워보일 수 있지만, 잘 생각해보면 TF는 이미 앞에서 구한 적이 있습니다. TF는 앞에서 배운 DTM의 예제에서 각 단어들이 가진 값들입니다. DTM이 각 문서에서의 각 단어의 등장 빈도를 나타내는 값이었기 때문입니다.\n",
    "\n",
    "(2) df(t) : 특정 단어 t가 등장한 문서의 수.\n",
    "**여기서 특정 단어가 각 문서, 또는 문서들에서 몇 번 등장했는지는 관심가지지 않으며 오직 특정 단어 t가 등장한 문서의 수에만 관심을 가집니다. 앞서 배운 DTM에서 바나나는 문서2와 문서3에서 등장했습니다.**\n",
    "이 경우, 바나나의 df는 2입니다. 문서3에서 바나나가 두 번 등장했지만, 그것은 중요한 게 아닙니다. 심지어 바나나란 단어가 문서2에서 100번 등장했고, 문서3에서 200번 등장했다고 하더라도 바나나의 df는 2가 됩니다.\n",
    "\n",
    "idf(t) : df(t)에 반비례하는 수.  -->idf(t)=log(n/1+df(t))\n",
    "IDF라는 이름을 보고 DF의 역수가 아닐까 생각했다면, IDF는 DF의 역수를 취하고 싶은 것이 맞습니다. 그런데 log와 분모에 1을 더해주는 식에 의아하실 수 있습니다. log를 사용하지 않았을 때, IDF를 DF의 역수((n/1+df(t))라는 식)로 사용한다면 총 문서의 수 n이 커질 수록, IDF의 값은 기하급수적으로 커지게 됩니다. 그렇기 때문에 log를 사용합니다.\n",
    "\n",
    "왜 log가 필요한지 n=1,000,000일 때의 예를 들어봅시다. log의 밑은 10을 사용한다고 가정하였을 때 결과는 아래와 같습니다.\n",
    "\n",
    "idf(t)=log(n/df(t)),  n=1,000,000\n",
    "\n",
    "단어t   df(t)       idf(d,f)\n",
    "word1\t1\t          6\n",
    "word2\t100           4\n",
    "word3\t1,000\t      3\n",
    "word4\t10,000\t      2\n",
    "word5\t100,000       1\n",
    "word6\t1,000,000\t  0\n",
    "\n",
    "\n",
    "그렇다면 log를 사용하지 않으면 idf의 값이 어떻게 커지는지 보겠습니다.\n",
    "\n",
    "idf(t)=(n/1+df(t)),  n=1,000,000\n",
    "\n",
    "단어t   df(t)         idf(t)\n",
    "word1\t1\t        1,000,000\n",
    "word2\t100\t        10,000\n",
    "word3\t1,000\t    1,000\n",
    "word4\t10,000\t    100\n",
    "word5\t100,000\t    10\n",
    "word6\t1,000,000\t1\n",
    "\n",
    "또 다른 직관적인 설명은 불용어 등과 같이 자주 쓰이는 단어들은 비교적 자주 쓰이지 않는 단어들보다 최소 수십 배 자주 등장합니다. 그런데 비교적 자주 쓰이지 않는 단어들조차 희귀 단어들과 비교하면 또 최소 수백 배는 더 자주 등장하는 편입니다. 이 때문에 log를 씌워주지 않으면, 희귀 단어들에 엄청난 가중치가 부여될 수 있습니다. 로그를 씌우면 이런 격차를 줄이는 효과가 있습니다. log 안의 식에서 분모에 1을 더해주는 이유는 첫번째 이유로는 특정 단어가 전체 문서에서 등장하지 않을 경우에 분모가 0이 되는 상황을 방지하기 위함입니다.\n",
    "\n",
    "TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단합니다. TF-IDF 값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것입니다. 즉, the나 a와 같이 불용어의 경우에는 모든 문서에 자주 등장하기 마련이기 때문에 자연스럽게 불용어의 TF-IDF의 값은 다른 단어의 TF-IDF에 비해서 낮아지게 됩니다.\n",
    "\n",
    "\t과일이\t길고\t노란\t먹고\t바나나\t사과\t싶은\t저는\t좋아요\n",
    "문서1\t0\t  0\t    0\t  1\t    0\t 1\t   1\t0\t   0\n",
    "문서2\t0\t  0\t    0\t  1\t    1\t 0\t   1\t0      0\n",
    "문서3\t0\t  1\t    1\t  0\t    2\t 0 \t   0\t0\t   0\n",
    "문서4\t1\t  0   \t0\t  0\t    0\t 0\t   0\t1\t   1\n",
    "\n",
    "앞서 DTM을 설명하기위해 들었던 위의 예제를 가지고 TF-IDF에 대해 이해해보겠습니다. 우선 TF는 앞서 사용한 DTM을 그대로 사용하면, 그것이 각 문서에서의 각 단어의 TF가 됩니다. 이제 구해야할 것은 TF와 곱해야할 값인 IDF입니다. 로그는 자연 로그를 사용하도록 하겠습니다.**자연 로그는 로그의 밑을 자연 상수 e(e=2.718281...)를 사용하는 로그를 말합니다. IDF 계산을 위해 사용하는 로그의 밑은 TF-IDF를 사용하는 사용자가 임의로 정할 수 있는데, 여기서 로그는 마치 기존의 값에 곱하여 값의 크기를 조절하는 상수의 역할을 합니다.** 각종 프로그래밍 언어에서 패키지로 지원하는 TF-IDF의 로그는 대부분 자연 로그를 사용합니다. 여기서도 자연 로그를 사용하겠습니다. 자연 로그는 보통 log라고 표현하지 않고, ln이라고 표현합니다.\n",
    "\n",
    "단어\t|  IDF(역 문서 빈도)\n",
    "과일이| ln(4/(1+1)) = 0.693147\n",
    "길고\t| ln(4/(1+1)) = 0.693147\n",
    "노란\t| ln(4/(1+1)) = 0.693147\n",
    "먹고\t| ln(4/(2+1)) = 0.287682\n",
    "바나나| ln(4/(2+1)) = 0.287682\n",
    "사과\t| ln(4/(1+1)) = 0.693147\n",
    "싶은\t| ln(4/(2+1)) = 0.287682\n",
    "저는\t| ln(4/(1+1)) = 0.693147\n",
    "좋아요| ln(4/(1+1)) = 0.693147\n",
    "\n",
    "문서의 총 수는 4이기 때문에 ln 안에서 분자는 늘 4으로 동일합니다. 분모의 경우에는 각 단어가 등장한 문서의 수(DF)를 의미하는데, 예를 들어서 '먹고'의 경우에는 총 2개의 문서(문서1, 문서2)에 등장했기 때문에 2라는 값을 가집니다. 각 단어에 대해서 IDF의 값을 비교해보면 문서 1개에만 등장한 단어와 문서2개에만 등장한 단어는 값의 차이를 보입니다. IDF는 여러 문서에서 등장한 단어의 가중치를 낮추는 역할을 하기 때문입니다.\n",
    "\n",
    "TF-IDF를 계산해보겠습니다. 각 단어의 TF는 DTM에서의 각 단어의 값과 같으므로, 앞서 사용한 DTM에서 단어 별로 위의 IDF값을 곱해주면 TF-IDF 값을 얻습니다\n",
    "\n",
    " \t  과일이\t  길고\t노란\t  먹고\t 바나나\t  사과\t싶은\t   저는\t    좋아요\n",
    "문서1    0\t    0   \t0\t 0.287682\t0\t    0.693147 0.287682\t0\t       0\n",
    "문서2\t   0   \t    0   \t0\t 0.287682 0.287682\t   0\t 0.287682\t0\t       0\n",
    "문서3\t   0\t0.693147 0.693147  0\t  0.575364\t   0\t   0\t    0\t       0\n",
    "문서4\t 0.693147   0    \t0\t   0\t    0\t       0\t   0\t 0.693147\t0.693147 \n",
    "\n",
    "사실 예제 문서가 굉장히 간단하기 때문에 계산은 매우 쉽습니다. 문서3에서의 바나나만 TF 값이 2이므로 IDF에 2를 곱해주고, 나머진 TF 값이 1이므로 그대로 IDF 값을 가져오면 됩니다. 문서2에서의 바나나의 TF-IDF 가중치와 문서3에서의 바나나의 TF-IDF 가중치가 다른 것을 볼 수 있습니다. 수식적으로 말하면, TF가 각각 1과 2로 달랐기 때문인데 TF-IDF에서의 관점에서 보자면 TF-IDF는 특정 문서에서 자주 등장하는 단어는 그 문서 내에서 중요한 단어로 판단하기 때문입니다. 문서2에서는 바나나를 한 번 언급했지만, 문서3에서는 바나나를 두 번 언급했기 때문에 문서3에서의 바나나를 더욱 중요한 단어라고 판단하는 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41d90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "2a7eb31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['좋아요', '사과', '길고', '싶은', '노란', '먹고', '과일이', '바나나', '저는']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] \n",
    "\n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "print(vocab)\n",
    "vocab.sort()\n",
    "\n",
    "n = len(docs)\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(n / (df + 1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t, d) * idf(t)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "result=[]\n",
    "for i in range(n):\n",
    "    result.append([])\n",
    "    d=docs[i]\n",
    "    for j in range(len(vocab)):\n",
    "        t=vocab[j]\n",
    "        result[-1].append(tf(t,d))\n",
    "        \n",
    "tf=ps.DataFrame(result,columns =vocab)\n",
    "tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "859ac52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "result=[]\n",
    "\n",
    "\n",
    "for j in range(len(vocab)):\n",
    "    t=vocab[j]\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf=ps.DataFrame(result, index=vocab, columns=['IDF'])\n",
    "idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7ee9ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['좋아요', '사과', '길고', '싶은', '노란', '먹고', '과일이', '바나나', '저는']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "docs = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] \n",
    "\n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "print(vocab)\n",
    "vocab.sort()\n",
    "\n",
    "n = len(docs)\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    return log(n / (df + 1))\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t, d) * idf(t)\n",
    "\n",
    "\n",
    "result = []\n",
    "for i in range(n):\n",
    "    result.append([])\n",
    "    d = docs[i]\n",
    "    for j in range(len(vocab)):    \n",
    "        t = vocab[j]\n",
    "        result[-1].append(tfidf(t,d))\n",
    "\n",
    "        \n",
    "        \n",
    "tfidf=pd.DataFrame(result, columns=vocab)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "4ee19cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n",
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "#사이킷런을 이용한 DTM과 TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do ',    \n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "\n",
    "print(vec.fit_transform(corpus).toarray())\n",
    "print(vec.vocabulary_)\n",
    "#DTM이 완성-----------------------------------------------------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf=TfidfVectorizer().fit(corpus)\n",
    "\n",
    "print(tfidf.transform(corpus).toarray())\n",
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59521a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6467478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acadd4db",
   "metadata": {},
   "source": [
    "코사인 유사도를 이용한 추천 시스템\n",
    "BoW에 기반한 단어 표현 방법인 DTM, TF-IDF, 또는 뒤에서 배우게 될 Word2Vec 등과 같이 단어를 수치화할 수 있는 방법을 이해했다면 이러한 표현 방법에 대해서 코사인 유사도를 이용하여 문서의 유사도를 구하는 게 가능합니다.\n",
    "\n",
    "1. 코사인 유사도(Cosine Similarity)\n",
    "코사인 유사도는 두 벡터 간의 코사인 각도를 이용하여 구할 수 있는 두 벡터의 유사도를 의미합니다. 두 벡터의 방향이 완전히 동일한 경우에는 1의 값을 가지며, 90°의 각을 이루면 0, 180°로 반대의 방향을 가지면 -1의 값을 갖게 됩니다. **즉, 결국 코사인 유사도는 -1 이상 1 이하의 값을 가지며 값이 1에 가까울수록 유사도가 높다고 판단할 수 있습니다. 이를 직관적으로 이해하면 두 벡터가 가리키는 방향이 얼마나 유사한가를 의미합니다.**\n",
    "\n",
    "서로 반대방향 = 유사도 -1  /  크로스 = 유사도 0  /  동일한 방향 = 1\n",
    "\n",
    "두 벡터 A, B에 대해서 코사인 유사도는 식으로 표현하면 다음과 같습니다.\n",
    "\n",
    " similerity = cos(seta)=(A dot B/||A|| * ||B||)  =  sigma(A*B)/root(sigma A^2 )*root(sigma B^2)\n",
    " \n",
    " \n",
    " 문서 단어 행렬이나 TF-IDF 행렬을 통해서 문서의 유사도를 구하는 경우에는 문서 단어 행렬이나 TF-IDF 행렬이 각각의 특징 벡터 A, B가 됩니다. 예시를 통해 문서 단어 행렬에 대해서 코사인 유사도를 구해봅시다.\n",
    "\n",
    "문서1 : 저는 사과 좋아요\n",
    "문서2 : 저는 바나나 좋아요\n",
    "문서3 : 저는 바나나 좋아요 저는 바나나 좋아요\n",
    "\n",
    "뛰어쓰기 기준 토큰화를 진행했다고 가정하고, 위의 세 문서에 대해서 문서 단어 행렬을 만들면 이와 같습니다.\n",
    "\n",
    " \t바나나\t사과\t저는\t좋아요\n",
    "문서1\t 0\t  1\t    1\t  1\n",
    "문서2\t 1\t  0\t    1 \t  1\n",
    "문서3\t 2\t  0\t    2\t  2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "87b703bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/lib/python3/dist-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "cd6c060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "(20000, 47487)\n",
      "(20000, 20000)\n",
      "[[1.         0.01575748 0.         ... 0.         0.         0.        ]\n",
      " [0.01575748 1.         0.04907345 ... 0.         0.         0.        ]\n",
      " [0.         0.04907345 1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.         0.08375766]\n",
      " [0.         0.         0.         ... 0.         1.         0.        ]\n",
      " [0.         0.         0.         ... 0.08375766 0.         1.        ]]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data=ps.read_csv('movies_metadata.csv',low_memory=False)\n",
    "\n",
    "data\n",
    "\n",
    "data=data.head(20000)\n",
    "print(data['overview'].isnull().sum())\n",
    "\n",
    "data['overview']=data['overview'].fillna('')\n",
    "\n",
    "tfidf=TfidfVectorizer(stop_words='english')\n",
    "matrix=tfidf.fit_transform(data['overview'])\n",
    "\n",
    "matrix\n",
    "print(matrix.shape)\n",
    "\n",
    "\n",
    "cos_similer=cosine_similarity(matrix,matrix)\n",
    "print(cos_similer.shape)\n",
    "print(cos_similer)\n",
    "\n",
    "#zip() 기본 문법\n",
    "#zip() 함수는 여러 개의 순회 가능한(iterable) 객체를 인자로 받고,\n",
    "#각 객체가 담고 있는 원소를 튜플의 형태로 차례로 접근할 수 있는 반복자(iterator)를 반환합니다. \n",
    "\n",
    "title_index=dict(zip(data['title'],data.index))\n",
    "\n",
    "idx=title_index['Father of the Bride Part II']\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "8c4b1340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12481                            The Dark Knight\n",
       "150                               Batman Forever\n",
       "1328                              Batman Returns\n",
       "15511                 Batman: Under the Red Hood\n",
       "585                                       Batman\n",
       "9230          Batman Beyond: Return of the Joker\n",
       "18035                           Batman: Year One\n",
       "19792    Batman: The Dark Knight Returns, Part 1\n",
       "3095                Batman: Mask of the Phantasm\n",
       "10122                              Batman Begins\n",
       "Name: title, dtype: object"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "enumerate\n",
    "순서가 있는 자료형(list, set, tuple, dictionary, string)을 입력으로 받았을 때, 인덱스와 값을 포함하여 리턴\n",
    "for문과 함께 자주 사용됨\n",
    "인덱스와 값을 동시에 접근하면서 루프를 돌리고 싶을 때 사용\n",
    "\n",
    "'''\n",
    "\n",
    "def recommend(title, cos_simler=cos_similer):\n",
    "    \n",
    "    idx=title_index[title]\n",
    "    sim_score=list(enumerate(cos_similer[idx]))\n",
    "    sim_score=sorted(sim_score,key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    sim_score=sim_score[1:11]    \n",
    "    movie_indice=[idx[0] for idx in sim_score]\n",
    "    return data['title'].iloc[movie_indice]\n",
    "\n",
    "recommend('The Dark Knight Rises')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513f239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf8516e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb2260bc",
   "metadata": {},
   "source": [
    "# 1. 유클리드 거리(Euclidean distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468efbf3",
   "metadata": {},
   "source": [
    "단어와 문서의 유사도를 구하는 다양한 방법\n",
    "문서의 유사도를 구하기 위한 방법으로는 코사인 유사도 외에도 여러가지 방법들이 있습니다. \n",
    "\n",
    "1. 유클리드 거리(Euclidean distance)\n",
    "유클리드 거리(euclidean distance)는 문서의 유사도를 구할 때 자카드 유사도나 코사인 유사도만큼, 유용한 방법은 아닙니다. 하지만 여러 가지 방법을 이해하고, 시도해보는 것 자체만으로 다른 개념들을 이해할 때 도움이 되므로 의미가 있습니다.\n",
    "\n",
    "다차원 공간에서 두개의 점 p와 q가 각각p=(p1,p2,p3......)과 q=(q1,q2,q3......)의 좌표를 가질 때 두 점 사이의 거리를 계산하는 유클리드 거리 공식은 다음과 같습니다.\n",
    "\n",
    "root((q1-p1)^2+(q2-p1)^2+.........)=root(sigma(q-p)^2)\n",
    "\n",
    "https://wikidocs.net/217508\n",
    "\n",
    "2차원 좌표 평면 상에서 두 점 q와 p사이의 직선 거리를 구하는 문제입니다. 위의 경우에는 직각 삼각형으로 표현이 가능하므로, 중학교 수학 과정인 피타고라스의 정리를 통해 q와 p 사이의 거리를 계산할 수 있습니다. 즉, 2차원 좌표 평면에서 두 점 사이의 유클리드 거리 공식은 피타고라스의 정리를 통해 두 점 사이의 거리를 구하는 것과 동일합니다.\n",
    "\n",
    "다시 원점으로 돌아가서 여러 문서에 대해서 유사도를 구하고자 유클리드 거리 공식을 사용한다는 것은, 앞서 본 2차원을 단어의 총 개수만큼의 차원으로 확장하는 것과 같습니다. 예를 들어 아래와 같은 DTM이 있다고 합시다.\n",
    "\n",
    " \t  바나나\t사과\t저는\t좋아요\n",
    "      \n",
    "문서1\t  2\t      3\t    0\t  1\n",
    "\n",
    "문서2\t  1\t      2 \t3\t  1\n",
    "\n",
    "문서3\t  2\t      1\t    2\t  2\n",
    "\n",
    "단어의 개수가 4개이므로, 이는 4차원 공간에 문서1, 문서2, 문서3을 배치하는 것과 같습니다. 이때 다음과 같은 문서Q에 대해서 문서1, 문서2, 문서3 중 가장 유사한 문서를 찾아내고자 합니다.\n",
    "\n",
    " \t 바나나\t사과\t저는\t좋아요\n",
    "문서Q\t 1\t      1\t   0\t   1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "d2fdd025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n",
      "3.1622776601683795\n",
      "2.449489742783178\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def disk(x,y):\n",
    "    return np.sqrt(np.sum((x-y)**2)) #sqrt = root\n",
    "\n",
    "doc1 = np.array((2,3,0,1))\n",
    "doc2 = np.array((1,2,3,1))\n",
    "doc3 = np.array((2,1,2,2))\n",
    "docQ = np.array((1,1,0,1))\n",
    "\n",
    "print(disk(doc1,docQ))\n",
    "print(disk(doc2,docQ))\n",
    "print(disk(doc3,docQ))\n",
    "\n",
    "#유클리드 거리의 값이 가장 작다는 것은 문서 간 거리가 가장 가깝다는 것을 의미합니다. \n",
    "#즉, 문서1이 문서Q와 가장 유사하다고 볼 수 있습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e58b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a13bab17",
   "metadata": {},
   "source": [
    "# 자카드 유사도(Jaccard similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ebb80",
   "metadata": {},
   "source": [
    "A와 B 두개의 집합이 있다고 합시다. 이때 교집합은 두 개의 집합에서 공통으로 가지고 있는 원소들의 집합을 말합니다. 즉, **합집합에서 교집합의 비율을 구한다면 두 집합 A와 B의 유사도를 구할 수 있다는 것이 자카드 유사도(jaccard similarity)의 아이디어입니다.** 자카드 유사도는 0과 1사이의 값을 가지며, 만약 두 집합이 동일하다면 1의 값을 가지고, 두 집합의 공통 원소가 없다면 0의 값을 갖습니다. 자카드 유사도를 구하는 함수를 J라고 하였을 때, 자카드 유사도 함수 J는 아래와 같습니다.\n",
    "\n",
    "J(A,B)=|A교B|/|A합B| = |A교B|/|A|+|B|-|A교B|\n",
    "\n",
    "두 개의 비교할 문서를 각각 doc1,doc2라고 했을 때 doc1과 doc2의 문서의 유사도를 구하기 위한 자카드 유사도는 이와 같습니다.\n",
    "\n",
    "J(doc1,doc2)=(doc1 교 doc2)/(doc1 합 doc2)\n",
    "\n",
    "두 문서 doc1,doc2 사이의 자카드 유사도 J(doc1,doc2)는 두 집합의 교집합 크기를 두 집합의 합집합 크기로 나눈 값으로 정의됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "09838e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'everyone', 'like', 'likey', 'watch', 'card', 'holder']\n",
      "['apple', 'banana', 'coupon', 'passport', 'love', 'you']\n",
      "===========================================================================\n",
      "{'apple', 'banana', 'everyone', 'likey', 'watch', 'holder', 'passport', 'you', 'like', 'coupon', 'card', 'love'}\n",
      "==========================================================================\n",
      "{'apple', 'banana'}\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"apple banana everyone like likey watch card holder\"\n",
    "doc2 = \"apple banana coupon passport love you\"\n",
    "\n",
    "\n",
    "\n",
    "token1=doc1.split()\n",
    "token2=doc2.split()\n",
    "\n",
    "print(token1)\n",
    "print(token2)\n",
    "print('===========================================================================')\n",
    "union=set(token1).union(set(token2))\n",
    "print(union)  #합집합\n",
    "\n",
    "print('==========================================================================')\n",
    "intersection=set(token1).intersection(set(token2))\n",
    "\n",
    "print(intersection)\n",
    "\n",
    "print(len(intersection)/len(union))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a09e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
