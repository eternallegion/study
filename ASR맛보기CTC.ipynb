{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be5df4c6-bad3-4130-ab86-fb071ecc2b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoProcessor,AutoModelForCTC,AutoModelForSeq2SeqLM  -> 정확히 매칭 되야함\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq # 이 부분을 변경!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2b4de9a-d4d3-4ed5-90af-c89f781a9c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2 모델 로드 완료: Wav2Vec2ForCTC\n",
      "Whisper 모델 로드 완료: WhisperForConditionalGeneration\n"
     ]
    }
   ],
   "source": [
    "# Wav2Vec2 예시 (CTC):\n",
    "model_name=\"facebook/wav2vec2-base-960h\"\n",
    "processor_wav=AutoProcessor.from_pretrained(model_name)\n",
    "model_CTC=AutoModelForCTC.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Whisper 예시 (Seq2Seq):\n",
    "model_wisper=\"openai/whisper-small\"\n",
    "processor_wisper= AutoProcessor.from_pretrained(model_wisper)\n",
    "#model_wis= AutoModelForSeq2SeqLM.from_pretrained(model_wisper)\n",
    "model_wis= AutoModelForSpeechSeq2Seq.from_pretrained(model_wisper) # <-- 이렇게 사용해야 합니다.\n",
    "\n",
    "\n",
    "print(f\"Wav2Vec2 모델 로드 완료: {model_CTC.__class__.__name__}\")\n",
    "print(f\"Whisper 모델 로드 완료: {model_wis.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d15d7efd-7d84-4e8c-8685-b907f346bee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wav2Vec2의 새로운 lm_head 출력 차원: 30\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "new_vocab=30\n",
    "model_CTC.lm_head=nn.Linear(model_CTC.config.hidden_size, new_vocab)\n",
    "print(f\"Wav2Vec2의 새로운 lm_head 출력 차원: {model_CTC.lm_head.out_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4aa5013-d9b7-4b18-96c7-177329b1b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "옵티마이저 설정 완료. 학습률: 1e-05\n"
     ]
    }
   ],
   "source": [
    "lr=1e-5\n",
    "optimy=optim.AdamW(model_CTC.parameters(),lr=lr)\n",
    "print(f\"옵티마이저 설정 완료. 학습률: {lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "16102c8b-9dbf-4090-9382-b5ee938e87d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCTC, Wav2Vec2CTCTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 0. 환경 설정 및 장치(Device) 설정 ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 사용할 모델 이름\n",
    "model_name = \"facebook/wav2vec2-base-960h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e1aaf062-e376-4285-96ee-ee66a7d9a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [\n",
    "    \" \", \"|\", # 공백과 파이프는 CTC에 자주 사용됨\n",
    "    # 한글 자음\n",
    "    \"ㄱ\", \"ㄴ\", \"ㄷ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅅ\", \"ㅇ\", \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\",\n",
    "    \"ㄲ\", \"ㄸ\", \"ㅃ\", \"ㅆ\", \"ㅉ\",\n",
    "    # 한글 모음\n",
    "    \"ㅏ\", \"ㅑ\", \"ㅓ\", \"ㅕ\", \"ㅗ\", \"ㅛ\", \"ㅜ\", \"ㅠ\", \"ㅡ\", \"ㅣ\", \"ㅐ\", \"ㅔ\", \"ㅚ\", \"ㅟ\", \"ㅢ\",\n",
    "    \"ㅘ\", \"ㅝ\", \"ㅙ\", \"ㅞ\",\n",
    "    # 숫자 및 기타 기호 (필요시 추가)\n",
    "    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n",
    "    \",\", \".\", \"?\", \"!\", \"-\",\n",
    "    # 더미 데이터셋에 사용된 문자도 명시적으로 추가\n",
    "    # \"가\", \"나\", \"다\", \"라\", \"마\", \"바\", \"사\", \"아\", \"자\", \"차\", \"카\", \"타\", \"파\", \"하\"\n",
    "    # 실제 ASR에서는 위 자모 목록이면 충분합니다.\n",
    "]\n",
    "\n",
    "# <unk> (알 수 없는 토큰), <pad> (패딩 토큰) 추가\n",
    "# Wav2Vec2CTCTokenizer는 기본적으로 pad_token, unk_token, word_delimiter_token을 가집니다.\n",
    "# add_tokens는 이미 어휘에 없는 토큰만 추가합니다.\n",
    "vocab_list = sorted(list(set(vocab_list))) # 중복 제거 및 정렬\n",
    "\n",
    "# Vocab 파일 저장 (Tokenization 과정에서 필요)\n",
    "vocab_dict = {token: i for i, token in enumerate(vocab_list)}\n",
    "\n",
    "class DummyAudioDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, audio_length_sec=2, sample_rate=16000, processor=None):\n",
    "        self.num_samples = num_samples\n",
    "        self.audio_length_sec = audio_length_sec\n",
    "        self.sample_rate = sample_rate\n",
    "        self.processor = processor\n",
    "\n",
    "        base_chars = \"가나다라마바사아자차카타파하\" # 더미 텍스트 생성에 사용될 문자\n",
    "        self.texts = []\n",
    "        for _ in range(num_samples):\n",
    "            text_len = torch.randint(5, 15, (1,)).item()\n",
    "            random_text = \"\"\n",
    "            for _ in range(text_len):\n",
    "                char_idx = torch.randint(0, len(base_chars), (1,)).item()\n",
    "                random_text += base_chars[char_idx]\n",
    "            self.texts.append(random_text)\n",
    "\n",
    "        self.audio_data = [torch.randn(audio_length_sec * sample_rate) for _ in range(num_samples)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_waveform = self.audio_data[idx]\n",
    "        text_label = self.texts[idx]\n",
    "\n",
    "        input_values = self.processor.feature_extractor(audio_waveform.unsqueeze(0), sampling_rate=self.sample_rate).input_values[0]\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            input_ids = self.processor.tokenizer(text_label).input_ids\n",
    "\n",
    "        return {\"input_values\": input_values, \"labels\": input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0ab3b3cd-1106-43ba-866e-8bac2153ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(features):\n",
    "    input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "    batch = processor.feature_extractor.pad(\n",
    "        input_features,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "    labels_batch = processor.tokenizer.pad(\n",
    "        label_features,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09518323-f955-4f1d-97ed-20fbac5b6c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2d60298b-71e5-43fc-baa4-ea50564f403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_dict, f, ensure_ascii=False)\n",
    "\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    \"vocab.json\", # 위에서 생성한 vocab.json 파일 경로\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    word_delimiter_token=\" \" # 공백을 단어 구분자로\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "8856e271-c2d7-421c-b906-540444e81901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processor 직접 구성 및 토큰 설정 완료. 최종 Vocab Size: 55\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from transformers import Wav2Vec2Processor # Import the specific processor class\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# 최종 어휘 크기 확인\n",
    "final_vocab_size = processor.tokenizer.vocab_size\n",
    "print(f\"Processor 직접 구성 및 토큰 설정 완료. 최종 Vocab Size: {final_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4abdb31d-3688-4e4d-83c0-d61f615b3fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([32]) in the checkpoint and torch.Size([55]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([32, 768]) in the checkpoint and torch.Size([55, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 'facebook/wav2vec2-base-960h' 로드 및 lm_head 수정 완료. lm_head.out_features: 55\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 사전 학습 모델 로드 및 lm_head 수정 (Wav2Vec2 모델용) ---\n",
    "model = AutoModelForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=final_vocab_size, # 모델의 config에 새로운 vocab_size를 알려줌\n",
    "    ignore_mismatched_sizes=True # <--- 이 부분이 핵심! 크기가 다른 레이어의 가중치 로드를 무시합니다.\n",
    ")\n",
    "\n",
    "# lm_head를 최종 vocab_size에 맞게 재정의\n",
    "model.lm_head = nn.Linear(model.config.hidden_size, final_vocab_size)\n",
    "\n",
    "# 모델을 GPU/CPU로 이동\n",
    "model.to(device)\n",
    "print(f\"모델 '{model_name}' 로드 및 lm_head 수정 완료. lm_head.out_features: {model.lm_head.out_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "bd7e6347-0292-4a8b-b295-1d32b5962ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터셋 크기: 100 샘플, 배치 개수: 25\n",
      "옵티마이저 설정 완료 (AdamW, LR=0.0001)\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 데이터셋 인스턴스 및 DataLoader 생성 ---\n",
    "train_set = DummyAudioDataset(num_samples=100, processor=processor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "print(f\"학습 데이터셋 크기: {len(train_set)} 샘플, 배치 개수: {len(train_loader)}\")\n",
    "\n",
    "# 학습 환경 설정 (옵티마이저)\n",
    "lr = 1e-4\n",
    "optimy = optim.AdamW(model.parameters(), lr=lr)\n",
    "print(f\"옵티마이저 설정 완료 (AdamW, LR={lr})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ca93e163-b89e-48d5-ad7c-6bbc922b2c69",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 학습 시작 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "type of None unknown: <class 'NoneType'>. Should be one of a python, numpy, pytorch or tensorflow object.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[294], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      8\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     10\u001b[0m         input_val \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m         label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[269], line 54\u001b[0m, in \u001b[0;36mDummyAudioDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     51\u001b[0m input_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mfeature_extractor(audio_waveform\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_rate)\u001b[38;5;241m.\u001b[39minput_values[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mas_target_processor():\n\u001b[1;32m---> 54\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_label\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids}\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2867\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2865\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2866\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2867\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2977\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   2955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2956\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2957\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2975\u001b[0m     )\n\u001b[0;32m   2976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2977\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2978\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2979\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   2980\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2981\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2982\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2983\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2984\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2985\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2986\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2987\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   2988\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2989\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2990\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2991\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2992\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2993\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2994\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2995\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2996\u001b[0m         split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   2997\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2998\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3052\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3023\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[0;32m   3025\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3040\u001b[0m \u001b[38;5;124;03m        method).\u001b[39;00m\n\u001b[0;32m   3041\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3043\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3044\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3045\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3050\u001b[0m )\n\u001b[1;32m-> 3052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_plus(\n\u001b[0;32m   3053\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   3054\u001b[0m     text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   3055\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3056\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3057\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3058\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3059\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3060\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3061\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3062\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   3063\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3064\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3065\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3066\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3067\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3068\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3069\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3070\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3071\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_special_tokens),\n\u001b[0;32m   3072\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3073\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\tokenization_utils.py:803\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    800\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[0;32m    801\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 803\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfirst_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpair_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecond_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3548\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.prepare_for_model\u001b[1;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[0;32m   3546\u001b[0m \u001b[38;5;66;03m# Padding\u001b[39;00m\n\u001b[0;32m   3547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_strategy \u001b[38;5;241m!=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mDO_NOT_PAD \u001b[38;5;129;01mor\u001b[39;00m return_attention_mask:\n\u001b[1;32m-> 3548\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoded_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3555\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_length:\n\u001b[0;32m   3558\u001b[0m     encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3335\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3333\u001b[0m     return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m return_tensors\n\u001b[0;32m   3334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3336\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_element\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unknown: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(first_element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3337\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of a python, numpy, pytorch or tensorflow object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3338\u001b[0m     )\n\u001b[0;32m   3340\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m encoded_inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   3341\u001b[0m     encoded_inputs[key] \u001b[38;5;241m=\u001b[39m to_py_obj(value)\n",
      "\u001b[1;31mValueError\u001b[0m: type of None unknown: <class 'NoneType'>. Should be one of a python, numpy, pytorch or tensorflow object."
     ]
    }
   ],
   "source": [
    "# --- 6. 학습 루프 실행 ---\n",
    "num_epochs = 3\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"\\n--- 학습 시작 ---\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        input_val = batch[\"input_values\"].to(device)\n",
    "        label = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimy.zero_grad()\n",
    "\n",
    "        out = model(input_values=input_val, labels=label)\n",
    "        loss = out.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimy.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n--- 학습 완료 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf8cf4-98da-4142-9f49-f5d54310e374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acddc319-4cf8-4087-a5d8-63ff638312e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hangul-jamo in c:\\users\\jh\\anaconda3\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install hangul-jamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef7f7b53-3fc1-4a75-9e11-29e76b8bdc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (선택 사항) 학습된 모델 저장 ---\n",
    "#model_save_path = \"./my_finetuned_wav2vec2\"\n",
    "#model.save_pretrained(model_save_path)\n",
    "#processor.save_pretrained(model_save_path)\n",
    "#print(f\"모델 저장 완료: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1e43b67-c963-44f2-ba26-4c0068dd8dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processor configured and tokens set. Final Vocab Size: 57\n",
      "Processor's pad_token_id: 56\n",
      "Is pad_token_id within vocab_size range? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([32]) in the checkpoint and torch.Size([57]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([32, 768]) in the checkpoint and torch.Size([57, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'facebook/wav2vec2-base-960h' loaded and lm_head modified. lm_head.out_features: 57\n",
      "Training dataset size: 100 samples, Number of batches: 25\n",
      "Optimizer configured (AdamW, LR=0.0001)\n",
      "\n",
      "--- Training Started ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 25/25 [00:01<00:00, 24.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Loss: 26.3305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 25/25 [00:00<00:00, 30.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Average Loss: 8.9754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 25/25 [00:00<00:00, 32.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Average Loss: 5.0271\n",
      "\n",
      "--- Training Completed ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor, AutoModelForCTC, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "# --- 0. Environment and Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model name to use\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "# --- 1. Custom Tokenizer Creation for Wav2Vec2 Model (MODIFIED VOCAB) ---\n",
    "# Ensure special tokens are at the end to guarantee their IDs\n",
    "# are within the valid range [0, vocab_size - 1]\n",
    "vocab_elements = [\n",
    "    # Korean Consonants (Jamo)\n",
    "    \"ㄱ\", \"ㄴ\", \"ㄷ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅅ\", \"ㅇ\", \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\",\n",
    "    \"ㄲ\", \"ㄸ\", \"ㅃ\", \"ㅆ\", \"ㅉ\",\n",
    "    # Korean Vowels (Jamo)\n",
    "    \"ㅏ\", \"ㅑ\", \"ㅓ\", \"ㅕ\", \"ㅗ\", \"ㅛ\", \"ㅜ\", \"ㅠ\", \"ㅡ\", \"ㅣ\", \"ㅐ\", \"ㅔ\", \"ㅚ\", \"ㅟ\", \"ㅢ\",\n",
    "    \"ㅘ\", \"ㅝ\", \"ㅙ\", \"ㅞ\",\n",
    "    # Numbers (optional, add if your data includes them)\n",
    "    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n",
    "    # Punctuation (optional)\n",
    "    \",\", \".\", \"?\", \"!\", \"-\",\n",
    "    # Important: Put space, word delimiter, and special tokens at the end\n",
    "    # to ensure their IDs are correctly handled relative to vocab_size\n",
    "    \" \", \"|\", \"<unk>\", \"<pad>\" # Ensure <pad> is present\n",
    "]\n",
    "\n",
    "# Create a unique, sorted list, then make sure special tokens are at the very end\n",
    "unique_vocab = sorted(list(set([v for v in vocab_elements if v not in [\" \", \"|\", \"<unk>\", \"<pad>\"]])))\n",
    "final_vocab_list = unique_vocab + [\" \", \"|\", \"<unk>\", \"<pad>\"] # Explicitly add special tokens at the end\n",
    "\n",
    "# Create vocab_dict with IDs based on their index in the final list\n",
    "vocab_dict = {token: i for i, token in enumerate(final_vocab_list)}\n",
    "\n",
    "# Save vocab.json (required by Wav2Vec2CTCTokenizer)\n",
    "with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_dict, f, ensure_ascii=False)\n",
    "\n",
    "# Load Wav2Vec2CTCTokenizer\n",
    "# Crucially, ensure the pad_token_id is correctly mapped from the vocab_dict\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    \"vocab.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    word_delimiter_token=\" \"\n",
    ")\n",
    "\n",
    "# Wav2Vec2FeatureExtractor creation\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1, # 1 for mono audio\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False # The model typically generates attention mask internally\n",
    ")\n",
    "\n",
    "# Combine Feature Extractor and Tokenizer into a Processor\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "\n",
    "# Verify final vocabulary size\n",
    "final_vocab_size = processor.tokenizer.vocab_size\n",
    "print(f\"Processor configured and tokens set. Final Vocab Size: {final_vocab_size}\")\n",
    "print(f\"Processor's pad_token_id: {processor.tokenizer.pad_token_id}\")\n",
    "print(f\"Is pad_token_id within vocab_size range? {processor.tokenizer.pad_token_id < final_vocab_size}\")\n",
    "\n",
    "\n",
    "# --- 2. Load Pre-trained Model and Modify lm_head (for Wav2Vec2 model) ---\n",
    "model = AutoModelForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id, # This is correct, ensuring model uses our pad_token_id\n",
    "    vocab_size=final_vocab_size, # Pass the final_vocab_size to the model config\n",
    "    ignore_mismatched_sizes=True # Ignore size mismatches (e.g., lm_head) when loading\n",
    ")\n",
    "\n",
    "# Redefine lm_head to match the final_vocab_size.\n",
    "# This ensures the output layer matches your custom vocabulary.\n",
    "model.lm_head = nn.Linear(model.config.hidden_size, final_vocab_size)\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model '{model_name}' loaded and lm_head modified. lm_head.out_features: {model.lm_head.out_features}\")\n",
    "\n",
    "# --- 3. Define DummyAudioDataset Class ---\n",
    "class DummyAudioDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, audio_length_sec=2, sample_rate=16000, processor=None):\n",
    "        self.num_samples = num_samples\n",
    "        self.audio_length_sec = audio_length_sec\n",
    "        self.sample_rate = sample_rate\n",
    "        self.processor = processor\n",
    "\n",
    "        jamo_chars = [char for char in final_vocab_list if char not in [\"<unk>\", \"<pad>\"]] # Exclude special tokens that aren't for generation\n",
    "\n",
    "        self.texts = []\n",
    "        for _ in range(num_samples):\n",
    "            text_len = torch.randint(5, 15, (1,)).item()\n",
    "            random_text_jamo = \"\"\n",
    "            for _ in range(text_len):\n",
    "                char_idx = torch.randint(0, len(jamo_chars), (1,)).item()\n",
    "                random_text_jamo += jamo_chars[char_idx]\n",
    "            \n",
    "            # Fallback for empty strings (should be rare with jamo_chars including space)\n",
    "            if not random_text_jamo.strip():\n",
    "                # Ensure fallback is also tokenizable by your defined vocab\n",
    "                fallback_jamo = \"안녕\" # Simpler fallback using just \"안녕\"\n",
    "                random_text_jamo = \"\".join(unicodedata.normalize(\"NFD\", char) for char in fallback_jamo if unicodedata.category(char) != 'Mn')\n",
    "            \n",
    "            self.texts.append(random_text_jamo)\n",
    "\n",
    "        self.audio_data = [torch.randn(audio_length_sec * sample_rate) for _ in range(num_samples)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_waveform = self.audio_data[idx]\n",
    "        text_label = self.texts[idx]\n",
    "        return {\"audio\": audio_waveform, \"text\": text_label}\n",
    "\n",
    "# --- 4. Data Collator Function ---\n",
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        audio_samples = [feature[\"audio\"] for feature in features]\n",
    "        text_labels = [feature[\"text\"] for feature in features]\n",
    "\n",
    "        padded_audio_batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_values\": audio_samples},\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        batch_input_values = padded_audio_batch[\"input_values\"]\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            tokenized_labels = []\n",
    "            for label in text_labels:\n",
    "                encoded_label = self.processor.tokenizer(label).input_ids\n",
    "                if encoded_label:\n",
    "                    tokenized_labels.append(encoded_label)\n",
    "                else:\n",
    "                    print(f\"Warning: Empty tokenized label for text: '{label}'. Using default.\")\n",
    "                    default_text = \"안녕\" # Fallback text\n",
    "                    decomposed_default_text = \"\".join(unicodedata.normalize(\"NFD\", char) for char in default_text if unicodedata.category(char) != 'Mn')\n",
    "                    tokenized_labels.append(self.processor.tokenizer(decomposed_default_text).input_ids)\n",
    "\n",
    "            labels_batch = self.processor.tokenizer.pad(\n",
    "                {\"input_ids\": tokenized_labels},\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch = {\n",
    "            \"input_values\": batch_input_values,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "# --- 5. Create Dataset Instance and DataLoader ---\n",
    "train_set = DummyAudioDataset(num_samples=100, processor=processor)\n",
    "\n",
    "data_collator_instance = DataCollatorCTCWithPadding(processor=processor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator_instance\n",
    ")\n",
    "print(f\"Training dataset size: {len(train_set)} samples, Number of batches: {len(train_loader)}\")\n",
    "\n",
    "# --- 6. Training Environment Setup (Optimizer) ---\n",
    "lr = 1e-4\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "print(f\"Optimizer configured (AdamW, LR={lr})\")\n",
    "\n",
    "# --- 7. Execute Training Loop ---\n",
    "num_epochs = 3\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"\\n--- Training Started ---\")\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "        input_val = batch[\"input_values\"].to(device)\n",
    "        label = batch[\"labels\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_values=input_val, labels=label)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n--- Training Completed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca783010-e888-4f14-8eef-fcd1647aa1ac",
   "metadata": {},
   "source": [
    "--\r\n",
    "\r\n",
    "### 1. `ValueError: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [4, 1, 1, 1, 32000]`\r\n",
    "\r\n",
    "**문제:** 이 오류는 Wav2Vec2 모델의 내부 `Conv1d` 레이어가 기대하는 입력 차원과 실제 전달된 `input_values`의 차원이 일치하지 않아 발생했습니다. Wav2Vec2 모델은 오디오 파형을 `(배치 크기, 채널 수, 시퀀스 길이)` 또는 `(배치 크기, 시퀀스 길이)` 형태로 받아서, 모델 내부에 있는 `feature_extractor`가 이를 처리합니다. 그런데 저희 코드에서는 `data_collator`에서 이미 `processor.feature_extractor`를 통해 오디오를 특징 맵으로 변환한 상태로 모델에 전달하려고 했습니다. 이렇게 되면 모델 내부의 `feature_extractor`가 이미 처리된 특징 맵을 다시 처리하려다 차원 불일치가 발생한 거죠.\r\n",
    "\r\n",
    "**해결:** `data_collator`에서 오디오 데이터는 **원시 파형 상태로 패딩만** 하도록 수정했습니다. `processor.feature_extractor.pad` 메서드를 사용하여 오디오 샘플들을 배치로 묶고 길이를 맞추는 역할만 하게 했습니다. 실제 특징 추출 및 변환은 모델의 `forward` 메서드 내부에서 자동으로 이루어지도록 변경했습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. `ValueError: type of None unknown: <class 'NoneType'>.`\r\n",
    "\r\n",
    "**문제:** 이 오류는 토크나이저의 `pad()` 메서드에서 발생했습니다. `data_collator`에서 텍스트 레이블을 토크나이징하고 패딩하는 과정에서, 특정 텍스트 레이블이 `None` 값을 반환하거나, 비어있는 토큰 리스트를 반환하여 토크나이저가 이를 처리하지 못했습니다. 특히 저희가 임의의 한글 자모를 생성했기 때문에, 유효하지 않거나 토크나이저가 인식하지 못하는 텍스트가 생성되었을 가능성이 있었습니다.\r\n",
    "\r\n",
    "**해결:**\r\n",
    "* **텍스트 생성 로직 강화:** `DummyAudioDataset`에서 텍스트를 생성할 때, `vocab_list`에 있는 실제 자모들만을 사용하여 텍스트를 구성하도록 했습니다. 또한, 혹시라도 빈 문자열이 생성될 경우를 대비해 \"안녕하세요\"와 같은 기본 문자열로 대체하는 안전 장치를 추가했습니다.\r\n",
    "* **토큰화 후 유효성 검사:** `data_collator`에서 각 텍스트 레이블을 토크나이징한 후, 생성된 `input_ids`가 비어있는지 확인하는 로직을 추가했습니다. 만약 비어있다면, 마찬가지로 \"안녕하세요\"의 토큰 ID를 사용하는 **폴백(Fallback)** 처리를 통해 `None` 값이 `tokenizer.pad()`로 전달되는 것을 방지했습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. `TypeError: SequenceFeatureExtractor.pad() got an unexpected keyword argument 'sampling_rate'`\r\n",
    "\r\n",
    "**문제:** 이 오류는 `data_collator`에서 오디오 패딩을 위해 `processor.feature_extractor.pad()`를 호출할 때 `sampling_rate` 인자를 전달해서 발생했습니다. `SequenceFeatureExtractor` (Wav2Vec2FeatureExtractor의 상위 클래스)의 `pad()` 메서드는 오디오의 샘플링 레이트를 인자로 받지 않습니다. 샘플링 레이트는 `feature_extractor`를 처음 생성할 때 이미 설정되는 값이기 때문입니다.\r\n",
    "\r\n",
    "**해결:** `processor.feature_extractor.pad()` 호출 시 **`sampling_rate` 인자를 단순히 제거**했습니다. 이는 `feature_extractor` 객체 내부에 이미 설정되어 있으므로 명시적으로 다시 전달할 필요가 없습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. `RuntimeError: blank must be in label range`\r\n",
    "\r\n",
    "**문제:** 이 오류는 CTC 손실 함수를 계산할 때, `blank` 토큰의 ID가 모델의 전체 어휘(vocab) 범위(`[0, vocab_size - 1]`)를 벗어났을 때 발생합니다. `Wav2Vec2ForCTC` 모델은 `pad_token_id`를 `blank` 토큰으로 사용하는데, 이 ID가 유효한 어휘 인덱스여야 합니다. 저희의 `vocab_dict` 생성 방식 때문에 `<pad>` 토큰이 `vocab_size`와 같거나 더 큰 ID를 할당받는 경우가 있었던 거죠.\r\n",
    "\r\n",
    "**해결:**\r\n",
    "* **어휘 목록 재정렬:** `vocab_list`를 생성할 때, 일반 문자열(`ㄱ`, `ㅏ`, `0` 등)을 먼저 배치하고, **특수 토큰(` `, `|`, `<unk>`, `<pad>`)을 가장 마지막에 명시적으로 추가**하도록 변경했습니다. 이렇게 하면 `enumerate`를 통해 `vocab_dict`를 생성할 때 `\"<pad>\"` 토큰이 항상 `len(final_vocab_list) - 1`의 ID를 갖게 됩니다.\r\n",
    "* **`vocab_size`와 `pad_token_id` 일치 확인:** `AutoModelForCTC.from_pretrained`를 호출할 때 `vocab_size=final_vocab_size`와 `pad_token_id=processor.tokenizer.pad_token_id`를 정확하게 전달하여, 모델이 올바른 어휘 크기와 패딩 토큰 ID를 사용하도록 명확히 했습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "이러한 단계들을 거쳐, 데이터셋 생성부터 데이터 로딩, 모델의 입력 형식 요구 사항까지 모든 파이프라인이 Wav2Vec2 모델의 훈련에 적합하게 조정되었습니다. 이제야 모델이 데이터를 올바르게 받아들이고 CTC 손실을 계산하며 훈련을 시작할 수 있게 된 것입니다!\r\n",
    "\r\n",
    "추가적인 질문이나 궁금한 점이 있으시면 언제든지 편하게 물어봐 주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82f5e2d-897d-48dc-80c1-f859d6053222",
   "metadata": {},
   "source": [
    "!\r\n",
    "\r\n",
    "이 오류는 CTC 손실 함수가 블랭크 토큰 ID가 모델의 어휘 범위 내에 있어야 한다고 요구하는 데서 발생했습니다. 저희가 `<pad>` 토큰이 `vocab_size`보다 같거나 큰 ID를 할당받는 경우를 수정함으로써, CTC 손실이 제대로 계산될 수 있게 된 거죠.\r\n",
    "\r\n",
    "이제 학습 루프가 정상적으로 시작되어 모델이 데이터를 처리하고 손실을 계산할 수 있게 된 것입니다.\r\n",
    "\r\n",
    "그동안 여러 차례의 오류를 끈기 있게 따라와 주셔서 정말 감사합니다. 이제 학습이 원활하게 진행될 것으로 보입니다\\!\r\n",
    "\r\n",
    "추가적으로 궁금한 점이나 다음 단계가 필요하시면 언제든지 알려주세요.\r\n",
    "잘하셨어요\\! 드디어 `RuntimeError: blank must be in label range` 오류까지 해결하셨네요\\! 이전의 복잡한 차원 문제와 `TypeError`를 넘어섰다는 건 정말 큰 진전입니다.\r\n",
    "\r\n",
    "이 오류는 CTC (Connectionist Temporal Classification) 손실 계산 시 `blank` 토큰의 ID가 모델의 유효한 레이블 범위 내에 없어서 발생하는 문제였습니다.\r\n",
    "\r\n",
    "제공해주신 마지막 코드를 보면, `vocab_list`를 다시 정리하여 `\"<pad>\"` 토큰이 `vocab_dict`에서 가장 마지막 인덱스(가장 큰 숫자)를 갖도록 명확하게 설정했습니다. 이렇게 함으로써 `processor.tokenizer.pad_token_id`가 항상 `vocab_size - 1`이 되어, CTC 손실 함수가 예상하는 유효한 범위 내에 들어가게 됩니다.\r\n",
    "\r\n",
    "이제 모델이 훈련을 시작하고 손실을 계산할 수 있을 거예요\\! 🎉\r\n",
    "\r\n",
    "-----\r\n",
    "\r\n",
    "**다음 단계는 무엇일까요?**\r\n",
    "\r\n",
    "이제 훈련 루프가 실행되기 시작했으니, 다음과 같은 부분들을 고려해볼 수 있습니다:\r\n",
    "\r\n",
    "  * **실제 데이터셋 통합**: 현재는 더미 데이터셋을 사용하고 있는데, 실제 한국어 음성 데이터셋(예: AI Hub의 음성 데이터, KSSPEECH 등)을 통합하는 작업을 시작할 수 있습니다. 데이터셋마다 오디오 파일 경로와 텍스트 전사본을 처리하는 방식이 다를 수 있으니, `DummyAudioDataset` 클래스를 실제 데이터셋에 맞게 수정해야 합니다.\r\n",
    "  * **훈련 지표 모니터링**: 에포크별 평균 손실을 출력하고 있는데, 워드 오류율 (WER)이나 문자 오류율 (CER)과 같은 실제 음성 인식 지표를 계산하여 모델 성능을 정량적으로 평가하는 것이 중요합니다.\r\n",
    "  * **하이퍼파라미터 튜닝**: 학습률, 배치 크기, 에포크 수 등을 조절하여 모델의 수렴 속도와 최종 성능을 최적화할 수 있습니다.\r\n",
    "  * **모델 저장 및 로드**: 훈련된 모델을 저장하고 나중에 다시 로드하여 추론에 사용하거나 추가 훈련을 진행할 수 있도록 기능을 추가하세요.\r\n",
    "  * **추론 구현**: 훈련된 모델을 사용하여 새로운 음성 파일에 대한 텍스트를 예측하는 추론 코드를 작성해 볼 수 있습니다.\r\n",
    "\r\n",
    "혹시 실제 데이터셋을 통합하거나 다른 질문이 있으시면 언제든지 말씀해주세요\\!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a6cdc-8527-4018-8529-ff05a3c86064",
   "metadata": {},
   "source": [
    "--\r\n",
    "\r\n",
    "### 1. 첫 번째 오류 해결: `ValueError: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [4, 1, 1, 1, 32000]`\r\n",
    "\r\n",
    "**기존 코드 (`data_collator` 내부):**\r\n",
    "```python\r\n",
    "# 기존 코드: processor.feature_extractor를 사용하여 특징 추출까지 시도\r\n",
    "input_features = processor.feature_extractor(\r\n",
    "    audio_samples,\r\n",
    "    sampling_rate=processor.feature_extractor.sampling_rate,\r\n",
    "    return_tensors=\"pt\",\r\n",
    "    padding=True\r\n",
    ")\r\n",
    "batch_input_values = input_features.input_values # 이 시점에서 이미 오디오가 특징 맵으로 변환된 상태\r\n",
    "```\r\n",
    "이 방식은 `processor.feature_extractor`가 원시 오디오를 받아서 Wav2Vec2 모델의 특징 추출 레이어를 거친 출력(특징 맵)을 반환하게 합니다. 그런데 `Wav2Vec2Model` 자체도 내부에 `feature_extractor`를 가지고 있어서, 이미 특징 맵인 `batch_input_values`를 다시 `feature_extractor`에 넣으려다 차원 오류가 발생했습니다.\r\n",
    "\r\n",
    "**수정된 코드 (`DataCollatorCTCWithPadding` 클래스의 `__call__` 메서드 내부):**\r\n",
    "```python\r\n",
    "# 수정된 코드: processor.feature_extractor.pad()를 사용하여 오직 패딩만 수행\r\n",
    "padded_audio_batch = self.processor.feature_extractor.pad(\r\n",
    "    {\"input_values\": audio_samples}, # `pad` 메서드는 딕셔너리를 인자로 받음\r\n",
    "    # sampling_rate 인자 제거 (다음 오류 해결)\r\n",
    "    return_tensors=\"pt\",\r\n",
    "    padding=True\r\n",
    ")\r\n",
    "batch_input_values = padded_audio_batch[\"input_values\"] # 이 시점의 input_values는 패딩된 '원시 오디오' 파형임\r\n",
    "```\r\n",
    "**수정 이유:** `Wav2Vec2Model`은 `input_values` 인자로 **원시 오디오 파형**을 직접 받기를 기대합니다. 모델 내부에서 자체적으로 오디오 특징 추출 단계를 수행하기 때문입니다. 따라서 `data_collator`에서는 오디오 샘플들의 길이를 맞춰주는 **패딩 기능만** 필요했습니다. `processor.feature_extractor.pad()`는 원시 오디오를 `(배치 크기, 시퀀스 길이)` 형태로 패딩하여 모델이 원하는 입력 형식을 맞춰줍니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. 두 번째 오류 해결: `ValueError: type of None unknown: <class 'NoneType'>.`\r\n",
    "\r\n",
    "**기존 코드 (`data_collator` 내부 - 텍스트 토큰화 부분):**\r\n",
    "```python\r\n",
    "# 기존 코드: 토큰화 결과에 대한 유효성 검사 없음\r\n",
    "with processor.as_target_processor():\r\n",
    "    tokenized_labels = [processor.tokenizer(label).input_ids for label in text_labels]\r\n",
    "```\r\n",
    "`DummyAudioDataset`에서 임의의 한글 텍스트를 생성했기 때문에, 때때로 토크나이저가 인식하지 못하는 자모 조합이나 빈 문자열이 `label`로 넘어갈 수 있었습니다. 이 경우 `processor.tokenizer(label).input_ids`가 `None` 또는 빈 리스트를 반환할 수 있었고, 이것이 다음 `tokenizer.pad()` 메서드로 넘어가면서 `ValueError`를 일으켰습니다.\r\n",
    "\r\n",
    "**수정된 코드 (`DataCollatorCTCWithPadding` 클래스의 `__call__` 메서드 내부):**\r\n",
    "```python\r\n",
    "# 수정된 코드: 텍스트 생성 로직 및 토큰화 후 유효성 검사 추가\r\n",
    "# (DummyAudioDataset 내부에도 텍스트 생성 로직이 개선됨)\r\n",
    "with self.processor.as_target_processor():\r\n",
    "    tokenized_labels = []\r\n",
    "    for label in text_labels:\r\n",
    "        encoded_label = self.processor.tokenizer(label).input_ids\r\n",
    "        if encoded_label: # 토큰화 결과가 비어있지 않은지 확인\r\n",
    "            tokenized_labels.append(encoded_label)\r\n",
    "        else:\r\n",
    "            print(f\"Warning: Empty tokenized label for text: '{label}'. Using default.\")\r\n",
    "            # 폴백 텍스트를 사용하여 토큰화하여 추가 (안전 장치)\r\n",
    "            default_text = \"안녕하세요\"\r\n",
    "            decomposed_default_text = \"\".join(unicodedata.normalize(\"NFD\", char) for char in default_text if unicodedata.category(char) != 'Mn')\r\n",
    "            tokenized_labels.append(self.processor.tokenizer(decomposed_default_text).input_ids)\r\n",
    "```\r\n",
    "**수정 이유:** 임의로 생성되는 텍스트가 항상 토큰화될 것이라는 보장이 없었기 때문에, 토큰화 과정에서 `None` 또는 빈 리스트가 생성되는 것을 방지해야 했습니다. `if encoded_label:` 조건문으로 토큰화 결과를 검사하고, 만약 비어 있다면 미리 정의된 유효한 텍스트(\"안녕하세요\"의 분해된 자모)로 대체하여 `tokenizer.pad()`가 항상 유효한 입력을 받도록 했습니다.\r\n",
    "\r\n",
    "또한, `DummyAudioDataset`의 텍스트 생성 로직도 개선하여, `vocab_list`에 포함된 자모들만으로 텍스트를 생성하도록 하여 애초에 토큰화되지 않을 텍스트가 생성될 가능성을 줄였습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. 세 번째 오류 해결: `TypeError: SequenceFeatureExtractor.pad() got an unexpected keyword argument 'sampling_rate'`\r\n",
    "\r\n",
    "**기존 코드 (`DataCollatorCTCWithPadding` 클래스의 `__call__` 메서드 내부 - 오디오 패딩 부분):**\r\n",
    "```python\r\n",
    "# 기존 코드: sampling_rate 인자를 pad() 메서드에 전달\r\n",
    "padded_audio_batch = self.processor.feature_extractor.pad(\r\n",
    "    {\"input_values\": audio_samples},\r\n",
    "    sampling_rate=self.processor.feature_extractor.sampling_rate, # 문제가 된 부분\r\n",
    "    return_tensors=\"pt\",\r\n",
    "    padding=True\r\n",
    ")\r\n",
    "```\r\n",
    "`sampling_rate`는 `Wav2Vec2FeatureExtractor` 객체가 처음 생성될 때 설정되는 속성입니다. `pad()` 메서드 자체는 오디오를 패딩하는 기능만 담당하며, 샘플링 레이트와 직접적인 관련이 없습니다.\r\n",
    "\r\n",
    "**수정된 코드 (`DataCollatorCTCWithPadding` 클래스의 `__call__` 메서드 내부):**\r\n",
    "```python\r\n",
    "# 수정된 코드: sampling_rate 인자 제거\r\n",
    "padded_audio_batch = self.processor.feature_extractor.pad(\r\n",
    "    {\"input_values\": audio_samples},\r\n",
    "    return_tensors=\"pt\",\r\n",
    "    padding=True\r\n",
    ")\r\n",
    "```\r\n",
    "**수정 이유:** `feature_extractor`는 이미 `sampling_rate` 값을 알고 있으므로, `pad()` 메서드를 호출할 때 이 인자를 다시 전달할 필요가 없습니다. 이는 불필요한 인자이며, `pad()` 메서드의 정의에 맞지 않아 `TypeError`가 발생했습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. 네 번째 오류 해결: `RuntimeError: blank must be in label range`\r\n",
    "\r\n",
    "**기존 코드 (어휘(vocab) 생성 부분):**\r\n",
    "```python\r\n",
    "# 기존 코드: 특수 토큰이 vocab_list의 어느 위치에 있을지 불확실\r\n",
    "vocab_list = [ ... 일반 자모 ... ] # 순서가 정해지지 않음\r\n",
    "vocab_list = sorted(list(set(vocab_list))) # 이로 인해 <pad>의 ID가 들쭉날쭉할 수 있음\r\n",
    "\r\n",
    "vocab_dict = {token: i for i, token in enumerate(vocab_list)}\r\n",
    "```\r\n",
    "CTC 손실 함수는 `blank` 토큰의 ID가 `0`부터 `vocab_size - 1` 사이의 유효한 범위 안에 있기를 요구합니다. `Wav2Vec2ForCTC`는 `config.pad_token_id`를 `blank` 토큰으로 사용합니다. 만약 `\"<pad>\"` 토큰이 `vocab_list`의 마지막에 위치하지 않거나, `sorted()`에 의해 다른 순서로 정렬되어 `vocab_size`와 같거나 더 큰 ID를 할당받으면 이 오류가 발생합니다.\r\n",
    "\r\n",
    "**수정된 코드 (어휘(vocab) 생성 부분):**\r\n",
    "```python\r\n",
    "# 수정된 코드: 특수 토큰을 vocab_list의 가장 마지막에 명시적으로 배치\r\n",
    "vocab_elements = [ ... 일반 자모와 특수 토큰 모두 포함 ... ]\r\n",
    "\r\n",
    "# 일반 토큰들을 먼저 정렬\r\n",
    "unique_vocab = sorted(list(set([v for v in vocab_elements if v not in [\" \", \"|\", \"<unk>\", \"<pad>\"]])))\r\n",
    "# 특수 토큰들을 원하는 순서(특히 <pad>를 마지막)로 명시적으로 추가\r\n",
    "final_vocab_list = unique_vocab + [\" \", \"|\", \"<unk>\", \"<pad>\"]\r\n",
    "\r\n",
    "# 이 final_vocab_list를 기반으로 vocab_dict 생성\r\n",
    "vocab_dict = {token: i for i, token in enumerate(final_vocab_list)}\r\n",
    "```\r\n",
    "**수정 이유:** `final_vocab_list`를 생성할 때 `\"<pad>\"` 토큰을 리스트의 가장 마지막에 배치함으로써, `enumerate`를 통해 할당되는 ID가 `len(final_vocab_list) - 1`이 되도록 보장했습니다. `vocab_size`는 `len(final_vocab_list)`이므로, `pad_token_id`는 항상 `vocab_size - 1`이 되어 유효한 범위 내에 있게 됩니다. 이는 CTC 손실 함수의 요구 사항을 만족시킵니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "이처럼 각 오류는 이전 단계의 문제 해결로 인해 새로운 단계에서 발생하는 문제였고, 각 문제의 원인을 정확히 파악하여 해당하는 코드 부분을 정교하게 수정함으로써 최종적으로 훈련 루프가 정상적으로 작동하게 된 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92247927-6f38-4bcf-abec-77e1045d867f",
   "metadata": {},
   "source": [
    "네, 오늘 배우고 해결한 내용들을 요약해 드릴게요. 핵심은 **Wav2Vec2 모델 훈련을 위한 데이터 준비 과정에서 발생했던 여러 차원 및 토큰화 관련 오류들을 해결하고, 최종적으로 훈련 루프가 정상적으로 시작되도록 만든 것**입니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## 오늘 학습 및 해결 요약\r\n",
    "\r\n",
    "### 1. `Wav2Vec2` 모델의 입력 데이터 이해 및 정렬\r\n",
    "* **오디오 데이터 처리 방식:** `Wav2Vec2` 모델은 특징 추출 레이어를 내부에 가지고 있기 때문에, `data_collator`에서 모델에 `input_values`를 전달할 때 이미 특징 추출된 데이터가 아닌 **패딩된 원시 오디오 파형**을 전달해야 한다는 것을 배웠습니다.\r\n",
    "* `processor.feature_extractor.pad()` 메서드가 이 역할을 수행하며, `sampling_rate` 같은 인자는 `pad()` 메서드에 필요 없다는 것을 확인하고 수정했습니다.\r\n",
    "\r\n",
    "### 2. 텍스트 토큰화의 견고성 확보\r\n",
    "* **빈 토큰화 결과 처리:** 임의로 생성된 텍스트가 `tokenizer`에 의해 유효하게 토큰화되지 않아 `None` 값이 발생하는 `ValueError`를 경험했습니다.\r\n",
    "* `data_collator`에서 텍스트를 토큰화한 후, 결과(`input_ids`)가 비어있는지 확인하고, 비어있을 경우 \"안녕하세요\"와 같은 **유효한 폴백(fallback) 텍스트**로 대체하여 오류를 방지하는 안전 장치를 구현했습니다.\r\n",
    "\r\n",
    "### 3. CTC 손실 함수를 위한 `blank` 토큰 설정\r\n",
    "* `CTC Loss` 계산 시 `blank` 토큰 ID가 모델의 어휘(vocab) 범위 내에 있어야 한다는 중요한 규칙을 배웠습니다.\r\n",
    "* `\"<pad>\"` 토큰이 이 `blank` 토큰으로 사용되는데, `vocab_dict`를 생성할 때 `\"<pad>\"` 토큰이 **항상 `vocab_list`의 마지막 인덱스를 갖도록 명시적으로 순서를 조정**하여 `pad_token_id`가 `vocab_size - 1`이 되도록 해결했습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "오늘 배운 핵심은 `Wav2Vec2`와 같은 복잡한 딥러닝 모델을 훈련할 때, 단순히 코드를 실행하는 것을 넘어 **데이터의 형태(차원), 전처리 과정, 그리고 모델의 내부 동작 방식(예: `feature_extractor`의 역할, `CTC loss`의 `blank` 토큰 요구사항)**을 정확히 이해하는 것이 얼마나 중요한지 깨달은 것입니다. 사소해 보이는 디테일들이 훈련을 막는 큰 오류로 이어질 수 있음을 직접 경험하고 해결하셨다는 점에서 큰 성과가 있습니다.\r\n",
    "\r\n",
    "이제 모델이 정상적으로 훈련을 시작할 수 있게 되었으니, 다음 단계에서는 실제 음성 데이터를 사용하고 모델의 성능을 더 깊이 분석하는 데 집중할 수 있을 거예요!\r\n",
    "\r\n",
    "오늘 배운 내용 중에 혹시 더 자세히 알고 싶은 부분이 있으신가요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2b51c-ce47-4b35-b9d0-47b046aedc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd09cf1-48a7-4ccb-b0b3-6c6cb42eece6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e359d-1da5-4327-a2df-fd23b4056f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547c65ff-1a76-462c-9f18-71eabbf0f50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd8ea995-3287-411b-aad6-f1524aaa8dc8",
   "metadata": {},
   "source": [
    "이 `ValueError`는 매우 흔하게 발생하며, Hugging Face `transformers` 라이브러리를 사용할 때 **모델 아키텍처와 `AutoModel` 클래스를 잘못 매칭했을 때 나타나는 오류**입니다.\r\n",
    "\r\n",
    "오류 메시지를 자세히 살펴보면 핵심은 다음과 같습니다:\r\n",
    "\r\n",
    "`ValueError: Unrecognized configuration class <class 'transformers.models.whisper.configuration_whisper.WhisperConfig'> for this kind of AutoModel: AutoModelForSeq2SeqLM.`\r\n",
    "\r\n",
    "**해석:**\r\n",
    "* `WhisperConfig` (Whisper 모델의 설정 클래스)는\r\n",
    "* `AutoModelForSeq2SeqLM` (시퀀스-투-시퀀스 언어 모델을 위한 자동 모델 클래스)에 대해\r\n",
    "* **\"인식할 수 없는 설정 클래스\"** 라는 뜻입니다.\r\n",
    "\r\n",
    "이는 즉, Hugging Face 라이브러리가 **Whisper 모델을 로드하기 위해 `AutoModelForSeq2SeqLM`을 사용하면 안 된다**는 것을 명확히 알려주는 것입니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 왜 이런 오류가 발생하는가?\r\n",
    "\r\n",
    "Hugging Face `transformers` 라이브러리에는 다양한 종류의 `AutoModel` 클래스들이 있습니다:\r\n",
    "* `AutoModel`: 일반적인 모델 로드 (주로 인코더 전용 모델)\r\n",
    "* `AutoModelForSequenceClassification`: 시퀀스 분류 모델\r\n",
    "* `AutoModelForTokenClassification`: 토큰 분류 모델\r\n",
    "* `AutoModelForMaskedLM`: 마스크드 언어 모델 (MLM)\r\n",
    "* `AutoModelForCausalLM`: 인과적 언어 모델 (CLM)\r\n",
    "* `AutoModelForCTC`: 음성 분야에서 CTC 손실을 사용하는 모델 (e.g., Wav2Vec2)\r\n",
    "* **`AutoModelForSeq2SeqLM`**: 일반적인 텍스트 기반 인코더-디코더 (Seq2Seq) 언어 모델 (e.g., BART, T5, MarianMT 등)\r\n",
    "* **`AutoModelForSpeechSeq2Seq`**: **음성 시퀀스-투-시퀀스 모델 (Whisper와 같은 ASR 모델용)**\r\n",
    "\r\n",
    "`Whisper` 모델은 분명히 인코더-디코더 구조를 가진 Seq2Seq 모델이 맞지만, 그것이 `AutoModelForSeq2SeqLM`에 바로 매핑되는 것은 아닙니다. `AutoModelForSeq2SeqLM`은 주로 **텍스트 입력/텍스트 출력**을 위한 범용적인 Seq2Seq 언어 모델들을 대상으로 합니다.\r\n",
    "\r\n",
    "`Whisper`는 **음성 입력/텍스트 출력**에 특화된 시퀀스-투-시퀀스 모델이며, 이를 위해 Hugging Face는 **`AutoModelForSpeechSeq2Seq`** 라는 전용 클래스를 제공합니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 해결 방법: `AutoModelForSpeechSeq2Seq` 사용\r\n",
    "\r\n",
    "`Whisper` 모델을 로드할 때는 `AutoModelForSeq2SeqLM` 대신 **`AutoModelForSpeechSeq2Seq`** 를 사용해야 합니다.\r\n",
    "\r\n",
    "**수정 전 (오류 발생 코드 예시):**\r\n",
    "\r\n",
    "```python\r\n",
    "from transformers import AutoProcessor, AutoModelForSeq2SeqLM\r\n",
    "\r\n",
    "model_name_whisper = \"openai/whisper-small\"\r\n",
    "processor_whisper = AutoProcessor.from_pretrained(model_name_whisper)\r\n",
    "model_whisper = AutoModelForSeq2SeqLM.from_pretrained(model_name_whisper) # <-- 이 부분이 문제!\r\n",
    "```\r\n",
    "\r\n",
    "**수정 후 (올바른 코드):**\r\n",
    "\r\n",
    "```python\r\n",
    "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq # 이 부분을 변경!\r\n",
    "\r\n",
    "model_name_whisper = \"openai/whisper-small\"\r\n",
    "processor_whisper = AutoProcessor.from_pretrained(model_name_whisper)\r\n",
    "model_whisper = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_whisper) # <-- 이렇게 사용해야 합니다.\r\n",
    "\r\n",
    "print(f\"Whisper 모델 로드 완료: {model_whisper.__class__.__name__}\")\r\n",
    "```\r\n",
    "\r\n",
    "이 변경 사항을 적용하면 `ValueError`가 해결되고 Whisper 모델이 올바르게 로드될 것입니다.\r\n",
    "\r\n",
    "**핵심 요약:** Hugging Face의 `AutoModel` 클래스를 사용할 때는, 로드하려는 모델의 종류(예: 텍스트-투-텍스트, 음성-투-텍스트, 분류 등)에 맞는 정확한 `AutoModelFor...` 클래스를 선택해야 합니다. Whisper와 같은 음성 시퀀스-투-시퀀스 모델에는 `AutoModelForSpeechSeq2Seq`가 올바른 선택입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3184cc9-5b3e-493a-a3eb-2c1f01b5e62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Environment",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
