{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0eb388df-ec8c-45e5-ac90-99e162b9d465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import torch.nn\n",
    "import urllib.request\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\",\n",
    "    filename=\"ChatBotData.csv\",\n",
    ")\n",
    "\n",
    "Chatbot_Data=ps.read_csv('ChatBotData.csv')\n",
    "\n",
    "Chatbot_Data=df[:300]\n",
    "Chatbot_Data.head(10)\n",
    "type(Chatbot_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b00edea5-75a4-47b1-9289-322ae61ad189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "BOS='</s>'\n",
    "EOS='</s>'\n",
    "PAD='<pad>'\n",
    "MASK='<unused0>'\n",
    "\n",
    "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token=BOS, eos_token=EOS, unk_token=\"<unk>\", pad_token=PAD, mask_token=MASK,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676022ac-39f2-4810-a899-151c48dcbff1",
   "metadata": {},
   "source": [
    "Tokenizer는 모델에 어떠한 입력을 넣어주기 위해서 전처리를 담당합니다. 토크나이저는 허깅페이스의 PreTrainedTokenizer 인 GPT2Tokenizer 를 사용합니다. Tokenizer들은 크게 3가지 기능을 제공합니다.\n",
    "\n",
    "Tokenizing : 입력 문자열을 token id로 변환(encoding), token id를 다시 문자열로 변환(decoding)의 기능\n",
    "기존의 구조(BPE, Sentencepiece 등)에 독립적으로 추가적인 token들을 추가하는 기능\n",
    "Special token들을 (mask, BOS, EOS 등) 관리하는 기능\n",
    "\n",
    "사용된 파라메터의 의미는 다음과 같습니다.\n",
    "\n",
    "bos_token : 문장의 시작을 나타내는 token\n",
    "eos_token : 문장의 끝을 나타내는 token\n",
    "unk_token : 모르는 단어를 나타내는 token\n",
    "pad_token : 동일한 batch 내에서 입력의 크기를 동일하게 하기 위해서 사용해는 token\n",
    "PreTrainedTokenizer 에서 제공되는 함수는\n",
    "\n",
    "tokenize() : tokenizer를 이용해서 string을 token id의 리스트로 변환한다.\n",
    "get_added_vocab() : token to index에 해당하는 dict를 리턴한다.\n",
    "batch_decode() : token id로 구성된 입력을 하나의 연결된 string으로 출력한다.\n",
    "convert_ids_to_tokens() : token id 의 리스트를 token으로 변환한다. skip_special_tokens=True로 하면 decoding할 때 special token들을 제거한다.\n",
    "convert_tokens_to_ids() : token string의 리스트를 token id 또는 Token id의 리스트로 변환한다.\n",
    "decode() : tokenizer 와 vocabulary를 이용해서 token id를 string으로 변환한다. skip_special_token=True로 지정하면 speical token들을 제외한다.\n",
    "encode() : token string을 token id 의 리스트로 변환한다. add_special_tokens=False로 지정하면 token id로 변환할 때 special token들을 제외한다. padding을 통해서 padding token을 어떻게 추가할지조 지정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df3111f3-8a0a-4d93-9427-9e77c97fe21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(data):\n",
    "    def __init__(self,chats,max_len=40):  # 데이터셋의 전처리를 해주는 부분\n",
    "        self._data=chats\n",
    "        self.max_len=max_len\n",
    "        self.q_token=Q_TKN\n",
    "        self.a_token=A_TKN\n",
    "        self.sent_token=SENT\n",
    "        self.eos=EOS\n",
    "        self.mask=MASK\n",
    "        self.tokenizer=koGPT2_TOKENIZER\n",
    "    def __len__(self):   # chatbotdata 의 길이를 리턴한다.\n",
    "        return len(self._data)\n",
    "    def __getitem__(self,idx):\n",
    "        trun=self._data.iloc[idx]\n",
    "        q=true['Q']\n",
    "        q=re.sub(r'[?.,!]',r' ',q)# 구둣점들을 제거한다.\n",
    "\n",
    "        a=trun['A']\n",
    "        a=re.sub(r'[?.,!]',r' ',a)# 구둣점들을 제거한다.\n",
    "\n",
    "        q_toked=self.tokenizer.tokenize(self.q_token+q+self.sent_token)\n",
    "        q_len=len(a_toked)\n",
    "        \n",
    "        a_toked=self.tokenizer.tokenize(self.a_token+a+self.eos)\n",
    "        a=len=len(a_toked)\n",
    "\n",
    "        if q_len>self.max_len:\n",
    "            a_len=self.max_len-q_len  #답변의 길이를 최대길이 - 질문길이\n",
    "            if a_len<=0:  #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
    "                q_toked=q_toked[-(int(self.max_len/2)):]  #질문길이를 최대길이의 반으로 \n",
    "                q_len=len(q_toked)\n",
    "                a_len=self.max_len-q_len\n",
    "            a_toked=a_toked[:a_len]\n",
    "            a_len=len(a_toked)\n",
    "\n",
    "        if q_len+a_len>self.max_len:\n",
    "            a_len=self.max_len-q_len\n",
    "            if a_len<=0:\n",
    "                q_toked=q_toked[-(int(self,max_len/2)):]\n",
    "                q_len=len(q_toked)\n",
    "                a_len=self.max_len-q_len\n",
    "            a_toked=a_toked[:a_len]\n",
    "            a_len=len(a_toked)\n",
    "\n",
    "        labels=[self.mask,]*q_len+[0]*(self.max_len-q_len-a_len)\n",
    "        labels_idx=self.tokenizer.convert_tokens_to_idx(labels)\n",
    "        while len(labels_idx)<self.max_len:\n",
    "            labels_idx+=[self.tokenizer/pad_token_id]\n",
    "        return (token_idx,np.array(mask),labels_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd195a2c-f2cf-4535-ac84-20d0e7f27a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collat_batch(batch):\n",
    "    data=[item[0]for item in batch]\n",
    "    mask=[item[1]for item in batch]\n",
    "    label=[item[2]for item in batch]\n",
    "    return torch.LongTensor(data),torch.LongTensor(mask),torch.LongTensor(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19783308-622a-45a1-8fbd-486eade08380",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_set\u001b[38;5;241m=\u001b[39m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mChatbot_Data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "train_set=dataset(Chatbot_Data,max_len=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf55b87-c0d4-4838-b3cb-573906a339aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a9699-be7c-46cb-aabc-4484bdaed040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea573f-a9d7-4c8a-85fe-8258f9c0310f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d3565-b31e-4855-8d98-a6de04d82142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3777ae2-7eba-4194-b1dc-8a3aea19b382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82fd8e0-7d59-4496-a87a-23a2be5449dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8cd762-9d71-4239-9bde-a5c7f4c63eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afbdf4-e558-4d44-928c-41e2c03bc264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ed08f93-0560-4b94-81fa-aae23ba4d650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import urllib.request\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\",\n",
    "    filename=\"ChatBotData.csv\",\n",
    ")\n",
    "Chatbot_Data = pd.read_csv(\"ChatBotData.csv\")\n",
    "\n",
    "# Test 용으로 300개 데이터만 처리한다.\n",
    "Chatbot_Data = Chatbot_Data[:300]\n",
    "Chatbot_Data.head()\n",
    "\n",
    "\n",
    "BOS = \"</s>\"\n",
    "EOS = \"</s>\"\n",
    "PAD = \"<pad>\"\n",
    "MASK = \"<unused0>\"\n",
    "\n",
    "# 허깅페이스 transformers 에 등록된 사전 학습된 koGTP2 토크나이저를 가져온다.\n",
    "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token=BOS, eos_token=EOS, unk_token=\"<unk>\", pad_token=PAD, mask_token=MASK,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c78c82c1-2701-4f70-8a89-f7f20e3e9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 챗봇 데이터를 처리하는 클래스를 만든다.\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, chats, max_len=40):  # 데이터셋의 전처리를 해주는 부분\n",
    "        self._data = chats\n",
    "        self.max_len = max_len\n",
    "        self.q_token = Q_TKN\n",
    "        self.a_token = A_TKN\n",
    "        self.sent_token = SENT\n",
    "        self.eos = EOS\n",
    "        self.mask = MASK\n",
    "        self.tokenizer = koGPT2_TOKENIZER\n",
    "\n",
    "    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):  # 로드한 챗봇 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n",
    "        turn = self._data.iloc[idx]\n",
    "        q = turn[\"Q\"]  # 질문을 가져온다.\n",
    "        q = re.sub(r\"([?.!,])\", r\" \", q)  # 구둣점들을 제거한다.\n",
    "\n",
    "        a = turn[\"A\"]  # 답변을 가져온다.\n",
    "        a = re.sub(r\"([?.!,])\", r\" \", a)  # 구둣점들을 제거한다.\n",
    "\n",
    "        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)\n",
    "        q_len = len(q_toked)\n",
    "\n",
    "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
    "        a_len = len(a_toked)\n",
    "\n",
    "        #질문의 길이가 최대길이보다 크면\n",
    "        if q_len > self.max_len:\n",
    "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
    "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "\n",
    "        #질문의 길이 + 답변의 길이가 최대길이보다 크면\n",
    "        if q_len + a_len > self.max_len:\n",
    "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
    "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "\n",
    "        # 답변 labels = [mask, mask, ...., mask, ..., <bos>,..답변.. <eos>, <pad>....]\n",
    "        labels = [self.mask,] * q_len + a_toked[1:]\n",
    "\n",
    "        # mask = 질문길이 0 + 답변길이 1 + 나머지 0\n",
    "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
    "        # 답변 labels을 index 로 만든다.\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
    "        # 최대길이만큼 PADDING\n",
    "        while len(labels_ids) < self.max_len:\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        # 질문 + 답변을 index 로 만든다.    \n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
    "        # 최대길이만큼 PADDING\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        #질문+답변, 마스크, 답변\n",
    "        return (token_ids, np.array(mask), labels_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fff97a25-1b67-4990-bd1c-bb8c42ffab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    mask = [item[1] for item in batch]\n",
    "    label = [item[2] for item in batch]\n",
    "    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5a77bde-1af3-43c2-acc3-a7d4a1fbbf67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Q_TKN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_set \u001b[38;5;241m=\u001b[39m \u001b[43mChatbotDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mChatbot_Data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#윈도우 환경에서 num_workers 는 무조건 0으로 지정, 리눅스에서는 2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_batch,)\n",
      "Cell \u001b[0;32mIn[37], line 6\u001b[0m, in \u001b[0;36mChatbotDataset.__init__\u001b[0;34m(self, chats, max_len)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m chats\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len \u001b[38;5;241m=\u001b[39m max_len\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_token \u001b[38;5;241m=\u001b[39m \u001b[43mQ_TKN\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_token \u001b[38;5;241m=\u001b[39m A_TKN\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msent_token \u001b[38;5;241m=\u001b[39m SENT\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Q_TKN' is not defined"
     ]
    }
   ],
   "source": [
    "train_set = ChatbotDataset(Chatbot_Data, max_len=40)\n",
    "\n",
    "#윈도우 환경에서 num_workers 는 무조건 0으로 지정, 리눅스에서는 2\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, num_workers=0, shuffle=True, collate_fn=collate_batch,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ca07b-809f-4c95-88e1-7826b0a651af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597b9d9-7dfd-43ed-b548-f08a80582330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
