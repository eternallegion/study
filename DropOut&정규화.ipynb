{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5747843b-a326-479c-946d-c305f9b0a53f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Day 22: 정규화 기법 & Dropout\n",
    "\n",
    "### 1. Theory: 왜 정규화(Regularization)가 필요할까?\n",
    "\n",
    "* **Overfitting(과적합)**\n",
    "\n",
    "  * 모델이 학습 데이터에 너무 최적화되어, 새로운 데이터에 대한 일반화 성능이 떨어짐\n",
    "  * 복잡한 모델일수록, 파라미터가 많을수록 과적합 위험이 커짐\n",
    "\n",
    "* **정규화 기법의 목적**\n",
    "\n",
    "  * 모델 파라미터에 패널티를 주어, 지나치게 복잡해지는 것을 억제\n",
    "  * 결과적으로 더 일반화된(새로운 데이터에도 잘 작동하는) 모델로 유도\n",
    "\n",
    "#### 1.1 L2 정규화 (Weight Decay)\n",
    "\n",
    "* **원리**: 손실 함수에 가중치의 제곱합을 추가\n",
    "\n",
    "  $$\n",
    "    \\mathcal{L}_{\\text{total}}\n",
    "      = \\mathcal{L}_{\\text{data}}\n",
    "      + \\lambda \\sum_i w_i^2\n",
    "  $$\n",
    "\n",
    "  * $\\lambda$: 정규화 강도(하이퍼파라미터)\n",
    "  * 큰 가중치 값에 더 큰 페널티 → 가중치가 0에 가깝도록 유도\n",
    "\n",
    "* **PyTorch 사용법**:\n",
    "\n",
    "  ```python\n",
    "  optimizer = torch.optim.SGD(\n",
    "      model.parameters(),\n",
    "      lr=0.01,\n",
    "      weight_decay=1e-4  # L2 λ 값\n",
    "  )\n",
    "  ```\n",
    "\n",
    "#### 1.2 L1 정규화\n",
    "\n",
    "* **원리**: 손실 함수에 가중치 절댓값 합을 추가\n",
    "\n",
    "  $$\n",
    "    \\mathcal{L}_{\\text{total}}\n",
    "      = \\mathcal{L}_{\\text{data}}\n",
    "      + \\lambda \\sum_i |w_i|\n",
    "  $$\n",
    "\n",
    "  * 많은 파라미터를 정확히 0으로 만들기 때문에, \\*\\*스파스(sparse)\\*\\*한 모델에 유리\n",
    "* **PyTorch 사용법** (직접 구현):\n",
    "\n",
    "  ```python\n",
    "  # training loop 내에서\n",
    "  l1_lambda = 1e-5\n",
    "  l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "  loss = data_loss + l1_lambda * l1_norm\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  ```\n",
    "\n",
    "#### 1.3 Dropout\n",
    "\n",
    "* **원리**: 학습 중에 뉴런을 랜덤으로 비활성화(drop)\n",
    "\n",
    "  * 각 미니배치마다 다른 서브네트워크(sub-network)를 학습하는 효과\n",
    "  * 뉴런에 의존하는 정도를 낮춰, 과적합 완화\n",
    "* **PyTorch 사용법**:\n",
    "\n",
    "  ```python\n",
    "  import torch.nn as nn\n",
    "\n",
    "  class MLP(nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "          self.fc1 = nn.Linear(784, 256)\n",
    "          self.dropout = nn.Dropout(p=0.5)  # 드롭아웃 확률\n",
    "          self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "      def forward(self, x):\n",
    "          x = torch.relu(self.fc1(x))\n",
    "          x = self.dropout(x)           # 학습 시에만 활성화\n",
    "          return self.fc2(x)\n",
    "  ```\n",
    "* **주의**:\n",
    "\n",
    "  * `model.train()` 모드에서만 드롭아웃이 적용되고 (`.eval()` 모드에서는 비활성화)\n",
    "  * 너무 큰 드롭아웃 확률(p>0.7)은 오히려 학습을 방해할 수 있음\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 실습 과제\n",
    "\n",
    "1. **L1 정규화**를 직접 추가해 보고\n",
    "\n",
    "   * `l1_lambda` 값을 바꿔가며 훈련 정확도·검증 정확도 관찰\n",
    "2. **학습률 스케줄러**(`StepLR` 등)와 조합하여 성능 비교\n",
    "3. **드롭아웃 확률**(0.2, 0.5, 0.8)을 바꿔 가며 모델 성능 변화 기록\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ea840bd-3496-45f9-8a66-6d6dd9be3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader as loader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89b90e04-56d8-4d99-ba59-9f4065dcb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_data=datasets.MNIST(root='./data', download=True, train=True, transform=transform)\n",
    "test_data=datasets.MNIST(root='./data', download=True, train=False, transform=transform)\n",
    "train_loader=loader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader=loader(test_data, batch_size=1000,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "884041c6-359a-4c67-98a4-268b26bed786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dropout_p=0.0):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1=nn.Linear(28*28,256)\n",
    "        self.drop=nn.Dropout(p=dropout_p)\n",
    "        self.fc2=nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.view(-1, 28*28)\n",
    "        x=self.fc1(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.drop(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "df7a143a-78ac-4dc0-845f-b404b6770409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available()else'cpu')\n",
    "print(device)\n",
    "def train(model, train_loader,test_loader, criter, optim, num_epoch):\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    avg_acc=0.0\n",
    "    for epoch in range(num_epoch):\n",
    "        total_loss=0.0\n",
    "        correct=0.0\n",
    "        for data, target in train_loader:\n",
    "            data,target=data.to(device), target.to(device)\n",
    "            optim.zero_grad()\n",
    "            out=model(data).to(device)\n",
    "            loss=criter(out, target)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss+=loss.item()\n",
    "            correct+=(out.argmax(1)==target).sum().item()\n",
    "        avg_acc=correct/len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        correct=0.0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                datas,targets=data.to(device), target.to(device)\n",
    "                correct+=(model(datas).to(device).argmax(1)==targets).sum().item()\n",
    "            val_acc = correct / len(test_loader.dataset)\n",
    "            print(f\"Epoch {epoch+1}: train_acc={avg_acc:.4f}, val_acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9176f285-1e96-4146-a584-9f6e7311a46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base MLP ===\n",
      "Epoch 1: train_acc=0.7778, val_acc=0.8801\n",
      "Epoch 2: train_acc=0.8425, val_acc=0.8179\n",
      "Epoch 3: train_acc=0.8409, val_acc=0.8808\n",
      "Epoch 4: train_acc=0.8701, val_acc=0.8651\n",
      "Epoch 5: train_acc=0.8640, val_acc=0.8612\n",
      "Epoch 6: train_acc=0.8571, val_acc=0.8482\n",
      "Epoch 7: train_acc=0.8537, val_acc=0.8652\n",
      "Epoch 8: train_acc=0.8586, val_acc=0.8680\n",
      "Epoch 9: train_acc=0.8618, val_acc=0.8523\n",
      "Epoch 10: train_acc=0.8505, val_acc=0.8575\n"
     ]
    }
   ],
   "source": [
    "model=Model(dropout_p=0.0).to(device)\n",
    "num_epoch=10\n",
    "lr=1e-4\n",
    "criter = nn.CrossEntropyLoss()\n",
    "optimy=optim.SGD(model.parameters(), lr=lr)\n",
    "print(\"=== Base MLP ===\")\n",
    "train(model, train_loader,test_loader, criter, optimy, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3835f6a1-d034-43c0-86e8-f0b4c15c92d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        # ——— 학습 단계 ———\n",
    "        model.train()  \n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()           # ◀ 반드시 호출!\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (logits.argmax(1) == yb).sum().item()\n",
    "\n",
    "        train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "        # ——— 평가 단계 ———\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb)\n",
    "                correct += (logits.argmax(1) == yb).sum().item()\n",
    "\n",
    "        val_acc = correct / len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Epoch {epoch}: \"\n",
    "              f\"train_acc={train_acc:.4f}, \"\n",
    "              f\"val_acc={val_acc:.4f}, \"\n",
    "              f\"avg_loss={total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c15f5361-871b-4d40-a901-cab04e05033c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regularized MLP (L2 + Dropout) ===\n",
      "Epoch 1: train_acc=0.0871, val_acc=0.0855, avg_loss=2.3139\n",
      "Epoch 2: train_acc=0.0854, val_acc=0.0855, avg_loss=2.3143\n",
      "Epoch 3: train_acc=0.0869, val_acc=0.0855, avg_loss=2.3144\n",
      "Epoch 4: train_acc=0.0852, val_acc=0.0855, avg_loss=2.3142\n",
      "Epoch 5: train_acc=0.0854, val_acc=0.0855, avg_loss=2.3145\n",
      "Epoch 6: train_acc=0.0864, val_acc=0.0855, avg_loss=2.3142\n",
      "Epoch 7: train_acc=0.0856, val_acc=0.0855, avg_loss=2.3143\n",
      "Epoch 8: train_acc=0.0862, val_acc=0.0855, avg_loss=2.3140\n",
      "Epoch 9: train_acc=0.0864, val_acc=0.0855, avg_loss=2.3145\n",
      "Epoch 10: train_acc=0.0857, val_acc=0.0855, avg_loss=2.3145\n"
     ]
    }
   ],
   "source": [
    "# 2) 정규화 + Dropout MLP\n",
    "model=Model(dropout_p=0.0).to(device)\n",
    "num_epoch=10\n",
    "lr=1e-4\n",
    "criter = nn.CrossEntropyLoss()\n",
    "reg_model = Model(dropout_p=0.1).to(device)\n",
    "# L2 λ=1e-4 적용\n",
    "opt2 = optim.SGD(reg_model.parameters(), lr=0.01)#, weight_decay=1e-4)\n",
    "print(\"\\n=== Regularized MLP (L2 + Dropout) ===\")\n",
    "train(reg_model, train_loader,test_loader, criter, optimy, num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7f446de9-8c48-4839-ae87-92f32591030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_acc=0.1775, val_acc=0.3011, avg_loss=2.2635\n",
      "Epoch 2: train_acc=0.3375, val_acc=0.4226, avg_loss=2.1658\n",
      "Epoch 3: train_acc=0.4395, val_acc=0.5014, avg_loss=2.0806\n",
      "Epoch 4: train_acc=0.5096, val_acc=0.5600, avg_loss=1.9982\n",
      "Epoch 5: train_acc=0.5612, val_acc=0.6092, avg_loss=1.9148\n",
      "Epoch 6: train_acc=0.6023, val_acc=0.6433, avg_loss=1.8298\n",
      "Epoch 7: train_acc=0.6280, val_acc=0.6731, avg_loss=1.7467\n",
      "Epoch 8: train_acc=0.6533, val_acc=0.6931, avg_loss=1.6641\n",
      "Epoch 9: train_acc=0.6746, val_acc=0.7119, avg_loss=1.5835\n",
      "Epoch 10: train_acc=0.6928, val_acc=0.7277, avg_loss=1.5062\n"
     ]
    }
   ],
   "source": [
    "reg_model = Model(dropout_p=0.1).to(device)\n",
    "optimizer = optim.SGD(\n",
    "    reg_model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(\n",
    "    model=reg_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e174a4a-272f-491c-a2b5-50cab8fafdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CUDA 12.4)",
   "language": "python",
   "name": "cuda124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
