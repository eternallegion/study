{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2c605c0-c423-4590-9375-5362ff33a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image #as mg\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchvision import models,transforms,datasets\n",
    "from torch.utils.data import DataLoader as loader\n",
    "from torchvision.datasets import CIFAR10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "64d10119-9e98-49ee-a944-550ea2d17054",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cuda'if torch.cuda.is_available()else'cpu')\n",
    "print(device, 'mode')\n",
    "model=models.resnet18(pretrained=True).to(device).eval()\n",
    "# 예시: 마지막 레이어만 10개로 교체\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10).to(device)\n",
    "# 그리고 나서 train_loader로 fine-tune\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4d60071a-7257-44f3-b137-2d2a30cc15b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer=model.layer4[1].conv2 #마지박부분\\\n",
    "prune.ln_structured(\n",
    "    layer,name='weight',\n",
    "    amount=0.3,# 30% 채널 제거\n",
    "    n=1, # L1-norm 기준\n",
    "    dim=0 # 출력 채널 축\n",
    ")\n",
    "\n",
    "prune.remove(layer,'weight')\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c787befb-7fa4-49d5-a151-8bff98bbd5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "preprocess=transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "test_data=CIFAR10(root='./data',train=False,download=True,transform=preprocess)\n",
    "#test_ds = CIFAR10(root='./data', train=False, download=True)\n",
    "print(test_data.classes)\n",
    "\n",
    "test_loader=loader(test_data,batch_size=1,shuffle=False)\n",
    "\n",
    "img,true_label=next(iter(test_loader))\n",
    "img=img.to(device)\n",
    "\n",
    "#img=preprocess(mg.open('dog.jpg')).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b860f7-2b65-4678-881b-540ae280bc17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b32dc03e-334c-49f8-9c0d-3e9858975c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True class:  cat\n",
      "Predicted:   frog\n"
     ]
    }
   ],
   "source": [
    "true_label = 3\n",
    "predicted  = 6\n",
    "print(f\"True class:  {test_data.classes[true_label]}\")\n",
    "print(f\"Predicted:   {test_data.classes[predicted]}\")\n",
    "# True class:  cat\n",
    "# Predicted:   frog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d39ce778-1d1a-47d5-b111-308e72a69798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.0882\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for x,y in test_loader:\n",
    "    x = x.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(x).argmax(1).item()\n",
    "    if pred == y:\n",
    "        correct += 1\n",
    "print(\"Test Accuracy:\", correct / len(test_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e26ca-7743-4cdc-9f4d-e1e2364179f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1abcd45a-f23a-48a9-84f0-d1b8fc5a9178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 3, Pruned ResNet18 prediction: 6\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logit=model(img)\n",
    "    pred=logit.argmax(1).item()\n",
    "\n",
    "print(f\"True label: {true_label}, Pruned ResNet18 prediction: {pred}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a7309e1-8923-4f65-9691-100b371d36fe",
   "metadata": {},
   "source": [
    "\n",
    "# 1) 미리 다운로드해 두었다고 가정한 imagenet_class_index.json 파일 로드\n",
    "import json\n",
    "with open(\"imagenet_class_index.json\") as f:\n",
    "    idx2label = json.load(f)\n",
    "# 2) 381번 인덱스에 해당하는 (ID, human-readable name) 조회\n",
    "print(idx2label[str(381)])  \n",
    "# 예) [\"n02119022\", \"red_fox\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7920c3-f523-4e79-a951-222519516306",
   "metadata": {},
   "source": [
    "여기서 출력된 값들의 의미는 이렇습니다:\r\n",
    "\r\n",
    "1. **True label: tensor(\\[3])**\r\n",
    "   이건 **CIFAR-10** 데이터셋에서 가져온 정답 레이블입니다.\r\n",
    "   CIFAR-10의 클래스 인덱스는\r\n",
    "\r\n",
    "   ```\r\n",
    "   0: airplane  \r\n",
    "   1: automobile  \r\n",
    "   2: bird  \r\n",
    "   3: cat  \r\n",
    "   4: deer  \r\n",
    "   5: dog  \r\n",
    "   6: frog  \r\n",
    "   7: horse  \r\n",
    "   8: ship  \r\n",
    "   9: truck  \r\n",
    "   ```\r\n",
    "\r\n",
    "   이므로, `3`은 **cat** (고양이)를 뜻합니다.\r\n",
    "\r\n",
    "2. **Pruned ResNet18 prediction: 381**\r\n",
    "   이 숫자 `381`은 **ImageNet**용으로 사전학습된 ResNet18이 출력한 **1000개 중 하나의 클래스 인덱스**입니다.\r\n",
    "   즉, 당신이 CIFAR-10 이미지를 `pretrained=True` ResNet18에 바로 넣었기 때문에,\r\n",
    "   모델은 “ImageNet”의 1000개 레이블 중 381번(382번째)에 해당하는 카테고리를 선택한 겁니다.\r\n",
    "\r\n",
    "   만약 이 인덱스를 사람이 읽을 수 있는 이름으로 바꾸려면, ImageNet의 클래스 맵 파일을 로드해서 조회하면 됩니다. 예를 들어:\r\n",
    "\r\n",
    "   ```python\r\n",
    "   # 1) 미리 다운로드해 두었다고 가정한 imagenet_class_index.json 파일 로드\r\n",
    "   import json\r\n",
    "   with open(\"imagenet_class_index.json\") as f:\r\n",
    "       idx2label = json.load(f)\r\n",
    "   # 2) 381번 인덱스에 해당하는 (ID, human-readable name) 조회\r\n",
    "   print(idx2label[str(381)])  \r\n",
    "   # 예) [\"n02119022\", \"red_fox\"]\r\n",
    "   ```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 해결책 및 권장 방법\r\n",
    "\r\n",
    "* **ImageNet 모델을 CIFAR-10에 바로 쓰지 마시고**,\r\n",
    "\r\n",
    "  1. **마지막 fc 레이어**만 `nn.Linear(512, 10)` 같이 10개 출력으로 교체 →\r\n",
    "  2. CIFAR-10 데이터로 **파인튜닝(fine-tune)** →\r\n",
    "  3. 그 다음에 `model(img)` 를 호출하면 `pred` 도 0\\~9 범위로 나옵니다.\r\n",
    "\r\n",
    "```python\r\n",
    "# 예시: 마지막 레이어만 10개로 교체\r\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10).to(device)\r\n",
    "# 그리고 나서 train_loader로 fine-tune\r\n",
    "```\r\n",
    "\r\n",
    "* 이렇게 하면 `pred` 가 0\\~9 사이의 **CIFAR-10 클래스 인덱스**(예: “3 → cat”)로 나오게 됩니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "정리하자면:\r\n",
    "\r\n",
    "> **“381”** 은 **ImageNet 1000개 레이블** 중 하나의 번호이고,\r\n",
    "> **“3”** 은 **CIFAR-10 10개 레이블** 중 하나의 번호입니다.\r\n",
    "\r\n",
    "두 레이블 체계가 다르니, CIFAR-10 작업을 하실 땐 모델을 **10개 클래스용**으로 바꾼 뒤 반드시 **재학습(fine-tuning)** 하셔야 올바른 예측 결과를 얻으실 수 있습니다.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3394029-26fb-46e4-be93-ff7584729ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da74533-9875-4597-a024-8bb2baff2afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6fe3e2c2-35a3-4621-ae83-9ad138ca5fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quantization import get_default_qconfig, quantize_jit, QuantStub, DeQuantStub, fuse_modules, prepare_qat, convert\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchvision import models\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import models,transforms,datasets\n",
    "from torch.utils.data import DataLoader as loader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "01139ddb-76d3-4b54-985b-9dd5cd88d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.quantization import get_default_qconfig, QuantStub, DeQuantStub, fuse_modules, prepare_qat, convert\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a01c26d4-3905-443e-aa9d-df73a0f8f51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda mode\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "num_epoch = 5 # FP32 모델 학습 에폭\n",
    "qat_epoch = 5 # QAT 학습 에폭\n",
    "save_path = \"./qat_resnet18_model.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device} mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8621f5-6504-4b0c-8559-ab1844e96a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "fdf2d61e-1cab-47ac-92a0-9d730017ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224), \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224), \n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab7af8-e7e4-4bf7-a2de-5bb7ae1fa742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "77bd0b06-cac5-43e9-890e-249078406038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches (subset): 79\n",
      "Number of test batches (subset): 16\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# 학습 속도를 위해 데이터셋 서브셋 사용 (선택 사항)\n",
    "subset_size_train = 5000\n",
    "subset_size_test = 1000\n",
    "\n",
    "train_subset_indices = torch.randperm(len(train_data))[:subset_size_train]\n",
    "test_subset_indices = torch.randperm(len(test_data))[:subset_size_test]\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_data, train_subset_indices)\n",
    "test_subset = torch.utils.data.Subset(test_data, test_subset_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Number of training batches (subset): {len(train_loader)}\")\n",
    "print(f\"Number of test batches (subset): {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "ab2a7081-cadf-4604-910b-114b0147e5e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_res(num_classes=10, pretrained=True): \n",
    "    \"\"\"\n",
    "    ResNet18 모델을 로드하고, 마지막 Fully Connected Layer를\n",
    "    지정된 num_classes에 맞게 수정합니다.\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c487465b-3ef0-4741-b88c-74d2ceff05d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model definition and QAT preparation functions defined.\n"
     ]
    }
   ],
   "source": [
    "def prepare_model_for_qat(model): \n",
    "    \"\"\"\n",
    "    QAT를 위해 모델에 QuantStub/DeQuantStub를 삽입하고,\n",
    "    ResNet18의 특정 모듈들을 퓨징합니다.\n",
    "    \"\"\"\n",
    "    model.quant = QuantStub()\n",
    "    model.dequant = DeQuantStub() \n",
    "\n",
    "    # ResNet18의 기본 블록 구조에 맞춰 모듈 퓨징\n",
    "    # (conv1, bn1, relu)와 각 BasicBlock 내부의 (conv, bn, relu)를 퓨징\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Sequential): \n",
    "            for basic_block_name, basic_block_module in module.named_children():\n",
    "                if isinstance(basic_block_module, models.resnet.BasicBlock):\n",
    "                    # BasicBlock의 첫 번째 Conv-BN-ReLU 시퀀스\n",
    "                    torch.quantization.fuse_modules(basic_block_module, [['conv1', 'bn1', 'relu']], inplace=True)\n",
    "                    # BasicBlock의 두 번째 Conv-BN (ReLU는 여기에 붙지 않음)\n",
    "                    torch.quantization.fuse_modules(basic_block_module, [['conv2', 'bn2']], inplace=True)\n",
    "    # 모델의 첫 번째 Conv-BN-ReLU 시퀀스\n",
    "    torch.quantization.fuse_modules(model, [['conv1', 'bn1', 'relu']], inplace=True)\n",
    "    return model\n",
    "\n",
    "print(\"Model definition and QAT preparation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "459ce99a-fcf5-43b7-a929-67b2779edf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mode(model, train_loader, criter, optim, num_epoch, device, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    모델을 학습시키는 일반적인 학습 루프입니다.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    print(f\"\\n--- {model_name} Training ---\")\n",
    "    for epoch in range(num_epoch):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epoch} ({model_name})\")):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optim.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criter(out, target)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        acc = 100 * correct_predictions / total_predictions\n",
    "        print(f\"Epoch {epoch+1} Complete: Avg Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%\")\n",
    "    print(f\"{model_name} Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "3458f1ac-7c43-438e-b2f8-2b7aa74e236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "def eval_mode(model, test_loader, device, model_name='Model'):\n",
    "    \"\"\"\n",
    "    모델의 정확도를 평가하는 함수입니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            out = model(data)\n",
    "            _, pred = torch.max(out.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (pred == target).sum().item()\n",
    "        acc = 100 * correct / total\n",
    "        print(f'Accuracy on test set ({model_name}): {acc:.2f}%')\n",
    "    return acc\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b3bc053a-555f-41bf-a995-e078f06755af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FP32 ResNet18 Model Training and Evaluation ---\n",
      "\n",
      "--- FP32 ResNet18 Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bc345478564b8bb9ddfbc09ebf5fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 (FP32 ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Complete: Avg Loss: 1.3560, Accuracy: 52.78%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927f9fc1f37d4255807c95b8992bb772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 (FP32 ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Complete: Avg Loss: 0.9098, Accuracy: 68.68%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bbb308b4324c95bef69785edfdb887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 (FP32 ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Complete: Avg Loss: 0.7525, Accuracy: 74.36%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0051362e384bc7bc53157248dac4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 (FP32 ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Complete: Avg Loss: 0.6734, Accuracy: 77.14%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3505119c89114fd49fe4455788308595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 (FP32 ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Complete: Avg Loss: 0.6106, Accuracy: 78.82%\n",
      "FP32 ResNet18 Training finished!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726b64604fdf43339cd4df06cae3f221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating FP32 ResNet18:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set (FP32 ResNet18): 85.30%\n",
      "FP32 ResNet18 model saved to ./qat_resnet18_model_fp32.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- FP32 ResNet18 Model Training and Evaluation ---\")\n",
    "fp32_model = get_res(num_classes=10, pretrained=True).to(device) \n",
    "criter = nn.CrossEntropyLoss()\n",
    "optimy = optim.Adam(fp32_model.parameters(), lr=lr)\n",
    "\n",
    "train_mode(fp32_model, train_loader, criter, optimy, num_epoch, device, model_name=\"FP32 ResNet18\")\n",
    "fp32_accuracy = eval_mode(fp32_model, test_loader, device, model_name=\"FP32 ResNet18\")\n",
    "\n",
    "fp32_model_path = save_path.replace(\".pth\", \"_fp32.pth\")\n",
    "torch.save(fp32_model.state_dict(), fp32_model_path)\n",
    "print(f\"FP32 ResNet18 model saved to {fp32_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1e40ebb2-5b0c-41c7-b4b4-0408bc26f822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Quantization-Aware Training (QAT) for ResNet18 ---\n",
      "QAT model initialized with FP32 model weights.\n",
      "Preparing ResNet18 for QAT (fusing modules)...\n",
      "ResNet18 QAT preparation complete.\n",
      "Setting QConfig (fbgemm)...\n",
      "QConfig set to: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "Calling torch.quantization.prepare_qat...\n",
      "Model prepared for QAT.\n",
      "\n",
      "Starting QAT for 5 epochs...\n",
      "\n",
      "--- QAT ResNet18 Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56ffd6905e84539a7528262b728638f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 (QAT ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Complete: Avg Loss: 235951.8902, Accuracy: 15.52%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de711fb8063d47eabb01b4de63e463a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 (QAT ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Complete: Avg Loss: 8159.6862, Accuracy: 23.98%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c24cef7870437b99c42f9eae09f684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 (QAT ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Complete: Avg Loss: 5892.9919, Accuracy: 27.12%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a67c5c5c7a04d2ca99466aad1e25ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 (QAT ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Complete: Avg Loss: 4801.2242, Accuracy: 28.80%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c111dfa3c8142d1b09be6cdb35f40ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 (QAT ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Complete: Avg Loss: 3820.8045, Accuracy: 31.86%\n",
      "QAT ResNet18 Training finished!\n",
      "\n",
      "Converting QAT model to quantized model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported qscheme: per_channel_affine",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[246], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConverting QAT model to quantized model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m qat_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 34\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m convert(qat_model, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel converted to fully quantized (INT8) ResNet18.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:655\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[0;32m    654\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[1;32m--> 655\u001b[0m _convert(\n\u001b[0;32m    656\u001b[0m     module,\n\u001b[0;32m    657\u001b[0m     mapping,\n\u001b[0;32m    658\u001b[0m     inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    659\u001b[0m     is_reference\u001b[38;5;241m=\u001b[39mis_reference,\n\u001b[0;32m    660\u001b[0m     convert_custom_config_dict\u001b[38;5;241m=\u001b[39mconvert_custom_config_dict,\n\u001b[0;32m    661\u001b[0m     use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant,\n\u001b[0;32m    662\u001b[0m )\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[0;32m    664\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:720\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[0;32m    711\u001b[0m     ):\n\u001b[0;32m    712\u001b[0m         _convert(\n\u001b[0;32m    713\u001b[0m             mod,\n\u001b[0;32m    714\u001b[0m             mapping,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    718\u001b[0m             use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant,\n\u001b[0;32m    719\u001b[0m         )\n\u001b[1;32m--> 720\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(\n\u001b[0;32m    721\u001b[0m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001b[0;32m    722\u001b[0m     )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    725\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:762\u001b[0m, in \u001b[0;36mswap_module\u001b[1;34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    760\u001b[0m sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(qmod\u001b[38;5;241m.\u001b[39mfrom_float)\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_precomputed_fake_quant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[1;32m--> 762\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(\n\u001b[0;32m    763\u001b[0m         mod, use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant\n\u001b[0;32m    764\u001b[0m     )\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:172\u001b[0m, in \u001b[0;36mConvReLU2d.from_float\u001b[1;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mod\u001b[38;5;241m.\u001b[39mbn\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mod\u001b[38;5;241m.\u001b[39mbn\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     mod\u001b[38;5;241m.\u001b[39mweight, mod\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m fuse_conv_bn_weights(\n\u001b[0;32m    164\u001b[0m         mod\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    165\u001b[0m         mod\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         mod\u001b[38;5;241m.\u001b[39mbn\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfrom_float(\n\u001b[0;32m    173\u001b[0m     mod, use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant\n\u001b[0;32m    174\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:606\u001b[0m, in \u001b[0;36mConv2d.from_float\u001b[1;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_float\u001b[39m(\u001b[38;5;28mcls\u001b[39m, mod, use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    600\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a quantized module from a float module or qparams_dict.\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;124;03m        mod (Module): a float module, either produced by torch.ao.quantization\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m          utilities or provided by the user\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ConvNd\u001b[38;5;241m.\u001b[39mfrom_float(\n\u001b[0;32m    607\u001b[0m         \u001b[38;5;28mcls\u001b[39m, mod, use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant\n\u001b[0;32m    608\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:322\u001b[0m, in \u001b[0;36m_ConvNd.from_float\u001b[1;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    320\u001b[0m         mod \u001b[38;5;241m=\u001b[39m mod[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    321\u001b[0m     weight_post_process \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mqconfig\u001b[38;5;241m.\u001b[39mweight()\n\u001b[1;32m--> 322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_qconv(mod, activation_post_process, weight_post_process)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:266\u001b[0m, in \u001b[0;36m_ConvNd.get_qconv\u001b[1;34m(cls, mod, activation_post_process, weight_post_process)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# the __init__ call used is the one from derived classes and not the one from _ConvNd\u001b[39;00m\n\u001b[0;32m    255\u001b[0m qconv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    256\u001b[0m     mod\u001b[38;5;241m.\u001b[39min_channels,\n\u001b[0;32m    257\u001b[0m     mod\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m     mod\u001b[38;5;241m.\u001b[39mpadding_mode,\n\u001b[0;32m    265\u001b[0m )\n\u001b[1;32m--> 266\u001b[0m qconv\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, mod\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    268\u001b[0m     activation_post_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m activation_post_process\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat\n\u001b[0;32m    270\u001b[0m ):\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qconv  \u001b[38;5;66;03m# dynamic quantization doesn't need scale/zero_point\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:567\u001b[0m, in \u001b[0;36mConv2d.set_weight_bias\u001b[1;34m(self, w, b)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, w: torch\u001b[38;5;241m.\u001b[39mTensor, b: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mconv2d_prepack(\n\u001b[0;32m    568\u001b[0m             w, b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    569\u001b[0m         )\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    571\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mconv2d_prepack(\n\u001b[0;32m    572\u001b[0m             w, b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    573\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\_ops.py:1158\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported qscheme: per_channel_affine"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Quantization-Aware Training (QAT) for ResNet18 ---\")\n",
    "qat_model = get_res(num_classes=10, pretrained=True).to(device)\n",
    "\n",
    "qat_model.load_state_dict(fp32_model.state_dict())\n",
    "print(\"QAT model initialized with FP32 model weights.\")\n",
    "\n",
    "print(\"Preparing ResNet18 for QAT (fusing modules)...\")\n",
    "# 퓨징 전에 eval() 모드로 전환\n",
    "qat_model.eval() \n",
    "qat_model = prepare_model_for_qat(qat_model) \n",
    "print(\"ResNet18 QAT preparation complete.\")\n",
    "\n",
    "print(\"Setting QConfig (fbgemm)...\")\n",
    "qat_model.qconfig = get_default_qconfig('fbgemm')\n",
    "print(f\"QConfig set to: {qat_model.qconfig}\")\n",
    "\n",
    "print(\"Calling torch.quantization.prepare_qat...\")\n",
    "# prepare_qat을 호출하기 전에 다시 train() 모드로 전환\n",
    "qat_model.train() # #### 변경 사항 8: prepare_qat 호출 전에 모델을 train() 모드로 전환\n",
    "prepare_qat(qat_model, inplace=True)\n",
    "print(\"Model prepared for QAT.\")\n",
    "\n",
    "# 퓨징 및 prepare_qat 완료 후, QAT 학습을 위해 이미 train() 모드이므로 이 줄은 중복입니다.\n",
    "# 하지만 코드가 명확하게 보이도록 그대로 두거나 제거할 수 있습니다.\n",
    "# qat_model.train() \n",
    "\n",
    "criter_qt = nn.CrossEntropyLoss()\n",
    "optimy_qt = optim.Adam(qat_model.parameters(), lr=lr * 0.1) \n",
    "print(f\"\\nStarting QAT for {qat_epoch} epochs...\")\n",
    "train_mode(qat_model,train_loader, criter_qt, optimy_qt, qat_epoch, device, model_name=\"QAT ResNet18\")\n",
    "\n",
    "print(\"\\nConverting QAT model to quantized model...\")\n",
    "qat_model.eval()\n",
    "quantized_model = convert(qat_model, inplace=True)\n",
    "print(\"Model converted to fully quantized (INT8) ResNet18.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a2443-9dfe-4a05-8bbb-f77082c8ffe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f2a5d-bab7-434d-92cf-9c986985cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Quantized ResNet18 Model Evaluation ---\")\n",
    "qat_accuracy = eval_mode(quantized_model, test_loader, device, model_name=\"Quantized ResNet18 (QAT)\")\n",
    "torch.save(quantized_model.state_dict(), save_path)\n",
    "print(f\"Quantized ResNet18 model saved to {save_path}\")\n",
    "\n",
    "fp32_model_size = os.path.getsize(fp32_model_path)\n",
    "quantized_model_size = os.path.getsize(save_path)\n",
    "print(f\"\\n--- Model Size Comparison ---\")\n",
    "print(f\"FP32 ResNet18 Model Size: {fp32_model_size / (1024*1024):.2f} MB\")\n",
    "print(f\"Quantized ResNet18 Model Size: {quantized_model_size / (1024*1024):.2f} MB\")\n",
    "print(f\"Quantized model is approximately {fp32_model_size / quantized_model_size:.2f}x smaller than FP32 model.\")\n",
    "\n",
    "print(f\"\\n--- Final Summary ---\")\n",
    "print(f\"FP32 ResNet18 Model Accuracy: {fp32_accuracy:.2f}%\")\n",
    "print(f\"Quantized ResNet18 Model Accuracy (QAT): {qat_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f023c827-1512-4f93-8414-f1d7758c6a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e655b2-dcaf-44f2-bbec-c90c576dc145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "431be95b-2f93-4686-ae02-5f01e50f4ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda mode\n",
      "Number of training batches (subset): 79\n",
      "Number of test batches (subset): 16\n",
      "Model definition and QAT preparation functions defined.\n",
      "Training and evaluation functions defined.\n",
      "\n",
      "--- FP32 ResNet18 Model Training and Evaluation ---\n",
      "\n",
      "--- FP32 ResNet18 Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d63e29c8df48e0bbc31cd700759f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 (FP32 ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Complete: Avg Loss: 1.3737, Accuracy: 52.54%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4733a018cc584970abc27bfc20d560a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 (FP32 ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Complete: Avg Loss: 0.8948, Accuracy: 68.82%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4000f941b74a278e2a3d5ab2cca967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 (FP32 ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Complete: Avg Loss: 0.7637, Accuracy: 73.66%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6666a320b07a417da26bf38a8dc18015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 (FP32 ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Complete: Avg Loss: 0.6551, Accuracy: 78.34%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d230969263014d48b7b6474cfb63c2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 (FP32 ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Complete: Avg Loss: 0.6062, Accuracy: 79.24%\n",
      "FP32 ResNet18 Training finished!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1119ae32d0964b4ca5a9ebc3ce673696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating FP32 ResNet18:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set (FP32 ResNet18): 87.10%\n",
      "FP32 ResNet18 model saved to ./qat_resnet18_model_fp32.pth\n",
      "\n",
      "--- Quantization-Aware Training (QAT) for ResNet18 ---\n",
      "QAT model initialized with FP32 model weights.\n",
      "Preparing ResNet18 for QAT (fusing modules)...\n",
      "ResNet18 QAT preparation complete.\n",
      "Setting QConfig (fbgemm)...\n",
      "QConfig set to: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n",
      "Calling torch.quantization.prepare_qat...\n",
      "Model prepared for QAT.\n",
      "\n",
      "Starting QAT for 5 epochs...\n",
      "\n",
      "--- QAT ResNet18 Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668c9aafe8954353bf9079e93720d49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5 (QAT ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Complete: Avg Loss: 163802.1837, Accuracy: 16.62%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4f9abaf5f14c45bc793e3fe06d8364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5 (QAT ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Complete: Avg Loss: 6452.2827, Accuracy: 25.76%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af08af8aae3a4e8b974f87c46d4bacc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5 (QAT ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Complete: Avg Loss: 5137.4137, Accuracy: 30.04%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71dcb36e00543868ff99e274b0baa4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5 (QAT ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Complete: Avg Loss: 4370.6192, Accuracy: 31.96%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb1a008791944e2afa3b78c62a598a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5 (QAT ResNet18):   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Complete: Avg Loss: 3580.3217, Accuracy: 34.08%\n",
      "QAT ResNet18 Training finished!\n",
      "\n",
      "Converting QAT model to quantized model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported qscheme: per_channel_affine",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[260], line 197\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConverting QAT model to quantized model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m qat_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m--> 197\u001b[0m quantized_model \u001b[38;5;241m=\u001b[39m convert(qat_model, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel converted to fully quantized (INT8) ResNet18.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;66;03m# --- 7. 양자화된 모델 평가 및 크기 비교 ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:655\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[0;32m    654\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[1;32m--> 655\u001b[0m _convert(\n\u001b[0;32m    656\u001b[0m     module,\n\u001b[0;32m    657\u001b[0m     mapping,\n\u001b[0;32m    658\u001b[0m     inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    659\u001b[0m     is_reference\u001b[38;5;241m=\u001b[39mis_reference,\n\u001b[0;32m    660\u001b[0m     convert_custom_config_dict\u001b[38;5;241m=\u001b[39mconvert_custom_config_dict,\n\u001b[0;32m    661\u001b[0m     use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant,\n\u001b[0;32m    662\u001b[0m )\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[0;32m    664\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:720\u001b[0m, in \u001b[0;36m_convert\u001b[1;34m(module, mapping, inplace, is_reference, convert_custom_config_dict, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule)\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[0;32m    711\u001b[0m     ):\n\u001b[0;32m    712\u001b[0m         _convert(\n\u001b[0;32m    713\u001b[0m             mod,\n\u001b[0;32m    714\u001b[0m             mapping,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    718\u001b[0m             use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant,\n\u001b[0;32m    719\u001b[0m         )\n\u001b[1;32m--> 720\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(\n\u001b[0;32m    721\u001b[0m         mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant\n\u001b[0;32m    722\u001b[0m     )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    725\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\quantize.py:762\u001b[0m, in \u001b[0;36mswap_module\u001b[1;34m(mod, mapping, custom_module_class_mapping, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    760\u001b[0m sig \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(qmod\u001b[38;5;241m.\u001b[39mfrom_float)\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_precomputed_fake_quant\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[1;32m--> 762\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(\n\u001b[0;32m    763\u001b[0m         mod, use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant\n\u001b[0;32m    764\u001b[0m     )\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:172\u001b[0m, in \u001b[0;36mConvReLU2d.from_float\u001b[1;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mod\u001b[38;5;241m.\u001b[39mbn\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mod\u001b[38;5;241m.\u001b[39mbn\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     mod\u001b[38;5;241m.\u001b[39mweight, mod\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m fuse_conv_bn_weights(\n\u001b[0;32m    164\u001b[0m         mod\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    165\u001b[0m         mod\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         mod\u001b[38;5;241m.\u001b[39mbn\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfrom_float(\n\u001b[0;32m    173\u001b[0m     mod, use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant\n\u001b[0;32m    174\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:606\u001b[0m, in \u001b[0;36mConv2d.from_float\u001b[1;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_float\u001b[39m(\u001b[38;5;28mcls\u001b[39m, mod, use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    600\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a quantized module from a float module or qparams_dict.\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;124;03m        mod (Module): a float module, either produced by torch.ao.quantization\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m          utilities or provided by the user\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ConvNd\u001b[38;5;241m.\u001b[39mfrom_float(\n\u001b[0;32m    607\u001b[0m         \u001b[38;5;28mcls\u001b[39m, mod, use_precomputed_fake_quant\u001b[38;5;241m=\u001b[39muse_precomputed_fake_quant\n\u001b[0;32m    608\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:322\u001b[0m, in \u001b[0;36m_ConvNd.from_float\u001b[1;34m(cls, mod, use_precomputed_fake_quant)\u001b[0m\n\u001b[0;32m    320\u001b[0m         mod \u001b[38;5;241m=\u001b[39m mod[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    321\u001b[0m     weight_post_process \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39mqconfig\u001b[38;5;241m.\u001b[39mweight()\n\u001b[1;32m--> 322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_qconv(mod, activation_post_process, weight_post_process)\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:266\u001b[0m, in \u001b[0;36m_ConvNd.get_qconv\u001b[1;34m(cls, mod, activation_post_process, weight_post_process)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;66;03m# the __init__ call used is the one from derived classes and not the one from _ConvNd\u001b[39;00m\n\u001b[0;32m    255\u001b[0m qconv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m    256\u001b[0m     mod\u001b[38;5;241m.\u001b[39min_channels,\n\u001b[0;32m    257\u001b[0m     mod\u001b[38;5;241m.\u001b[39mout_channels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m     mod\u001b[38;5;241m.\u001b[39mpadding_mode,\n\u001b[0;32m    265\u001b[0m )\n\u001b[1;32m--> 266\u001b[0m qconv\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, mod\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    268\u001b[0m     activation_post_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m activation_post_process\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat\n\u001b[0;32m    270\u001b[0m ):\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qconv  \u001b[38;5;66;03m# dynamic quantization doesn't need scale/zero_point\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py:567\u001b[0m, in \u001b[0;36mConv2d.set_weight_bias\u001b[1;34m(self, w, b)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, w: torch\u001b[38;5;241m.\u001b[39mTensor, b: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 567\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mconv2d_prepack(\n\u001b[0;32m    568\u001b[0m             w, b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    569\u001b[0m         )\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    571\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mconv2d_prepack(\n\u001b[0;32m    572\u001b[0m             w, b, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    573\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\_ops.py:1158\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported qscheme: per_channel_affine"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "# default_per_channel_weight_observer는 더 이상 직접 사용하지 않으므로 제거하거나 주석 처리합니다.\n",
    "# from torch.ao.quantization import get_default_qconfig, QuantStub, DeQuantStub, fuse_modules, prepare_qat, convert, default_per_channel_weight_observer \n",
    "from torch.ao.quantization import get_default_qconfig, QuantStub, DeQuantStub, fuse_modules, prepare_qat, convert\n",
    "from torch.ao.quantization import observer # observer 모듈을 직접 임포트합니다.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader \n",
    "import functools # QConfig 설정에 필수적입니다.\n",
    "\n",
    "# --- 1. 하이퍼파라미터 및 장치 설정 ---\n",
    "batch_size = 64\n",
    "lr = 0.0001\n",
    "num_epoch = 5 # FP32 모델 학습 에폭\n",
    "qat_epoch = 5 # QAT 학습 에폭\n",
    "save_path = \"./qat_resnet18_model.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device} mode\")\n",
    "\n",
    "# --- 2. 데이터 로드 및 전처리 ---\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224), # ResNet18은 224x224 입력 기대\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet 평균/분산 사용\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(224), # 224x224로 리사이즈\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "subset_size_train = 5000\n",
    "subset_size_test = 1000\n",
    "\n",
    "train_subset_indices = torch.randperm(len(train_data))[:subset_size_train]\n",
    "test_subset_indices = torch.randperm(len(test_data))[:subset_size_test]\n",
    "\n",
    "train_subset = torch.utils.data.Subset(train_data, train_subset_indices)\n",
    "test_subset = torch.utils.data.Subset(test_data, test_subset_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Number of training batches (subset): {len(train_loader)}\")\n",
    "print(f\"Number of test batches (subset): {len(test_loader)}\")\n",
    "\n",
    "# --- 3. 모델 정의 및 QAT 수정 함수 ---\n",
    "def get_res(num_classes=10, pretrained=True): # #### 변경 사항 1: num_classes 기본값 변경 (CIFAR10용)\n",
    "    \"\"\"\n",
    "    ResNet18 모델을 로드하고, 마지막 Fully Connected Layer를\n",
    "    지정된 num_classes에 맞게 수정합니다.\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes) # CIFAR10 클래스 수에 맞게 final layer 조정\n",
    "    return model\n",
    "\n",
    "def prepare_model_for_qat(model): # #### 변경 사항 2: 함수명 변경 (pretrain_Model -> prepare_model_for_qat)\n",
    "    \"\"\"\n",
    "    QAT를 위해 모델에 QuantStub/DeQuantStub를 삽입하고,\n",
    "    ResNet18의 특정 모듈들을 퓨징합니다.\n",
    "    \"\"\"\n",
    "    model.quant = QuantStub()\n",
    "    model.dequant = DeQuantStub() # #### 변경 사항 3: DeQuantStub()으로 인스턴스화 필요\n",
    "\n",
    "    # ResNet18의 기본 블록 구조에 맞춰 모듈 퓨징\n",
    "    # (conv1, bn1, relu)와 각 BasicBlock 내부의 (conv, bn, relu)를 퓨징\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Sequential): # Layer blocks (e.g., layer1, layer2, ...)\n",
    "            for basic_block_name, basic_block_module in module.named_children():\n",
    "                if isinstance(basic_block_module, models.resnet.BasicBlock):\n",
    "                    # BasicBlock의 첫 번째 Conv-BN-ReLU 시퀀스\n",
    "                    torch.quantization.fuse_modules(basic_block_module, [['conv1', 'bn1', 'relu']], inplace=True)\n",
    "                    # BasicBlock의 두 번째 Conv-BN (ReLU는 여기에 붙지 않음)\n",
    "                    torch.quantization.fuse_modules(basic_block_module, [['conv2', 'bn2']], inplace=True)\n",
    "    # 모델의 첫 번째 Conv-BN-ReLU 시퀀스\n",
    "    torch.quantization.fuse_modules(model, [['conv1', 'bn1', 'relu']], inplace=True)\n",
    "    return model\n",
    "\n",
    "print(\"Model definition and QAT preparation functions defined.\")\n",
    "\n",
    "# --- 4. 학습 및 평가 함수 ---\n",
    "def train_mode(model, train_loader, criter, optim, num_epoch, device, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    모델을 학습시키는 일반적인 학습 루프입니다.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    print(f\"\\n--- {model_name} Training ---\")\n",
    "    for epoch in range(num_epoch):\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epoch} ({model_name})\")):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optim.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criter(out, target)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(out.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        acc = 100 * correct_predictions / total_predictions\n",
    "        print(f\"Epoch {epoch+1} Complete: Avg Loss: {avg_loss:.4f}, Accuracy: {acc:.2f}%\")\n",
    "    print(f\"{model_name} Training finished!\")\n",
    "\n",
    "\n",
    "def eval_mode(model, test_loader, device, model_name='Model'):\n",
    "    \"\"\"\n",
    "    모델의 정확도를 평가하는 함수입니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=f\"Evaluating {model_name}\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            out = model(data)\n",
    "            _, pred = torch.max(out.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (pred == target).sum().item()\n",
    "        acc = 100 * correct / total\n",
    "        print(f'Accuracy on test set ({model_name}): {acc:.2f}%')\n",
    "    return acc\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")\n",
    "\n",
    "# --- 5. FP32 (원본) ResNet18 학습 및 평가 ---\n",
    "print(\"\\n--- FP32 ResNet18 Model Training and Evaluation ---\")\n",
    "fp32_model = get_res(num_classes=10, pretrained=True).to(device) # CIFAR10 클래스 10개\n",
    "criter = nn.CrossEntropyLoss()\n",
    "optimy = optim.Adam(fp32_model.parameters(), lr=lr)\n",
    "\n",
    "train_mode(fp32_model, train_loader, criter, optimy, num_epoch, device, model_name=\"FP32 ResNet18\")\n",
    "fp32_accuracy = eval_mode(fp32_model, test_loader, device, model_name=\"FP32 ResNet18\")\n",
    "\n",
    "fp32_model_path = save_path.replace(\".pth\", \"_fp32.pth\")\n",
    "torch.save(fp32_model.state_dict(), fp32_model_path)\n",
    "print(f\"FP32 ResNet18 model saved to {fp32_model_path}\")\n",
    "\n",
    "# --- 6. Quantization-Aware Training (QAT) for ResNet18 ---\n",
    "print(\"\\n--- Quantization-Aware Training (QAT) for ResNet18 ---\")\n",
    "qat_model = get_res(num_classes=10, pretrained=True).to(device)\n",
    "\n",
    "qat_model.load_state_dict(fp32_model.state_dict())\n",
    "print(\"QAT model initialized with FP32 model weights.\")\n",
    "\n",
    "print(\"Preparing ResNet18 for QAT (fusing modules)...\")\n",
    "qat_model.eval() \n",
    "qat_model = prepare_model_for_qat(qat_model) \n",
    "print(\"ResNet18 QAT preparation complete.\")\n",
    "\n",
    "print(\"Setting QConfig (fbgemm)...\")\n",
    "# #### 최종 변경 사항 11: QConfig 설정 방식 변경 (AttributeError 및 Unsupported qscheme 재시도)\n",
    "# `get_default_qconfig('fbgemm')`는 기본적으로 activation=HistogramObserver, weight=PerChannelMinMaxObserver를 사용합니다.\n",
    "# 하지만 qscheme 이슈가 지속되므로, qscheme을 명시적으로 symmetric으로 설정하고,\n",
    "# 활성화와 가중치 옵저버를 직접 functools.partial로 구성합니다.\n",
    "# HistogramObserver for activation usually uses reduce_range=True.\n",
    "# PerChannelMinMaxObserver for weights needs qscheme=torch.per_channel_symmetric for fbgemm.\n",
    "qat_model.qconfig = torch.ao.quantization.QConfig(\n",
    "    activation=functools.partial(observer.HistogramObserver, reduce_range=True),\n",
    "    weight=functools.partial(observer.PerChannelMinMaxObserver, dtype=torch.qint8, qscheme=torch.per_channel_symmetric)\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"QConfig set to: {qat_model.qconfig}\")\n",
    "\n",
    "print(\"Calling torch.quantization.prepare_qat...\")\n",
    "qat_model.train() \n",
    "prepare_qat(qat_model, inplace=True)\n",
    "print(\"Model prepared for QAT.\")\n",
    "\n",
    "criter_qt = nn.CrossEntropyLoss()\n",
    "optimy_qt = optim.Adam(qat_model.parameters(), lr=lr * 0.1) \n",
    "print(f\"\\nStarting QAT for {qat_epoch} epochs...\")\n",
    "train_mode(qat_model,train_loader, criter_qt, optimy_qt, qat_epoch, device, model_name=\"QAT ResNet18\")\n",
    "\n",
    "print(\"\\nConverting QAT model to quantized model...\")\n",
    "qat_model.eval()\n",
    "quantized_model = convert(qat_model, inplace=True)\n",
    "print(\"Model converted to fully quantized (INT8) ResNet18.\")\n",
    "\n",
    "# --- 7. 양자화된 모델 평가 및 크기 비교 ---\n",
    "print(\"\\n--- Quantized ResNet18 Model Evaluation ---\")\n",
    "qat_accuracy = eval_mode(quantized_model, test_loader, device, model_name=\"Quantized ResNet18 (QAT)\")\n",
    "torch.save(quantized_model.state_dict(), save_path)\n",
    "print(f\"Quantized ResNet18 model saved to {save_path}\")\n",
    "\n",
    "fp32_model_size = os.path.getsize(fp32_model_path)\n",
    "quantized_model_size = os.path.getsize(save_path)\n",
    "print(f\"\\n--- Model Size Comparison ---\")\n",
    "print(f\"FP32 ResNet18 Model Size: {fp32_model_size / (1024*1024):.2f} MB\")\n",
    "print(f\"Quantized ResNet18 Model Size: {quantized_model_size / (1024*1024):.2f} MB\")\n",
    "print(f\"Quantized model is approximately {fp32_model_size / quantized_model_size:.2f}x smaller than FP32 model.\")\n",
    "\n",
    "print(f\"\\n--- Final Summary ---\")\n",
    "print(f\"FP32 ResNet18 Model Accuracy: {fp32_accuracy:.2f}%\")\n",
    "print(f\"Quantized ResNet18 Model Accuracy (QAT): {qat_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "647f2826-45a5-4972-a068-28a5d939b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.7.0+cu126\n",
      "torchvision version: 0.22.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2eef8896-3daf-4f5b-868c-7e3e83b506cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting QConfig (fbgemm)...\n"
     ]
    }
   ],
   "source": [
    "#3. Model Definition and Modification for QAT\n",
    "# Change the QConfig setting again\n",
    "print(\"Setting QConfig (fbgemm)...\")\n",
    "qat_model.qconfig = torch.ao.quantization.QConfig(\n",
    "    # Try MovingAverageMinMaxObserver for activation\n",
    "    activation=functools.partial(observer.MovingAverageMinMaxObserver, reduce_range=True),\n",
    "    # Keep PerChannelMinMaxObserver for weight with symmetric qscheme\n",
    "    weight=functools.partial(observer.PerChannelMinMaxObserver, dtype=torch.qint8, qscheme=torch.per_channel_symmetric)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7ef7e6b9-8786-4d1d-b721-4eecc4a2cfa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train batches: 79, Test batches: 16\n",
      "Model definitions ready.\n",
      "\n",
      "=== FP32 ResNet18 ===\n",
      "\n",
      "--- FP32 Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0ee449e2734addab06307f50f6bca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 1: Loss=1.3870, Acc=51.20%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce05ed72f9f640bea82113c70cde8d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 2: Loss=0.8966, Acc=69.34%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b189f9a3887040a4a610a07cd4748188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 3: Loss=0.7603, Acc=74.18%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32da43df64ce433fb7d6db8ba1ae4711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 4: Loss=0.6360, Acc=78.30%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64952ef95a24a9e91096a1745c0a187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 5: Loss=0.6119, Acc=79.16%\n",
      "FP32 training done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612742c7402d49beb59f10ee89c6419c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating FP32:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Test Accuracy: 87.70%\n",
      "Saved FP32 model → ./qat_resnet18_model_fp32.pth\n",
      "\n",
      "=== QAT ResNet18 ===\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Fusion only for eval!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[264], line 168\u001b[0m\n\u001b[0;32m    165\u001b[0m qat_fp32\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(fp32_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    167\u001b[0m \u001b[38;5;66;03m# 2) QuantWrapper 생성 (fuse 포함)\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m qat_model \u001b[38;5;241m=\u001b[39m QuantizedResNet18(qat_fp32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFused and wrapped for QAT.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# 3) QConfig 설정\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[264], line 88\u001b[0m, in \u001b[0;36mQuantizedResNet18.__init__\u001b[1;34m(self, fp32_model)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant \u001b[38;5;241m=\u001b[39m DeQuantStub()\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel   \u001b[38;5;241m=\u001b[39m fp32_model\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fuse_modules()\n",
      "Cell \u001b[1;32mIn[264], line 92\u001b[0m, in \u001b[0;36mQuantizedResNet18._fuse_modules\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fuse_modules\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# 전체 모델 fuse\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     quant\u001b[38;5;241m.\u001b[39mfuse_modules(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbn1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m]], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# BasicBlock fuse (conv1+bn1+relu, conv2+bn2) 및 downsample fuse\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnamed_children():\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\fuse_modules.py:191\u001b[0m, in \u001b[0;36mfuse_modules\u001b[1;34m(model, modules_to_fuse, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfuse_modules\u001b[39m(\n\u001b[0;32m    131\u001b[0m     model,\n\u001b[0;32m    132\u001b[0m     modules_to_fuse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m     fuse_custom_config_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    136\u001b[0m ):\n\u001b[0;32m    137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Fuse a list of modules into a single module.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    Fuses only the following sequence of modules:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _fuse_modules(\n\u001b[0;32m    192\u001b[0m         model,\n\u001b[0;32m    193\u001b[0m         modules_to_fuse,\n\u001b[0;32m    194\u001b[0m         is_qat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    195\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m    196\u001b[0m         fuser_func\u001b[38;5;241m=\u001b[39mfuser_func,\n\u001b[0;32m    197\u001b[0m         fuse_custom_config_dict\u001b[38;5;241m=\u001b[39mfuse_custom_config_dict,\n\u001b[0;32m    198\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\fuse_modules.py:124\u001b[0m, in \u001b[0;36m_fuse_modules\u001b[1;34m(model, modules_to_fuse, is_qat, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# Handle case of modules_to_fuse being a list of lists\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module_list \u001b[38;5;129;01min\u001b[39;00m modules_to_fuse:\n\u001b[1;32m--> 124\u001b[0m         _fuse_modules_helper(\n\u001b[0;32m    125\u001b[0m             model, module_list, is_qat, fuser_func, fuse_custom_config_dict\n\u001b[0;32m    126\u001b[0m         )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\fuse_modules.py:98\u001b[0m, in \u001b[0;36m_fuse_modules_helper\u001b[1;34m(model, modules_to_fuse, is_qat, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[0;32m     95\u001b[0m mod_list \u001b[38;5;241m=\u001b[39m [_get_module(model, item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m modules_to_fuse]\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Fuse list of modules\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m new_mod_list \u001b[38;5;241m=\u001b[39m fuser_func(mod_list, is_qat, additional_fuser_method_mapping)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Replace original module list with fused module list\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules_to_fuse):\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\fuse_modules.py:63\u001b[0m, in \u001b[0;36mfuse_known_modules\u001b[1;34m(mod_list, is_qat, additional_fuser_method_mapping)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot fuse modules: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m new_mod: \u001b[38;5;28mlist\u001b[39m[Optional[nn\u001b[38;5;241m.\u001b[39mModule]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(mod_list)\n\u001b[1;32m---> 63\u001b[0m fused \u001b[38;5;241m=\u001b[39m fuser_method(is_qat, \u001b[38;5;241m*\u001b[39mmod_list)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# NOTE: forward hooks not processed in the two following for loops will be lost after the fusion\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Move pre forward hooks of the base module to resulting fused module\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pre_hook_fn \u001b[38;5;129;01min\u001b[39;00m mod_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\fuser_method_mappings.py:113\u001b[0m, in \u001b[0;36mfuse_conv_bn_relu\u001b[1;34m(is_qat, conv, bn, relu)\u001b[0m\n\u001b[0;32m    111\u001b[0m fused_module \u001b[38;5;241m=\u001b[39m map_to_fused_module_eval\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mtype\u001b[39m(conv), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     fused_conv \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mfusion\u001b[38;5;241m.\u001b[39mfuse_conv_bn_eval(conv, bn)\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fused_module(fused_conv, relu)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\torch\\nn\\utils\\fusion.py:38\u001b[0m, in \u001b[0;36mfuse_conv_bn_eval\u001b[1;34m(conv, bn, transpose)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfuse_conv_bn_eval\u001b[39m(\n\u001b[0;32m     21\u001b[0m     conv: ConvT,\n\u001b[0;32m     22\u001b[0m     bn: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mbatchnorm\u001b[38;5;241m.\u001b[39m_BatchNorm,\n\u001b[0;32m     23\u001b[0m     transpose: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConvT:\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Fuse a convolutional module and a BatchNorm module into a single, new convolutional module.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m        Both ``conv`` and ``bn`` must be in eval mode, and ``bn`` must have its running buffers computed.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (conv\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m bn\u001b[38;5;241m.\u001b[39mtraining), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFusion only for eval!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m     fused_conv \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(conv)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m bn\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m bn\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Fusion only for eval!"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.ao.quantization as quant\n",
    "from torch.ao.quantization import (\n",
    "    get_default_qconfig,\n",
    "    QuantStub,\n",
    "    DeQuantStub,\n",
    "    prepare_qat,\n",
    "    convert,\n",
    "    observer\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. 하이퍼파라미터 및 장치 설정 ---\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "num_epoch = 5      # FP32 학습 에폭\n",
    "qat_epoch = 5      # QAT 학습 에폭\n",
    "save_path = \"./qat_resnet18_model.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# quantization 백엔드 설정\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "# --- 2. 데이터 로드 및 전처리 ---\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std= [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std= [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_data  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# subset 사용 (연습용)\n",
    "subset_size_train = 5000\n",
    "subset_size_test  = 1000\n",
    "train_subset = torch.utils.data.Subset(train_data, torch.randperm(len(train_data))[:subset_size_train])\n",
    "test_subset  = torch.utils.data.Subset(test_data,  torch.randperm(len(test_data))[:subset_size_test])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
    "test_loader  = DataLoader(test_subset,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "\n",
    "# --- 3. 모델 정의 및 QAT 준비 ---\n",
    "def get_resnet18(num_classes=10, pretrained=True):\n",
    "    \"\"\"ResNet18 로드 & 마지막 FC 레이어 클래스 수에 맞춰 교체\"\"\"\n",
    "    model = models.resnet18(\n",
    "        weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    )\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    \"\"\"QuantStub/DeQuantStub을 래핑하고, fuse까지 포함\"\"\"\n",
    "    def __init__(self, fp32_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.quant   = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        self.model   = fp32_model\n",
    "        self._fuse_modules()\n",
    "\n",
    "    def _fuse_modules(self):\n",
    "        # 전체 모델 fuse\n",
    "        quant.fuse_modules(self.model, [['conv1', 'bn1', 'relu']], inplace=True)\n",
    "\n",
    "        # BasicBlock fuse (conv1+bn1+relu, conv2+bn2) 및 downsample fuse\n",
    "        for _, layer in self.model.named_children():\n",
    "            if isinstance(layer, nn.Sequential):\n",
    "                for block in layer:\n",
    "                    if isinstance(block, models.resnet.BasicBlock):\n",
    "                        quant.fuse_modules(block, [['conv1', 'bn1', 'relu']], inplace=True)\n",
    "                        quant.fuse_modules(block, [['conv2', 'bn2']], inplace=True)\n",
    "                        if block.downsample is not None:\n",
    "                            quant.fuse_modules(block.downsample, ['0', '1'], inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "print(\"Model definitions ready.\")\n",
    "\n",
    "# --- 4. 학습/평가 루프 정의 ---\n",
    "def train_mode(model, loader, criterion, optimizer, epochs, device, name=\"Model\"):\n",
    "    model.train()\n",
    "    print(f\"\\n--- {name} Training ---\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for data, target in pbar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total   += target.size(0)\n",
    "        print(f\"[{name}] Epoch {epoch+1}: Loss={total_loss/len(loader):.4f}, Acc={100*correct/total:.2f}%\")\n",
    "    print(f\"{name} training done.\")\n",
    "\n",
    "def eval_mode(model, loader, device, name=\"Model\"):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(loader, desc=f\"Evaluating {name}\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total   += target.size(0)\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"[{name}] Test Accuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "# --- 5. FP32 모델 학습 및 평가 ---\n",
    "print(\"\\n=== FP32 ResNet18 ===\")\n",
    "fp32 = get_resnet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_fp32 = optim.Adam(fp32.parameters(), lr=lr)\n",
    "\n",
    "train_mode(fp32, train_loader, criterion, optimizer_fp32, num_epoch, device, name=\"FP32\")\n",
    "fp32_acc = eval_mode(fp32, test_loader, device, name=\"FP32\")\n",
    "fp32_path = save_path.replace(\".pth\", \"_fp32.pth\")\n",
    "torch.save(fp32.state_dict(), fp32_path)\n",
    "print(f\"Saved FP32 model → {fp32_path}\")\n",
    "\n",
    "# --- 6. QAT 준비 및 학습 ---\n",
    "print(\"\\n=== QAT ResNet18 ===\")\n",
    "# 1) FP32 모델 weight 로드\n",
    "qat_fp32 = get_resnet18().to('cpu')\n",
    "qat_fp32.load_state_dict(torch.load(fp32_path, map_location='cpu'))\n",
    "\n",
    "# 2) QuantWrapper 생성 (fuse 포함)\n",
    "qat_model = QuantizedResNet18(qat_fp32).to('cpu')\n",
    "print(\"Fused and wrapped for QAT.\")\n",
    "\n",
    "# 3) QConfig 설정\n",
    "qat_model.qconfig = torch.ao.quantization.QConfig(\n",
    "    activation=functools.partial(observer.HistogramObserver, reduce_range=True),\n",
    "    weight=functools.partial(\n",
    "        observer.PerChannelMinMaxObserver,\n",
    "        dtype=torch.qint8,\n",
    "        qscheme=torch.per_channel_symmetric\n",
    "    )\n",
    ")\n",
    "print(\"QConfig:\", qat_model.qconfig)\n",
    "\n",
    "# 4) Prepare QAT\n",
    "prepare_qat(qat_model, inplace=True)\n",
    "print(\"Prepared for QAT (fake-quant enabled).\")\n",
    "\n",
    "# 5) QAT 학습 (GPU에서 가능)\n",
    "qat_model.to(device)\n",
    "optimizer_qat = optim.Adam(qat_model.parameters(), lr=lr * 0.1)\n",
    "train_mode(qat_model, train_loader, criterion, optimizer_qat, qat_epoch, device, name=\"QAT\")\n",
    "\n",
    "# --- 7. 최종 양자화 및 평가 (CPU 전용) ---\n",
    "print(\"\\n--- Converting to INT8 ---\")\n",
    "qat_model.to('cpu')\n",
    "quantized = convert(qat_model, inplace=True)\n",
    "print(\"Converted to INT8.\")\n",
    "\n",
    "quantized_acc = eval_mode(quantized, test_loader, torch.device('cpu'), name=\"Quantized\")\n",
    "\n",
    "# 8. 모델 저장 및 크기 비교\n",
    "torch.save(quantized.state_dict(), save_path)\n",
    "print(f\"Saved quantized model → {save_path}\")\n",
    "\n",
    "fp32_size = os.path.getsize(fp32_path) / (1024**2)\n",
    "quant_size = os.path.getsize(save_path) / (1024**2)\n",
    "print(f\"FP32 size: {fp32_size:.2f} MB\")\n",
    "print(f\"Quant size: {quant_size:.2f} MB\")\n",
    "print(f\"Size reduction: {fp32_size/quant_size:.2f}×\")\n",
    "print(f\"FP32 Acc: {fp32_acc:.2f}%, Quant Acc: {quantized_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "badf1508-d07c-41a2-89ec-3f34bf54da9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train batches: 79, Test batches: 16\n",
      "\n",
      "=== FP32 ResNet18 ===\n",
      "\n",
      "--- FP32 Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb71635befa54951b21b17fe25fceaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Epoch 1/5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 1: Loss=1.4009, Acc=50.76%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9695575b6c4037916bbcffd1e3d1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Epoch 2/5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 2: Loss=0.8908, Acc=69.28%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea158e0c8814a89bbb33cb422dc3e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Epoch 3/5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 3: Loss=0.7633, Acc=73.28%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db337760ef2040e1b1f8328e3e679d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Epoch 4/5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 4: Loss=0.6708, Acc=77.16%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b3652b3f26442d97f44dd250d47656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Epoch 5/5:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 5: Loss=0.6140, Acc=78.90%\n",
      "FP32 training done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fcf28b634a44f990d32cfd35cd3cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating FP32:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Test Accuracy: 87.00%\n",
      "Saved FP32 model → ./qat_resnet18_model_fp32.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import os\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.ao.quantization as quant\n",
    "from torch.ao.quantization import (\n",
    "    QuantStub,\n",
    "    DeQuantStub,\n",
    "    prepare_qat,\n",
    "    convert,\n",
    "    observer\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. 하이퍼파라미터 및 장치 설정 ---\n",
    "batch_size = 64\n",
    "lr = 1e-4\n",
    "num_epoch = 5      # FP32 학습 에폭\n",
    "qat_epoch = 5      # QAT 학습 에폭\n",
    "save_path = \"./qat_resnet18_model.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# quantization 백엔드 설정\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "# --- 2. 데이터 로드 및 전처리 ---\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std= [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std= [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_data  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "subset_size_train = 5000\n",
    "subset_size_test  = 1000\n",
    "train_subset = torch.utils.data.Subset(train_data, torch.randperm(len(train_data))[:subset_size_train])\n",
    "test_subset  = torch.utils.data.Subset(test_data,  torch.randperm(len(test_data))[:subset_size_test])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
    "test_loader  = DataLoader(test_subset,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "\n",
    "# --- 3. 모델 정의 및 QAT 준비 함수들 ---\n",
    "def get_resnet18(num_classes=10, pretrained=True):\n",
    "    \"\"\"ResNet18 로드 & 마지막 FC 레이어를 num_classes에 맞춰 교체\"\"\"\n",
    "    # PyTorch 2.x API\n",
    "    weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    model = models.resnet18(weights=weights)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    \"\"\"QuantStub/DeQuantStub 래퍼 + fuse 처리\"\"\"\n",
    "    def __init__(self, fp32_model: nn.Module):\n",
    "        super().__init__()\n",
    "        # 1) 기존 FP32 모델\n",
    "        self.model = fp32_model\n",
    "        # 2) fake-quant stub\n",
    "        self.quant   = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        # 3) fuse를 위해 eval 모드로 전환 → fuse → train 모드 복귀\n",
    "        self.model.eval()\n",
    "        self._fuse_modules()\n",
    "        self.model.train()\n",
    "\n",
    "    def _fuse_modules(self):\n",
    "        # 최상위 conv1, bn1, relu\n",
    "        quant.fuse_modules(self.model, [['conv1', 'bn1', 'relu']], inplace=True)\n",
    "\n",
    "        # BasicBlock 내부 fuse (conv1+bn1+relu, conv2+bn2) 및 downsample\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, models.resnet.BasicBlock):\n",
    "                quant.fuse_modules(module, [['conv1', 'bn1', 'relu']], inplace=True)\n",
    "                quant.fuse_modules(module, [['conv2', 'bn2']], inplace=True)\n",
    "                if module.downsample is not None:\n",
    "                    # downsample: [Conv, BN]\n",
    "                    quant.fuse_modules(module.downsample, ['0', '1'], inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# --- 4. 학습/평가 루프 정의 ---\n",
    "def train_mode(model, loader, criterion, optimizer, epochs, device, name=\"Model\"):\n",
    "    model.train()\n",
    "    print(f\"\\n--- {name} Training ---\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        for data, target in tqdm(loader, desc=f\"{name} Epoch {epoch+1}/{epochs}\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total   += target.size(0)\n",
    "\n",
    "        print(f\"[{name}] Epoch {epoch+1}: \"\n",
    "              f\"Loss={total_loss/len(loader):.4f}, \"\n",
    "              f\"Acc={100*correct/total:.2f}%\")\n",
    "    print(f\"{name} training done.\")\n",
    "\n",
    "def eval_mode(model, loader, device, name=\"Model\"):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(loader, desc=f\"Evaluating {name}\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total   += target.size(0)\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"[{name}] Test Accuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "# --- 5. FP32 ResNet18 학습 및 평가 ---\n",
    "print(\"\\n=== FP32 ResNet18 ===\")\n",
    "fp32_model = get_resnet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_fp32 = optim.Adam(fp32_model.parameters(), lr=lr)\n",
    "\n",
    "train_mode(fp32_model, train_loader, criterion, optimizer_fp32, num_epoch, device, name=\"FP32\")\n",
    "fp32_acc = eval_mode(fp32_model, test_loader, device, name=\"FP32\")\n",
    "\n",
    "fp32_path = save_path.replace(\".pth\", \"_fp32.pth\")\n",
    "torch.save(fp32_model.state_dict(), fp32_path)\n",
    "print(f\"Saved FP32 model → {fp32_path}\")\n",
    "\n",
    "# --- 6. QAT 준비 및 학습 ---\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2eee81-37db-47ff-a6df-89f6adbda6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train batches: 313, Test batches: 63\n",
      "\n",
      "=== FP32 ResNet18 ===\n",
      "\n",
      "--- FP32 Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899dcdda27ae4c029da6e6a8ea2b79c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Epoch 1/2:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 1: Loss=1.2930, Acc=54.96%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8fd72eb2644567914783ba7e1c192b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Epoch 2/2:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Epoch 2: Loss=0.9496, Acc=67.42%\n",
      "FP32 training done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7525b71440444079a7fca0ed9938927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating FP32:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] Test Accuracy: 84.60%\n",
      "Saved FP32 → ./qat_resnet18_model_fp32.pth\n",
      "\n",
      "=== QAT ResNet18 ===\n",
      "Fused+wrapped for QAT.\n",
      "QConfig: QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n",
      "Prepared for QAT.\n",
      "\n",
      "--- QAT Training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68d8dd7aa0f434786007d0ed54e4491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QAT Epoch 1/2:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QAT] Epoch 1: Loss=62055.0435, Acc=24.46%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9f40ae35b444f4b96b007b6114a7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QAT Epoch 2/2:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QAT] Epoch 2: Loss=2771.6464, Acc=34.64%\n",
      "QAT training done.\n",
      "\n",
      "--- Converting to INT8 ---\n",
      "Converted to INT8.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a94a94ba774180b536659dc8942b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Quantized:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 전체 수정된 QAT 파이프라인 코드\n",
    "# 커널 폭파범 QAT 파이프라인 코드\n",
    "import os\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# quantization 관련\n",
    "import torch.ao.quantization as quant\n",
    "from torch.ao.quantization import (\n",
    "    QuantStub,\n",
    "    DeQuantStub,\n",
    "    prepare_qat,\n",
    "    convert,\n",
    "    observer\n",
    ")\n",
    "# *** 수정된 부분: FloatFunctional import 경로 ***\n",
    "from torch.nn.quantized import FloatFunctional\n",
    "\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. 하이퍼파라미터 & 디바이스 설정 ---\n",
    "batch_size  = 16\n",
    "lr          = 1e-4\n",
    "num_epoch   = 2    # FP32 학습 에폭\n",
    "qat_epoch   = 2    # QAT 학습 에폭\n",
    "save_path   = \"./qat_resnet18_model.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# INT8 변환에 사용할 백엔드\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "# --- 2. 데이터 준비 ---\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10('./data', train=True,  download=True, transform=transform_train)\n",
    "test_dataset  = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# 연습용으로 subset\n",
    "train_subset = torch.utils.data.Subset(train_dataset, torch.randperm(len(train_dataset))[:5000])\n",
    "test_subset  = torch.utils.data.Subset(test_dataset,  torch.randperm(len(test_dataset))[:1000])\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
    "test_loader  = DataLoader(test_subset,  batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n",
    "\n",
    "\n",
    "# --- 3. 모델 정의 Helpers ---\n",
    "def get_resnet18(num_classes=10, pretrained=True):\n",
    "    \"\"\"ResNet18 불러와서 마지막 FC 교체\"\"\"\n",
    "    weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    model = models.resnet18(weights=weights)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "class QBasicBlock(nn.Module):\n",
    "    \"\"\"BasicBlock에서 skip connection에 FloatFunctional을 쓰도록 변경\"\"\"\n",
    "    def __init__(self, orig: BasicBlock):\n",
    "        super().__init__()\n",
    "        self.conv1      = orig.conv1\n",
    "        self.bn1        = orig.bn1\n",
    "        self.relu       = orig.relu\n",
    "        self.conv2      = orig.conv2\n",
    "        self.bn2        = orig.bn2\n",
    "        self.downsample = orig.downsample\n",
    "        self.skip_add   = FloatFunctional()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.skip_add.add(out, identity)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    \"\"\"QuantStub, DeQuantStub 래핑 + fuse + BasicBlock 교체\"\"\"\n",
    "    def __init__(self, fp32_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model   = fp32_model\n",
    "        self.quant   = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        self.model.eval()\n",
    "        self._fuse_modules()\n",
    "        self._replace_basic_blocks()\n",
    "        self.model.train()\n",
    "\n",
    "    def _fuse_modules(self):\n",
    "        quant.fuse_modules(self.model, [['conv1','bn1','relu']], inplace=True)\n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, BasicBlock):\n",
    "                quant.fuse_modules(module, [['conv1','bn1','relu']], inplace=True)\n",
    "                quant.fuse_modules(module, [['conv2','bn2']],   inplace=True)\n",
    "                if module.downsample is not None:\n",
    "                    quant.fuse_modules(module.downsample, ['0','1'], inplace=True)\n",
    "\n",
    "    def _replace_basic_blocks(self):\n",
    "        for name, child in list(self.model.named_children()):\n",
    "            if isinstance(child, nn.Sequential):\n",
    "                new_seq = []\n",
    "                for blk in child:\n",
    "                    if isinstance(blk, BasicBlock):\n",
    "                        new_seq.append(QBasicBlock(blk))\n",
    "                    else:\n",
    "                        new_seq.append(blk)\n",
    "                setattr(self.model, name, nn.Sequential(*new_seq))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- 4. 학습/평가 루프 ---\n",
    "def train_mode(model, loader, criterion, optimizer, epochs, device, name=\"Model\"):\n",
    "    model.train()\n",
    "    print(f\"\\n--- {name} Training ---\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        for data, target in tqdm(loader, desc=f\"{name} Epoch {epoch+1}/{epochs}\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = out.argmax(1)\n",
    "            correct   += (preds == target).sum().item()\n",
    "            total     += target.size(0)\n",
    "\n",
    "        print(f\"[{name}] Epoch {epoch+1}: \"\n",
    "              f\"Loss={total_loss/len(loader):.4f}, \"\n",
    "              f\"Acc={100*correct/total:.2f}%\")\n",
    "    print(f\"{name} training done.\")\n",
    "\n",
    "def eval_mode(model, loader, device, name=\"Model\"):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(loader, desc=f\"Evaluating {name}\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            out = model(data)\n",
    "            preds = out.argmax(1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total   += target.size(0)\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"[{name}] Test Accuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "# --- 5. FP32 학습 & 저장 ---\n",
    "print(\"\\n=== FP32 ResNet18 ===\")\n",
    "fp32_model   = get_resnet18().to(device)\n",
    "criterion    = nn.CrossEntropyLoss()\n",
    "optimizer_fp = optim.Adam(fp32_model.parameters(), lr=lr)\n",
    "\n",
    "train_mode(fp32_model, train_loader, criterion, optimizer_fp, num_epoch, device, name=\"FP32\")\n",
    "fp32_acc = eval_mode(fp32_model, test_loader, device, name=\"FP32\")\n",
    "\n",
    "fp32_path = save_path.replace(\".pth\",\"_fp32.pth\")\n",
    "torch.save(fp32_model.state_dict(), fp32_path)\n",
    "print(f\"Saved FP32 → {fp32_path}\")\n",
    "\n",
    "\n",
    "# --- 6. QAT 준비 & 학습 ---\n",
    "print(\"\\n=== QAT ResNet18 ===\")\n",
    "qat_fp32 = get_resnet18(pretrained=False)\n",
    "qat_fp32.load_state_dict(torch.load(fp32_path, map_location='cpu'))\n",
    "\n",
    "qat_model = QuantizedResNet18(qat_fp32).to('cpu')\n",
    "print(\"Fused+wrapped for QAT.\")\n",
    "\n",
    "qat_model.qconfig = quant.QConfig(\n",
    "    activation=functools.partial(observer.HistogramObserver, reduce_range=True),\n",
    "    weight=functools.partial(\n",
    "        observer.PerChannelMinMaxObserver,\n",
    "        dtype=torch.qint8,\n",
    "        qscheme=torch.per_channel_symmetric\n",
    "    )\n",
    ")\n",
    "print(\"QConfig:\", qat_model.qconfig)\n",
    "\n",
    "prepare_qat(qat_model, inplace=True)\n",
    "print(\"Prepared for QAT.\")\n",
    "\n",
    "qat_model.to(device)\n",
    "optimizer_qat = optim.Adam(qat_model.parameters(), lr=lr * 0.1)\n",
    "train_mode(qat_model, train_loader, criterion, optimizer_qat, qat_epoch, device, name=\"QAT\")\n",
    "\n",
    "\n",
    "# --- 7. INT8 변환 & 평가 (CPU) ---\n",
    "print(\"\\n--- Converting to INT8 ---\")\n",
    "qat_model.to('cpu')\n",
    "quantized_model = convert(qat_model, inplace=True)\n",
    "print(\"Converted to INT8.\")\n",
    "\n",
    "quant_acc = eval_mode(quantized_model, test_loader, torch.device('cpu'), name=\"Quantized\")\n",
    "\n",
    "\n",
    "# --- 8. 저장 & 크기 비교 ---\n",
    "torch.save(quantized_model.state_dict(), save_path)\n",
    "print(f\"Saved Quantized → {save_path}\")\n",
    "\n",
    "#fp32_sz  = os.path.getsize(fp32_path)/(1024**2)\n",
    "quant_sz = os.path.getsize(save_path)/(1024**2)\n",
    "#print(f\"FP32 size: {fp32_sz:.2f} MB\")\n",
    "print(f\"Quant size: {quant_sz:.2f} MB\")\n",
    "#print(f\"Size reduction: {fp32_sz/quant_sz:.2f}×\")\n",
    "#print(f\"Acc FP32: {fp32_acc:.2f}%, Quant: {quant_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5bc38b-7e8b-4771-b6f6-f87ebaae6717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\AppData\\Local\\Temp\\ipykernel_10532\\754648523.py:119: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FP32 Training ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced48ea242954d1db0bb843eda361c84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Ep1/2:   0%|          | 0/3125 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\AppData\\Local\\Temp\\ipykernel_10532\\754648523.py:127: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FP32 Ep1: Loss=0.8852 Acc=69.30%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84866972678b4b98aff76569cf8b374a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Ep2/2:   0%|          | 0/3125 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FP32 Ep2: Loss=0.6884 Acc=76.23%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b222e9c988284b38b64f7959f8305a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval FP32:   0%|          | 0/625 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FP32 Accuracy: 90.89%\n",
      "=== QAT Setup ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== QAT Training ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd58cfdf9c5b4f6ba94674ddd6910752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QAT Ep1/2:   0%|          | 0/3125 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QAT Ep1: Loss=2941.6958 Acc=39.58%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a73e8233644212b10cb18a431f4e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QAT Ep2/2:   0%|          | 0/3125 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QAT Ep2: Loss=44.7330 Acc=34.67%\n",
      "=== Converting to INT8 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7360b3ff34049af82e89acc8e6809df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Quantized:   0%|          | 0/625 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torch.ao.quantization as quant\n",
    "from torch.ao.quantization import (\n",
    "    QuantStub, DeQuantStub,\n",
    "    prepare_qat, convert, observer\n",
    ")\n",
    "from torch.nn.quantized import FloatFunctional\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. 하이퍼파라미터 & 디바이스 ---\n",
    "# **수정: 배치 사이즈를 더 적극적으로 줄였습니다.**\n",
    "# 문제가 지속되면 8, 4까지도 시도해 보세요.\n",
    "batch_size  = 16 # 이전 32에서 16으로 변경. 메모리 문제가 가장 유력한 원인.\n",
    "lr          = 1e-4\n",
    "num_epoch   = 2 # FP32 학습 에폭 (공부용으로 짧게 유지)\n",
    "qat_epoch   = 2 # QAT 학습 에폭 (공부용으로 짧게 유지)\n",
    "save_path   = \"./qat_resnet18_model.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True # GPU 사용 시 성능 향상\n",
    "torch.backends.quantized.engine = 'fbgemm' # 양자화 백엔드 설정 (x86 CPU 환경용)\n",
    "\n",
    "# --- 2. 데이터 로드 ---\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_ds  = datasets.CIFAR10(\"./data\", train=False,download=True, transform=transform_test)\n",
    "\n",
    "# **수정: num_workers를 1로 줄이고 pin_memory=False로 설정.**\n",
    "# CPU로 모델을 옮겨 양자화 변환 시, 데이터 로딩으로 인한 메모리 부하를 최소화합니다.\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=1, pin_memory=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=1, pin_memory=False)\n",
    "\n",
    "# --- 3. 모델 및 QuantBlock 정의 ---\n",
    "def get_resnet18(num_classes=10, pretrained=True):\n",
    "    weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    m = models.resnet18(weights=weights)\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "class QBasicBlock(nn.Module):\n",
    "    def __init__(self, orig: BasicBlock):\n",
    "        super().__init__()\n",
    "        # fused block 그대로 재사용\n",
    "        self.conv1      = orig.conv1\n",
    "        self.bn1        = orig.bn1\n",
    "        self.relu       = orig.relu\n",
    "        self.conv2      = orig.conv2\n",
    "        self.bn2        = orig.bn2\n",
    "        self.downsample = orig.downsample\n",
    "        self.skip_add   = FloatFunctional()\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n",
    "        out = self.conv2(out); out = self.bn2(out)\n",
    "        if self.downsample: identity = self.downsample(x)\n",
    "        out = self.skip_add.add(out, identity)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    def __init__(self, fp32_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model    = fp32_model\n",
    "        self.quant    = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        # fuse 전에 eval → fuse → replace → train\n",
    "        self.model.eval()\n",
    "        self._fuse_modules()\n",
    "        self._replace_blocks()\n",
    "        self.model.train()\n",
    "    def _fuse_modules(self):\n",
    "        quant.fuse_modules(self.model, [['conv1','bn1','relu']], inplace=True)\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, BasicBlock):\n",
    "                quant.fuse_modules(m, [['conv1','bn1','relu']], inplace=True)\n",
    "                quant.fuse_modules(m, [['conv2','bn2']],   inplace=True)\n",
    "                if m.downsample:\n",
    "                    quant.fuse_modules(m.downsample, ['0','1'], inplace=True)\n",
    "    def _replace_blocks(self):\n",
    "        for name, child in list(self.model.named_children()):\n",
    "            if isinstance(child, nn.Sequential):\n",
    "                new_seq = []\n",
    "                for blk in child:\n",
    "                    new_seq.append(QBasicBlock(blk) if isinstance(blk, BasicBlock) else blk)\n",
    "                setattr(self.model, name, nn.Sequential(*new_seq))\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# --- 4. 학습/평가 루프 정의 ---\n",
    "# FP32 training with AMP\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "def train_fp32(model, loader, criterion, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        tot_loss, corr, tot = 0,0,0\n",
    "        for x,y in tqdm(loader, desc=f\"FP32 Ep{ep+1}/{epochs}\"):\n",
    "            x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            tot_loss += loss.item()\n",
    "            preds = out.argmax(1)\n",
    "            corr    += (preds==y).sum().item()\n",
    "            tot     += y.size(0)\n",
    "        print(f\"  FP32 Ep{ep+1}: Loss={tot_loss/len(loader):.4f} Acc={100*corr/tot:.2f}%\")\n",
    "\n",
    "# QAT training without AMP (올바른 접근 방식)\n",
    "def train_qat(model, loader, criterion, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        tot_loss, corr, tot = 0,0,0\n",
    "        for x,y in tqdm(loader, desc=f\"QAT Ep{ep+1}/{epochs}\"):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += loss.item()\n",
    "            preds = out.argmax(1)\n",
    "            corr    += (preds==y).sum().item()\n",
    "            tot     += y.size(0)\n",
    "        print(f\"  QAT Ep{ep+1}: Loss={tot_loss/len(loader):.4f} Acc={100*corr/tot:.2f}%\")\n",
    "\n",
    "def eval_model(model, loader, device, name):\n",
    "    model.eval()\n",
    "    corr, tot = 0,0\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm(loader, desc=f\"Eval {name}\"):\n",
    "            x,y = x.to(device), y.to(device) # 데이터를 명시된 device로 이동 (CPU 또는 GPU)\n",
    "            out = model(x)\n",
    "            corr += (out.argmax(1)==y).sum().item()\n",
    "            tot  += y.size(0)\n",
    "    acc = 100*corr/tot\n",
    "    print(f\"  {name} Accuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "# --- 5. FP32 학습 & 평가 ---\n",
    "print(\"=== FP32 Training ===\")\n",
    "fp32 = get_resnet18().to(device)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt_fp = optim.Adam(fp32.parameters(), lr=lr)\n",
    "train_fp32(fp32, train_loader, crit, opt_fp, num_epoch, device)\n",
    "fp32_acc = eval_model(fp32, test_loader, device, \"FP32\")\n",
    "torch.save(fp32.state_dict(), save_path.replace(\".pth\",\"_fp32.pth\"))\n",
    "\n",
    "# --- 6. QAT 준비 & 학습 ---\n",
    "print(\"=== QAT Setup ===\")\n",
    "qat_fp32 = get_resnet18(pretrained=False)\n",
    "qat_fp32.load_state_dict(torch.load(save_path.replace(\".pth\",\"_fp32.pth\"), map_location='cpu'))\n",
    "qat_model = QuantizedResNet18(qat_fp32).to('cpu') # QAT 모델은 CPU에서 준비\n",
    "\n",
    "qat_model.qconfig = quant.QConfig(\n",
    "    activation=functools.partial(observer.HistogramObserver, reduce_range=True),\n",
    "    weight=functools.partial(observer.PerChannelMinMaxObserver,\n",
    "                             dtype=torch.qint8,\n",
    "                             qscheme=torch.per_channel_symmetric)\n",
    ")\n",
    "prepare_qat(qat_model, inplace=True)\n",
    "\n",
    "print(\"=== QAT Training ===\")\n",
    "qat_model.to(device) # QAT 학습은 GPU에서 진행\n",
    "opt_qat = optim.Adam(qat_model.parameters(), lr=lr*0.1)\n",
    "train_qat(qat_model, train_loader, crit, opt_qat, qat_epoch, device)\n",
    "\n",
    "\n",
    "# --- 7. INT8 변환 & 평가 ---\n",
    "print(\"=== Converting to INT8 ===\")\n",
    "qat_model.to('cpu') # 변환 전 CPU로 이동\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() # **추가: GPU 캐시 비우기**\n",
    "\n",
    "# **수정: convert 전에 모델을 eval() 모드로 전환하고, inplace=False로 새로운 모델 객체 생성**\n",
    "quantized = convert(qat_model.eval(), inplace=False)\n",
    "# 양자화 모델 평가 시 명시적으로 'cpu' device 사용\n",
    "quant_acc = eval_model(quantized, test_loader, torch.device('cpu'), \"Quantized\")\n",
    "\n",
    "# --- 8. 저장 & 크기 비교 ---\n",
    "torch.save(quantized.state_dict(), save_path)\n",
    "print(\"Sizes:\",\n",
    "      f\"FP32 {(os.path.getsize(save_path.replace('.pth','_fp32.pth'))/1e6):.2f}MB →\",\n",
    "      f\"Quant {(os.path.getsize(save_path)/1e6):.2f}MB\")\n",
    "print(f\"Accuracies: FP32 {fp32_acc:.2f}%  Quant {quant_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d9411-2b87-4f75-9aa3-ef7fd5d5b646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\AppData\\Local\\Temp\\ipykernel_30592\\1891647054.py:115: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FP32 Training ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97f5b3fe015431da1c167a4da2a0fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Ep1/2:   0%|          | 0/782 [00:22<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\AppData\\Local\\Temp\\ipykernel_30592\\1891647054.py:123: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FP32 Ep1: Loss=0.7929 Acc=72.35%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae91d40247a40d38aee29b663ecd3eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FP32 Ep2/2:   0%|          | 0/782 [00:22<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FP32 Ep2: Loss=0.5723 Acc=80.07%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4775f05e2cf94d5898471381dd6e2c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval FP32:   0%|          | 0/157 [00:20<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FP32 Accuracy: 91.88%\n",
      "=== QAT Setup ===\n",
      "=== QAT Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\anaconda\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcc10b6ec68497cbd513f7e36230c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QAT Ep1/2:   0%|          | 0/782 [00:22<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QAT Ep1: Loss=8463.5545 Acc=34.96%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90560e0b07cd4ff7b6c105b0d151eaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QAT Ep2/2:   0%|          | 0/782 [00:22<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QAT Ep2: Loss=698.6713 Acc=42.55%\n",
      "=== Converting to INT8 ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b235ce66164c009217f29658973730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval Quantized:   0%|          | 0/157 [00:20<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torch.ao.quantization as quant\n",
    "from torch.ao.quantization import (\n",
    "    QuantStub, DeQuantStub,\n",
    "    prepare_qat, convert, observer\n",
    ")\n",
    "from torch.nn.quantized import FloatFunctional\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 1. 하이퍼파라미터 & 디바이스 ---\n",
    "batch_size  = 64\n",
    "lr          = 1e-4\n",
    "num_epoch   = 2\n",
    "qat_epoch   = 2\n",
    "save_path   = \"./qat_resnet18_model.pth\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "# --- 2. 데이터 로드 ---\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=transform_train)\n",
    "test_ds  = datasets.CIFAR10(\"./data\", train=False,download=True, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=8, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=8, pin_memory=True)\n",
    "\n",
    "# --- 3. 모델 및 QuantBlock 정의 ---\n",
    "def get_resnet18(num_classes=10, pretrained=True):\n",
    "    weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "    m = models.resnet18(weights=weights)\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "class QBasicBlock(nn.Module):\n",
    "    def __init__(self, orig: BasicBlock):\n",
    "        super().__init__()\n",
    "        # fused block 그대로 재사용\n",
    "        self.conv1      = orig.conv1\n",
    "        self.bn1        = orig.bn1\n",
    "        self.relu       = orig.relu\n",
    "        self.conv2      = orig.conv2\n",
    "        self.bn2        = orig.bn2\n",
    "        self.downsample = orig.downsample\n",
    "        self.skip_add   = FloatFunctional()\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x); out = self.bn1(out); out = self.relu(out)\n",
    "        out = self.conv2(out); out = self.bn2(out)\n",
    "        if self.downsample: identity = self.downsample(x)\n",
    "        out = self.skip_add.add(out, identity)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    def __init__(self, fp32_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.model   = fp32_model\n",
    "        self.quant   = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        # fuse 전에 eval → fuse → replace → train\n",
    "        self.model.eval()\n",
    "        self._fuse_modules()\n",
    "        self._replace_blocks()\n",
    "        self.model.train()\n",
    "    def _fuse_modules(self):\n",
    "        quant.fuse_modules(self.model, [['conv1','bn1','relu']], inplace=True)\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, BasicBlock):\n",
    "                quant.fuse_modules(m, [['conv1','bn1','relu']], inplace=True)\n",
    "                quant.fuse_modules(m, [['conv2','bn2']],   inplace=True)\n",
    "                if m.downsample:\n",
    "                    quant.fuse_modules(m.downsample, ['0','1'], inplace=True)\n",
    "    def _replace_blocks(self):\n",
    "        for name, child in list(self.model.named_children()):\n",
    "            if isinstance(child, nn.Sequential):\n",
    "                new_seq = []\n",
    "                for blk in child:\n",
    "                    new_seq.append(QBasicBlock(blk) if isinstance(blk, BasicBlock) else blk)\n",
    "                setattr(self.model, name, nn.Sequential(*new_seq))\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.model(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "# --- 4. 학습/평가 루프 정의 ---\n",
    "# FP32 training with AMP\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "def train_fp32(model, loader, criterion, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        tot_loss, corr, tot = 0,0,0\n",
    "        for x,y in tqdm(loader, desc=f\"FP32 Ep{ep+1}/{epochs}\"):\n",
    "            x,y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(x)\n",
    "                loss = criterion(out, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            tot_loss += loss.item()\n",
    "            preds = out.argmax(1)\n",
    "            corr    += (preds==y).sum().item()\n",
    "            tot     += y.size(0)\n",
    "        print(f\"  FP32 Ep{ep+1}: Loss={tot_loss/len(loader):.4f} Acc={100*corr/tot:.2f}%\")\n",
    "\n",
    "# QAT training without AMP\n",
    "def train_qat(model, loader, criterion, optimizer, epochs, device):\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        tot_loss, corr, tot = 0,0,0\n",
    "        for x,y in tqdm(loader, desc=f\"QAT Ep{ep+1}/{epochs}\"):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)               # ← 여기서는 FP32 모드\n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tot_loss += loss.item()\n",
    "            preds = out.argmax(1)\n",
    "            corr    += (preds==y).sum().item()\n",
    "            tot     += y.size(0)\n",
    "        print(f\"  QAT Ep{ep+1}: Loss={tot_loss/len(loader):.4f} Acc={100*corr/tot:.2f}%\")\n",
    "\n",
    "def eval_model(model, loader, device, name):\n",
    "    model.eval()\n",
    "    corr, tot = 0,0\n",
    "    with torch.no_grad():\n",
    "        for x,y in tqdm(loader, desc=f\"Eval {name}\"):\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            corr += (out.argmax(1)==y).sum().item()\n",
    "            tot  += y.size(0)\n",
    "    acc = 100*corr/tot\n",
    "    print(f\"  {name} Accuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "# --- 5. FP32 학습 & 평가 ---\n",
    "print(\"=== FP32 Training ===\")\n",
    "fp32 = get_resnet18().to(device)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt_fp = optim.Adam(fp32.parameters(), lr=lr)\n",
    "train_fp32(fp32, train_loader, crit, opt_fp, num_epoch, device)\n",
    "fp32_acc = eval_model(fp32, test_loader, device, \"FP32\")\n",
    "torch.save(fp32.state_dict(), save_path.replace(\".pth\",\"_fp32.pth\"))\n",
    "\n",
    "# --- 6. QAT 준비 & 학습 ---\n",
    "print(\"=== QAT Setup ===\")\n",
    "qat_fp32 = get_resnet18(pretrained=False)\n",
    "qat_fp32.load_state_dict(torch.load(save_path.replace(\".pth\",\"_fp32.pth\"), map_location='cpu'))\n",
    "qat_model = QuantizedResNet18(qat_fp32).to('cpu')\n",
    "\n",
    "qat_model.qconfig = quant.QConfig(\n",
    "    activation=functools.partial(observer.HistogramObserver, reduce_range=True),\n",
    "    weight=functools.partial(observer.PerChannelMinMaxObserver,\n",
    "                             dtype=torch.qint8,\n",
    "                             qscheme=torch.per_channel_symmetric)\n",
    ")\n",
    "prepare_qat(qat_model, inplace=True)\n",
    "\n",
    "print(\"=== QAT Training ===\")\n",
    "qat_model.to(device)\n",
    "opt_qat = optim.Adam(qat_model.parameters(), lr=lr*0.1)\n",
    "train_qat(qat_model, train_loader, crit, opt_qat, qat_epoch, device)\n",
    "\n",
    "# --- 7. INT8 변환 & 평가 ---\n",
    "print(\"=== Converting to INT8 ===\")\n",
    "qat_model.to('cpu')\n",
    "quantized = convert(qat_model, inplace=True)\n",
    "quant_acc = eval_model(quantized, test_loader, torch.device('cpu'), \"Quantized\")\n",
    "\n",
    "# --- 8. 저장 & 크기 비교 ---\n",
    "torch.save(quantized.state_dict(), save_path)\n",
    "print(\"Sizes:\",\n",
    "      f\"FP32 {(os.path.getsize(save_path.replace('.pth','_fp32.pth'))/1e6):.2f}MB →\",\n",
    "      f\"Quant {(os.path.getsize(save_path)/1e6):.2f}MB\")\n",
    "print(f\"Accuracies: FP32 {fp32_acc:.2f}%  Quant {quant_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf8ecd5d-ff0a-482e-b06c-227d6c664a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'watch' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!watch -n 0.5 nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f350e3f-03a2-434f-954b-9cda2d811714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f06e3-1024-46a0-ac33-6981ff0c1790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5dc4e9-3795-4e0d-8aef-b0be5187241d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dab89e-67fe-437b-b3d6-8bbafa371388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CUDA 12.4)",
   "language": "python",
   "name": "cuda124"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
