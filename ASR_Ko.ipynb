{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0fbaf3e-07a8-49ed-9e7c-a2527dc4b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def  load_ai_hub_data(path):\n",
    "    for root, dirs,files in os.walk(path):\n",
    "        for file in files:\n",
    "            audio_path=os.path.join(root,file)\n",
    "            text_file=file.replace(\".wav\", \".txt\")\n",
    "            text_path=os.path.join(root.replace(\"원천데이터\", \"라벨링데이터\"), text_file)\n",
    "\n",
    "            if os.path.exists(text_path):\n",
    "                try:\n",
    "                    with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        text = f.read().strip()\n",
    "                        data.append({\"audio_path\": audio_path, \"text\": text})\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {text_path}: {e}\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab2cd10-9da5-4fa5-935c-4f3fb94a48ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_path = \"./dummy_ai_hub_data\"\n",
    "\n",
    "# (실제 데이터셋이 없으므로, 더미 파일 생성 로직 추가)\n",
    "# 이 부분은 실제 데이터셋을 다운로드했다면 건너뛰세요.\n",
    "if not os.path.exists(base_data_path):\n",
    "    os.makedirs(os.path.join(base_data_path, \"원천데이터\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_data_path, \"라벨링데이터\"), exist_ok=True)\n",
    "    for i in range(5): # 5개의 더미 파일 생성\n",
    "        dummy_wav_path = os.path.join(base_data_path, \"원천데이터\", f\"dummy_{i:03d}.wav\")\n",
    "        dummy_txt_path = os.path.join(base_data_path, \"라벨링데이터\", f\"dummy_{i:03d}.txt\")\n",
    "        # 더미 .wav 파일 (실제 데이터는 아니지만 경로 존재를 위해 생성)\n",
    "        with open(dummy_wav_path, \"wb\") as f:\n",
    "            f.write(b\"RIFF\\x00\\x00\\x00\\x00WAVEfmt \\x10\\x00\\x00\\x00\\x01\\x00\\x01\\x00\\x80>\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x10\\x00data\\x00\\x00\\x00\\x00\") # Minimal WAV header\n",
    "        with open(dummy_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"안녕하세요 더미 텍스트입니다 {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac1deba-4895-4652-b1a3-7f94f1f76f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JH\\anaconda\\envs\\ai_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processor configured. Final Vocab Size: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([32]) in the checkpoint and torch.Size([57]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([32, 768]) in the checkpoint and torch.Size([57, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'facebook/wav2vec2-base-960h' loaded and lm_head modified. lm_head.out_features: 57\n",
      "Loading data from: ./dummy_ai_hub_data\n",
      "로드된 실제 데이터 샘플 수: 5\n",
      "훈련 데이터셋 크기: 5 샘플, 배치 수: 2\n",
      "Optimizer configured (AdamW, LR=0.0001)\n",
      "\n",
      "--- Training Started with AI Hub-like Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s]C:\\Users\\JH\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (0). Kernel size: (10). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 267\u001b[0m\n\u001b[0;32m    263\u001b[0m label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    265\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# 그라디언트 초기화\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;66;03m# CTC 손실 값\u001b[39;00m\n\u001b[0;32m    270\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m# 역전파\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:2128\u001b[0m, in \u001b[0;36mWav2Vec2ForCTC.forward\u001b[1;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[0;32m   2125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size:\n\u001b[0;32m   2126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel values must be <= vocab_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2128\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwav2vec2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2136\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   2137\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1714\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[1;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1709\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1710\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1711\u001b[0m )\n\u001b[0;32m   1712\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1714\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m extract_features\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1718\u001b[0m     \u001b[38;5;66;03m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:437\u001b[0m, in \u001b[0;36mWav2Vec2FeatureEncoder.forward\u001b[1;34m(self, input_values)\u001b[0m\n\u001b[0;32m    432\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    433\u001b[0m             conv_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    434\u001b[0m             hidden_states,\n\u001b[0;32m    435\u001b[0m         )\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 437\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:334\u001b[0m, in \u001b[0;36mWav2Vec2GroupNormConvLayer.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m--> 334\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m    336\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (0). Kernel size: (10). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import os\n",
    "import unicodedata\n",
    "from transformers import AutoProcessor, AutoModelForCTC, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "import json\n",
    "from tqdm.auto import tqdm # tqdm 임포트 확인\n",
    "\n",
    "# --- 0. 환경 설정 (이전 코드에서 재사용) ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "# --- 1. Custom Tokenizer Creation for Wav2Vec2 Model (재사용) ---\n",
    "vocab_elements = [\n",
    "    \"ㄱ\", \"ㄴ\", \"ㄷ\", \"ㄹ\", \"ㅁ\", \"ㅂ\", \"ㅅ\", \"ㅇ\", \"ㅈ\", \"ㅊ\", \"ㅋ\", \"ㅌ\", \"ㅍ\", \"ㅎ\",\n",
    "    \"ㄲ\", \"ㄸ\", \"ㅃ\", \"ㅆ\", \"ㅉ\",\n",
    "    \"ㅏ\", \"ㅑ\", \"ㅓ\", \"ㅕ\", \"ㅗ\", \"ㅛ\", \"ㅜ\", \"ㅠ\", \"ㅡ\", \"ㅣ\", \"ㅐ\", \"ㅔ\", \"ㅚ\", \"ㅟ\", \"ㅢ\",\n",
    "    \"ㅘ\", \"ㅝ\", \"ㅙ\", \"ㅞ\",\n",
    "    \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\",\n",
    "    \",\", \".\", \"?\", \"!\", \"-\",\n",
    "    \" \", \"|\", \"<unk>\", \"<pad>\"\n",
    "]\n",
    "unique_vocab = sorted(list(set([v for v in vocab_elements if v not in [\" \", \"|\", \"<unk>\", \"<pad>\"]])))\n",
    "final_vocab_list = unique_vocab + [\" \", \"|\", \"<unk>\", \"<pad>\"]\n",
    "\n",
    "vocab_dict = {token: i for i, token in enumerate(final_vocab_list)}\n",
    "\n",
    "with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab_dict, f, ensure_ascii=False)\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    \"vocab.json\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    word_delimiter_token=\" \"\n",
    ")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(\n",
    "    feature_size=1,\n",
    "    sampling_rate=16000,\n",
    "    padding_value=0.0,\n",
    "    do_normalize=True,\n",
    "    return_attention_mask=False\n",
    ")\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "final_vocab_size = processor.tokenizer.vocab_size\n",
    "print(f\"Processor configured. Final Vocab Size: {final_vocab_size}\")\n",
    "\n",
    "# --- 2. 모델 로드 (재사용) ---\n",
    "model = AutoModelForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=final_vocab_size,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.lm_head = torch.nn.Linear(model.config.hidden_size, final_vocab_size)\n",
    "model.to(device)\n",
    "print(f\"Model '{model_name}' loaded and lm_head modified. lm_head.out_features: {model.lm_head.out_features}\")\n",
    "\n",
    "# --- 3. 데이터 콜레이터 (재사용) ---\n",
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, features):\n",
    "        audio_samples = [feature[\"audio\"] for feature in features]\n",
    "        text_labels = [feature[\"text\"] for feature in features]\n",
    "\n",
    "        padded_audio_batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_values\": audio_samples},\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        batch_input_values = padded_audio_batch[\"input_values\"]\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            tokenized_labels = []\n",
    "            for label in text_labels:\n",
    "                encoded_label = self.processor.tokenizer(label).input_ids\n",
    "                if encoded_label:\n",
    "                    tokenized_labels.append(encoded_label)\n",
    "                else:\n",
    "                    print(f\"Warning: Empty tokenized label for text: '{label}'. Using default.\")\n",
    "                    default_text = \"안녕\"\n",
    "                    decomposed_default_text = \"\".join(unicodedata.normalize(\"NFD\", char) for char in default_text if unicodedata.category(char) != 'Mn')\n",
    "                    tokenized_labels.append(self.processor.tokenizer(decomposed_default_text).input_ids)\n",
    "\n",
    "            labels_batch = self.processor.tokenizer.pad(\n",
    "                {\"input_ids\": tokenized_labels},\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch = {\n",
    "            \"input_values\": batch_input_values,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "# --- 4. AIHubSpeechDataset 클래스 정의 (NEW) ---\n",
    "class AIHubSpeechDataset(Dataset):\n",
    "    def __init__(self, data_list, processor, sample_rate=16000):\n",
    "        self.data_list = data_list\n",
    "        self.processor = processor\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data_list[idx]\n",
    "        audio_path = item[\"audio_path\"]\n",
    "        text_label = item[\"text\"]\n",
    "\n",
    "        # 1. 오디오 로드 및 리샘플링\n",
    "        # torchaudio.load는 (waveform, sample_rate) 튜플을 반환\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(audio_path)\n",
    "            if sr != self.sample_rate:\n",
    "                # 리샘플러를 생성하고 적용\n",
    "                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "                waveform = resampler(waveform)\n",
    "            \n",
    "            # 스테레오 오디오인 경우 모노로 변환 (첫 번째 채널만 사용)\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform[0, :] # 첫 번째 채널만 선택\n",
    "            else:\n",
    "                waveform = waveform.squeeze(0) # (1, sequence_length) -> (sequence_length)\n",
    "            \n",
    "            # Wav2Vec2 모델의 feature extractor는 1D 넘파이 배열 또는 파이토치 텐서를 기대함\n",
    "            audio_array = waveform.numpy() # 또는 .tolist()\n",
    "            \n",
    "        except Exception as e:\n",
    "            # 오디오 로드 실패 시 경고 출력 및 더미 데이터 반환\n",
    "            print(f\"Warning: Could not load audio {audio_path}: {e}. Returning dummy data.\")\n",
    "            audio_array = torch.randn(self.sample_rate * 2).numpy() # 2초 길이의 더미 오디오\n",
    "            text_label = \"오류 발생 더미 텍스트\" # 더미 텍스트도 설정\n",
    "\n",
    "        # 2. 텍스트 전처리 (한글 자모 분리)\n",
    "        # Wav2Vec2CTCTokenizer는 한글 자모 단위를 예상하므로 NFD 정규화 필요\n",
    "        # 단, 한글이 아닌 문자는 정규화하지 않도록 유니코드 카테고리 'Mn'(Mark, Nonspacing) 제외\n",
    "        # (이전 코드에서 텍스트 생성 시 이 부분을 이미 고려했으므로, 여기서는 한 번 더 확인차 적용)\n",
    "        decomposed_text = \"\".join(unicodedata.normalize(\"NFD\", char) for char in text_label if unicodedata.category(char) != 'Mn')\n",
    "        \n",
    "        # 라벨링 데이터 전사 시 간혹 긴 공백이 있을 수 있으므로 공백 정규화\n",
    "        decomposed_text = \" \".join(decomposed_text.split())\n",
    "        \n",
    "        return {\"audio\": audio_array, \"text\": decomposed_text}\n",
    "\n",
    "# --- 5. 데이터셋 로드 및 DataLoader 준비 ---\n",
    "\n",
    "# 이 부분은 실제 AI Hub 데이터셋 경로로 바꿔주세요!\n",
    "# 예: base_ai_hub_path = \"/path/to/your/AI_Hub_Korean_Speech_Dataset/일반인_음성/01.초등학생\"\n",
    "# 현재는 더미 데이터를 생성하여 사용하겠습니다.\n",
    "\n",
    "base_data_path = \"./dummy_ai_hub_data\"\n",
    "\n",
    "# (실제 데이터셋이 없다면 이 더미 파일 생성 로직을 사용)\n",
    "if not os.path.exists(base_data_path):\n",
    "    os.makedirs(os.path.join(base_data_path, \"원천데이터\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_data_path, \"라벨링데이터\"), exist_ok=True)\n",
    "    print(\"Dummy AI Hub data directories created.\")\n",
    "    for i in range(10): # 10개의 더미 파일 생성 (훈련 데이터셋 크기 조절)\n",
    "        dummy_wav_path = os.path.join(base_data_path, \"원천데이터\", f\"dummy_{i:03d}.wav\")\n",
    "        dummy_txt_path = os.path.join(base_data_path, \"라벨링데이터\", f\"dummy_{i:03d}.txt\")\n",
    "        # 더미 .wav 파일 (torchaudio가 로드할 수 있는 간단한 오디오 데이터 생성)\n",
    "        # 16000Hz, 1초 길이의 사인파\n",
    "        sample_rate = 16000\n",
    "        duration = 1 # seconds\n",
    "        frequency = 440 # Hz\n",
    "        t = torch.linspace(0, duration, int(sample_rate * duration), requires_grad=False)\n",
    "        dummy_waveform = 0.5 * torch.sin(2 * torch.pi * frequency * t)\n",
    "        torchaudio.save(dummy_wav_path, dummy_waveform.unsqueeze(0), sample_rate) # (channels, samples) 형태로 저장\n",
    "\n",
    "        with open(dummy_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"테스트 문장 번호 {i} 입니다\") # 실제 한글 텍스트 사용\n",
    "\n",
    "print(f\"Loading data from: {base_data_path}\")\n",
    "\n",
    "def load_ai_hub_data_paths(base_path):\n",
    "    data = []\n",
    "    # AI Hub 데이터셋의 실제 구조에 따라 이 로직을 수정해야 합니다!\n",
    "    # 여기서는 '원천데이터'와 '라벨링데이터'가 같은 부모 아래에 있다고 가정합니다.\n",
    "    # 각 음성 파일의 메타데이터 파일(JSON/TXT) 위치를 정확히 파악해야 합니다.\n",
    "    \n",
    "    # 예시: '원천데이터' 폴더를 탐색하며 .wav 파일과 그에 매칭되는 .txt 라벨링 파일을 찾음\n",
    "    audio_data_dir = os.path.join(base_path, \"원천데이터\")\n",
    "    label_data_dir = os.path.join(base_path, \"라벨링데이터\")\n",
    "\n",
    "    if not os.path.exists(audio_data_dir) or not os.path.exists(label_data_dir):\n",
    "        print(f\"Warning: '{audio_data_dir}' or '{label_data_dir}' not found. Check AI Hub data structure.\")\n",
    "        return []\n",
    "\n",
    "    for filename in os.listdir(audio_data_dir):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            audio_path = os.path.join(audio_data_dir, filename)\n",
    "            \n",
    "            # AI Hub 데이터는 파일명만 같고 확장자가 다른 경우가 많음\n",
    "            # 예: xxx.wav -> xxx.txt\n",
    "            text_filename = filename.replace(\".wav\", \".txt\")\n",
    "            text_path = os.path.join(label_data_dir, text_filename)\n",
    "\n",
    "            if os.path.exists(text_path):\n",
    "                try:\n",
    "                    with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        text = f.read().strip()\n",
    "                        if text: # 텍스트 내용이 비어있지 않은 경우만 추가\n",
    "                            data.append({\"audio_path\": audio_path, \"text\": text})\n",
    "                        else:\n",
    "                            print(f\"Skipping empty text for {audio_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading text for {audio_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"Warning: Text file not found for {audio_path} at {text_path}. Skipping.\")\n",
    "    return data\n",
    "\n",
    "# 실제 데이터 로드\n",
    "ai_hub_data_list = load_ai_hub_data_paths(base_data_path)\n",
    "print(f\"로드된 실제 데이터 샘플 수: {len(ai_hub_data_list)}\")\n",
    "\n",
    "if not ai_hub_data_list:\n",
    "    print(\"오류: 로드된 데이터가 없습니다. AI Hub 데이터셋 경로와 구조를 다시 확인해주세요!\")\n",
    "    print(\"훈련을 계속하려면 최소한의 데이터가 필요합니다.\")\n",
    "else:\n",
    "    # 데이터셋 인스턴스 생성\n",
    "    train_dataset = AIHubSpeechDataset(ai_hub_data_list, processor)\n",
    "\n",
    "    # DataLoader 인스턴스 생성\n",
    "    data_collator_instance = DataCollatorCTCWithPadding(processor=processor)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4, # 배치 크기는 하드웨어 사양에 따라 조절\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator_instance,\n",
    "        num_workers=0 # 멀티 프로세싱 (Windows에서는 0으로 설정하는 것이 안전)\n",
    "    )\n",
    "    print(f\"훈련 데이터셋 크기: {len(train_dataset)} 샘플, 배치 수: {len(train_loader)}\")\n",
    "\n",
    "    # --- 6. 훈련 환경 설정 (Optimizer) 재사용 ---\n",
    "    lr = 1e-4\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    print(f\"Optimizer configured (AdamW, LR={lr})\")\n",
    "\n",
    "    # --- 7. 훈련 루프 (재사용) ---\n",
    "    num_epochs = 3 # 예시 에포크 수\n",
    "\n",
    "    model.train() # 모델을 훈련 모드로 설정\n",
    "\n",
    "    print(\"\\n--- Training Started with AI Hub-like Data ---\")\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        # tqdm으로 훈련 진행 상황 표시\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\")):\n",
    "            input_val = batch[\"input_values\"].to(device)\n",
    "            label = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad() # 그라디언트 초기화\n",
    "\n",
    "            outputs = model(input_values=input_val, labels=label)\n",
    "            loss = outputs.loss # CTC 손실 값\n",
    "\n",
    "            loss.backward() # 역전파\n",
    "            optimizer.step() # 옵티마이저 스텝\n",
    "\n",
    "            total_loss += loss.item() # 손실 누적\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Training Completed ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Environment",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
