{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ccf22e-d115-4e86-b59d-43a233908a78",
   "metadata": {},
   "source": [
    "강화 학습 및 모델 경량화 1일차 수업을 지금 바로 시작하겠습니다!\n",
    "\n",
    "---\n",
    "\n",
    "## **1일차: 강화 학습 소개 및 핵심 개념, Gymnasium 환경 맛보기**\n",
    "\n",
    "오늘은 강화 학습(Reinforcement Learning, RL)의 세계로 첫 발을 내딛는 날입니다. 강화 학습이 무엇인지, 그리고 앞으로 학습할 내용을 위한 기본적인 개념들을 이해하는 데 집중해 보겠습니다.\n",
    "\n",
    "### **1. 강화 학습이란 무엇인가?**\n",
    "\n",
    "강화 학습은 **에이전트(Agent)**가 **환경(Environment)**과 상호작용하면서, 시행착오를 통해 **최적의 행동(Optimal Action)**을 학습하는 인공지능 분야입니다. 마치 어린아이가 자전거 타는 법을 배우듯, 넘어져보고(실패), 균형을 잡는(성공) 과정을 반복하며 스스로 익히는 것과 비슷합니다.\n",
    "\n",
    "* **에이전트(Agent)**: 학습하고 행동하는 주체입니다. (예: 게임 속 플레이어, 자율주행 자동차의 제어 시스템)\n",
    "* **환경(Environment)**: 에이전트가 상호작용하는 세상입니다. 에이전트의 행동에 반응하고, 새로운 상태와 보상을 제공합니다. (예: 게임 맵, 도로, 주식 시장)\n",
    "* **상태(State, $S$)**: 에이전트가 현재 처한 상황에 대한 정보입니다. (예: 게임 캐릭터의 위치, 자동차의 속도와 위치)\n",
    "* **행동(Action, $A$)**: 에이전트가 특정 상태에서 취할 수 있는 선택입니다. (예: 위로 점프, 좌회전)\n",
    "* **보상(Reward, $R$)**: 에이전트의 행동에 대한 환경의 피드백입니다. 목표 달성에 기여하면 양의 보상, 실패하면 음의 보상을 받습니다. 강화 학습은 이 보상을 **최대화**하는 것을 목표로 합니다.\n",
    "* **정책(Policy, $\\pi$)**: 특정 상태에서 어떤 행동을 취할지에 대한 에이전트의 전략 또는 규칙입니다. 에이전트의 \"뇌\"라고 생각할 수 있습니다. (예: 상태 $S$에서 행동 $A$를 할 확률)\n",
    "* **가치 함수(Value Function, $V(s)$ 또는 $Q(s,a)$)**: 특정 상태나 상태-행동 쌍이 얼마나 좋은지를 숫자로 나타낸 것입니다. 미래에 받을 보상까지 고려하여 계산합니다.\n",
    "\n",
    "강화 학습의 목표는 **최적 정책($\\pi^*$)**을 찾는 것입니다. 이 정책을 따르면 에이전트가 장기적으로 가장 많은 보상을 받을 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Gymnasium 환경 맛보기**\n",
    "\n",
    "우리는 강화 학습 에이전트를 학습시키기 위해 **Gymnasium**이라는 표준 라이브러리를 사용할 것입니다. Gymnasium은 다양한 강화 학습 환경을 제공하여, 복잡한 환경을 직접 구현할 필요 없이 알고리즘 개발에 집중할 수 있게 해줍니다.\n",
    "\n",
    "오늘의 첫 실습 환경은 가장 간단하면서도 강화 학습의 핵심 개념을 이해하는 데 도움이 되는 `FrozenLake-v1`입니다.\n",
    "\n",
    "**`FrozenLake-v1` 환경 설명:**\n",
    "\n",
    "* 4x4 또는 8x8 격자로 이루어진 얼어붙은 호수입니다.\n",
    "* 시작 지점(S)에서 목표 지점(G)까지 이동해야 합니다.\n",
    "* 타일은 **F(얼음, 안전)**, **H(구멍, 위험)**, **S(시작)**, **G(목표)**로 구성됩니다.\n",
    "* **행동**: 상(Up), 하(Down), 좌(Left), 우(Right) 4가지 방향으로 이동할 수 있습니다.\n",
    "* **보상**: 목표 지점에 도달하면 +1의 보상을 받고, 구멍에 빠지거나 이동할 수 없는 곳으로 가면 0의 보상을 받습니다.\n",
    "* **미끄러운(slippery) 특성**: 이 환경의 중요한 특징은 \"미끄럽다\"는 것입니다. 즉, 원하는 방향으로 행동하더라도 1/3의 확률로 의도하지 않은 옆 방향으로 미끄러질 수 있습니다. (초기 설정 `is_slippery=True`)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 실습: `FrozenLake-v1` 환경과 상호작용하기**\n",
    "\n",
    "이제 Jupyter Lab을 열고 `rl_gpu_env` 커널로 새로운 노트북을 생성한 후, 아래 코드를 따라가면서 `FrozenLake-v1` 환경을 탐색해 봅시다.\n",
    "\n",
    "```python\n",
    "# 필요한 라이브러리 임포트\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output # 주피터 랩에서 화면을 깔끔하게 하기 위함\n",
    "\n",
    "# 1. 환경 생성\n",
    "# is_slippery=False로 설정하여 미끄러움을 제거하고 먼저 기본 동작을 이해합니다.\n",
    "# 미끄러운 환경은 나중에 Q-러닝 등에서 다룰 때 다시 True로 설정할 것입니다.\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human') # human 모드로 설정하면 시각화 됨\n",
    "\n",
    "# 2. 환경 초기화\n",
    "# reset() 메서드는 환경을 초기 상태로 되돌리고, 초기 상태 정보와 추가 정보를 반환합니다.\n",
    "state, info = env.reset()\n",
    "print(f\"초기 상태: {state}\") # 0부터 15까지의 정수 (4x4 격자에서 각 칸에 대응)\n",
    "print(f\"상태 공간 크기: {env.observation_space.n}\") # 4x4 = 16\n",
    "print(f\"행동 공간 크기: {env.action_space.n}\") # 상, 하, 좌, 우 = 4\n",
    "\n",
    "# 환경 렌더링 (시각화)\n",
    "env.render()\n",
    "time.sleep(1) # 잠시 대기하여 시각화된 화면 확인\n",
    "\n",
    "# 3. 환경과 상호작용 (임의의 행동)\n",
    "# 몇 가지 임의의 행동을 취하며 환경의 변화를 관찰합니다.\n",
    "# 행동: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\n",
    "print(\"\\n--- 임의의 행동 시퀀스 ---\")\n",
    "actions = [1, 1, 2, 2, 1] # 예시: DOWN, DOWN, RIGHT, RIGHT, DOWN\n",
    "\n",
    "for i, action in enumerate(actions):\n",
    "    print(f\"\\n--- {i+1}번째 행동: {action} ---\")\n",
    "    # env.step(action) 메서드는 행동을 취한 후의 새로운 상태, 보상, 에피소드 종료 여부 등을 반환합니다.\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "    clear_output(wait=True) # 이전 화면 지우기\n",
    "    env.render() # 현재 상태 렌더링\n",
    "    print(f\"현재 상태: {new_state}, 보상: {reward}, 종료 여부(terminated): {terminated}, 잘림 여부(truncated): {truncated}\")\n",
    "    time.sleep(1) # 잠시 대기\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"에피소드 종료! 환경을 다시 초기화합니다.\")\n",
    "        state, info = env.reset() # 에피소드가 종료되면 환경을 다시 초기화\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(1)\n",
    "        break # 시퀀스 종료\n",
    "\n",
    "# 4. 환경 닫기 (메모리 해제)\n",
    "env.close()\n",
    "print(\"\\n환경이 닫혔습니다.\")\n",
    "\n",
    "```\n",
    "\n",
    "**코드 실행 후 관찰할 점:**\n",
    "\n",
    "* `env.observation_space.n`: 상태 공간의 크기 (4x4 = 16개의 칸).\n",
    "* `env.action_space.n`: 행동 공간의 크기 (상, 하, 좌, 우 4가지).\n",
    "* `env.reset()`: 에피소드가 시작될 때 환경을 초기화하고, 에이전트의 초기 위치(상태 0)를 반환합니다.\n",
    "* `env.step(action)`: 에이전트가 특정 `action`을 취했을 때 환경이 어떻게 변하는지 보여줍니다.\n",
    "    * `new_state`: 에이전트가 이동한 새로운 칸의 번호.\n",
    "    * `reward`: 해당 행동으로 받은 보상. (목표에 도달하면 1.0, 그 외에는 0.0)\n",
    "    * `terminated`: 에피소드가 성공(목표 도달) 또는 실패(구멍에 빠짐)로 완전히 종료되었는지 여부 (True/False).\n",
    "    * `truncated`: 에피소드가 시간 제한 등의 이유로 잘렸는지 여부 (True/False).\n",
    "* `render_mode='human'`: 이 설정을 통해 팝업창으로 환경 시각화를 볼 수 있습니다. (만약 팝업창이 뜨지 않는다면, Jupyter Lab에서 X 서버나 디스플레이 환경 설정이 필요할 수 있습니다. 그럴 경우 `render_mode='ansi'`로 변경하면 텍스트 기반으로 출력됩니다.)\n",
    "\n",
    "---\n",
    "\n",
    "### **오늘의 핵심 정리 및 다음 수업 예고**\n",
    "\n",
    "* **강화 학습의 기본 구성 요소**: 에이전트, 환경, 상태, 행동, 보상, 정책, 가치 함수.\n",
    "* **Gymnasium**: 강화 학습 환경을 위한 표준 라이브러리.\n",
    "* **`FrozenLake-v1` 환경**: 강화 학습의 개념을 이해하기 위한 간단한 격자 환경.\n",
    "\n",
    "오늘 우리는 `FrozenLake-v1` 환경에서 직접 행동을 취하며 환경이 어떻게 반응하는지 살펴보았습니다. 아직 \"학습\"은 없었지만, 이것이 강화 학습의 첫걸음입니다.\n",
    "\n",
    "내일은 **마르코프 의사결정 과정(MDP)**에 대해 더 깊이 파고들 것입니다. MDP는 강화 학습 문제의 수학적 모델링을 위한 핵심 개념입니다.\n",
    "\n",
    "궁금한 점이 있다면 언제든지 질문해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad36ddb-9ca0-453b-a26f-a53960741965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 상태: 14, 보상: 0.0, 종료 여부(terminated): False, 잘림 여부(truncated): False\n",
      "\n",
      "환경이 닫혔습니다.\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output # 주피터 랩에서 화면을 깔끔하게 하기 위함\n",
    "\n",
    "# 1. 환경 생성\n",
    "# is_slippery=False로 설정하여 미끄러움을 제거하고 먼저 기본 동작을 이해합니다.\n",
    "# 미끄러운 환경은 나중에 Q-러닝 등에서 다룰 때 다시 True로 설정할 것입니다.\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human') # human 모드로 설정하면 시각화 됨\n",
    "#env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='ansi')  # 환경을 설정하고 불러오는 핵심적인 부분\n",
    "# 2. 환경 초기화\n",
    "# reset() 메서드는 환경을 초기 상태로 되돌리고, 초기 상태 정보와 추가 정보를 반환합니다.\n",
    "state, info = env.reset()\n",
    "print(f\"초기 상태: {state}\") # 0부터 15까지의 정수 (4x4 격자에서 각 칸에 대응)\n",
    "print(f\"상태 공간 크기: {env.observation_space.n}\") # 4x4 = 16\n",
    "print(f\"행동 공간 크기: {env.action_space.n}\") # 상, 하, 좌, 우 = 4\n",
    "\n",
    "# 환경 렌더링 (시각화)\n",
    "env.render()\n",
    "time.sleep(1) # 잠시 대기하여 시각화된 화면 확인\n",
    "\n",
    "# 3. 환경과 상호작용 (임의의 행동)\n",
    "# 몇 가지 임의의 행동을 취하며 환경의 변화를 관찰합니다.\n",
    "# 행동: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\n",
    "print(\"\\n--- 임의의 행동 시퀀스 ---\")\n",
    "actions = [1, 1, 2, 2, 1] # 예시: DOWN, DOWN, RIGHT, RIGHT, DOWN\n",
    "\n",
    "for i, action in enumerate(actions):\n",
    "    print(f\"\\n--- {i+1}번째 행동: {action} ---\")\n",
    "    # env.step(action) 메서드는 행동을 취한 후의 새로운 상태, 보상, 에피소드 종료 여부 등을 반환합니다.\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "    clear_output(wait=True) # 이전 화면 지우기\n",
    "    env.render() # 현재 상태 렌더링\n",
    "    print(f\"현재 상태: {new_state}, 보상: {reward}, 종료 여부(terminated): {terminated}, 잘림 여부(truncated): {truncated}\")\n",
    "    time.sleep(1) # 잠시 대기\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(\"에피소드 종료! 환경을 다시 초기화합니다.\")\n",
    "        state, info = env.reset() # 에피소드가 종료되면 환경을 다시 초기화\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(1)\n",
    "        break # 시퀀스 종료\n",
    "\n",
    "# 4. 환경 닫기 (메모리 해제)\n",
    "env.close()\n",
    "print(\"\\n환경이 닫혔습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdbfff8-0934-4a0b-81e9-228317219ca1",
   "metadata": {},
   "source": [
    "네, `env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='ansi')` 이 한 줄 코드가 **`FrozenLake-v1` 환경을 설정하고 불러오는 핵심적인 부분**입니다. 각 부분이 어떤 의미를 가지는지 자세히 설명해 드릴게요.\r\n",
    "\r\n",
    "### `env = gym.make(...)`\r\n",
    "\r\n",
    "* `gym.make()`: Gymnasium 라이브러리에서 제공하는 함수로, 지정된 이름의 강화 학습 환경을 생성하는 역할을 합니다. 생성된 환경 객체는 보통 `env` 변수에 할당됩니다.\r\n",
    "\r\n",
    "### `'FrozenLake-v1'`\r\n",
    "\r\n",
    "* 이것은 우리가 사용할 **강화 학습 환경의 이름**입니다. Gymnasium은 다양한 환경을 내장하고 있으며, 각 환경은 고유한 이름을 가지고 있습니다.\r\n",
    "* `FrozenLake`는 얼어붙은 호수에서 목표 지점까지 가는 게임을 시뮬레이션하는 환경이고, `-v1`은 이 환경의 버전 번호를 나타냅니다. (버전이 다르면 환경의 설정이나 규칙이 약간 다를 수 있습니다.)\r\n",
    "\r\n",
    "### `is_slippery=False`\r\n",
    "\r\n",
    "* 이것은 `FrozenLake-v1` 환경에 특화된 **추가적인 설정(파라미터)**입니다.\r\n",
    "* **`is_slippery`**: 환경의 **\"미끄러움\"** 특성을 제어합니다.\r\n",
    "    * `True` (기본값): 에이전트가 특정 방향으로 이동을 시도해도, 1/3의 확률로 의도하지 않은 옆 방향으로 미끄러질 수 있습니다. 이것이 `FrozenLake` 환경의 핵심적인 **불확실성(확률적 요소)**입니다.\r\n",
    "    * **`False`**: 이 설정을 사용하면, 에이전트가 `상하좌우` 행동을 취했을 때 **항상 의도한 방향으로만 정확하게 이동**합니다. 즉, 미끄러짐이 없는 **결정론적인 환경**이 됩니다.\r\n",
    "* 우리가 초반에 이 설정을 `False`로 한 이유는, 복잡한 확률적 요소를 잠시 제외하고 **환경의 기본적인 작동 방식과 에이전트의 이동 경로를 더 직관적으로 이해**하기 위함입니다. 미끄러움이 없으면 미로 찾기가 훨씬 쉬워지겠죠?\r\n",
    "\r\n",
    "### `render_mode='ansi'`\r\n",
    "\r\n",
    "* 이것은 환경의 **시각화(렌더링) 방식**을 설정하는 파라미터입니다. 에이전트와 환경의 현재 상태를 우리가 눈으로 볼 수 있도록 출력하는 방식을 결정합니다.\r\n",
    "* **`'ansi'`**: 이 모드는 환경의 현재 상태를 **ANSI 이스케이프 코드**를 사용하여 텍스트 기반으로 출력합니다.\r\n",
    "    * Jupyter Lab의 출력 셀이나 일반 터미널에 **문자열 형태의 그림**처럼 표시됩니다. (예: `S F F F \\nF H F H \\nF F F H \\nH F F G`)\r\n",
    "    * **장점**: `pygame`과 같은 추가적인 그래픽 라이브러리 설치 없이도 환경의 변화를 확인할 수 있습니다. 서버 환경이나 원격 접속 환경에서 특히 유용합니다.\r\n",
    "    * **단점**: 시각적인 효과는 떨어집니다.\r\n",
    "\r\n",
    "* **`'human'` (이전 코드에서 사용)**: 이 모드는 별도의 **팝업 창**을 띄워서 환경을 그래픽적으로 보여줍니다.\r\n",
    "    * **장점**: 직관적이고 시각적으로 이해하기 좋습니다.\r\n",
    "    * **단점**: `pygame` 라이브러리가 필요하고, 일부 환경에서는 그래픽 드라이버나 X 서버 설정이 필요할 수 있어 설치가 번거로울 때가 있습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**요약하자면, 이 코드는:**\r\n",
    "\r\n",
    "\"**미끄럽지 않은** `FrozenLake-v1` 환경을 만들고, 이 환경의 현재 상태를 **텍스트 기반(ANSI)으로 화면에 표시**할 수 있도록 설정해라.\"\r\n",
    "\r\n",
    "라는 의미입니다. 이렇게 설정하면, 복잡한 그래픽 설정 없이도 환경의 변화를 실시간으로 확인하면서 강화 학습의 기본 동작을 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfc91f70-591f-4a0e-8c94-b9bec92014af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee8c16b0-503f-480f-b324-6d9142855a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "log=\"frozen_lake_simple_log.txt\"\n",
    "log_file=open(log,'w',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d69a2d9d-14ba-4992-b91a-587fef7e3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_print(messege):\n",
    "    print(messege)\n",
    "    log_file.write(messege+'\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a6c94c4-5f22-4f0f-88c5-3f9bbc571899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "환경 초기화: 초기 상태 = 0\n",
      "상태 공간 크기: 16, 행동 공간 크기: 4\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('FrozenLake-v1',is_slippery=False,render_mode='human')\n",
    "state,info=env.reset()\n",
    "log_print(f\"환경 초기화: 초기 상태 = {state}\")\n",
    "log_print(f\"상태 공간 크기: {env.observation_space.n}, 행동 공간 크기: {env.action_space.n}\")\n",
    "env.render()\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ab88feb-0555-4340-9562-9c649e781d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 임의의 행동 시퀀스 시작 ---\n"
     ]
    }
   ],
   "source": [
    "act=[1, 1, 2, 2, 1, 1, 0, 1]\n",
    "# 행동: 0=LEFT, 1=DOWN, 2=RIGHT, 3=UP\n",
    "log_print(\"\\n--- 임의의 행동 시퀀스 시작 ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee29a879-1a43-43eb-8ce3-9f08679e87b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> 새로운 상태: 13, 보상: 0.0, 종료 여부(terminated): False, 잘림 여부(truncated): False\n"
     ]
    }
   ],
   "source": [
    "for i, action in enumerate(act):\n",
    "    act_name={0: \"LEFT\", 1: \"DOWN\", 2: \"RIGHT\", 3: \"UP\"}[action]\n",
    "    log_print(f\"step {i+1}: actions={act_name}({action})\")\n",
    "\n",
    "    new_state,reward,terminated,truncate,info= env.step(action)\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "\n",
    "    log_print(f\"  -> 새로운 상태: {new_state}, 보상: {reward}, 종료 여부(terminated): {terminated}, 잘림 여부(truncated): {truncate}\")\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    if terminated or truncate:\n",
    "        log_print(\"에피소드 종료! 환경을 다시 초기화합니다.\")\n",
    "        clear_output(wait=True)\n",
    "        env.rander()\n",
    "        time.sleep(0.5)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "892ee296-22ea-44ef-b2d3-6b1b34c40b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 임의의 행동 시퀀스 종료 ---\n",
      "환경이 닫혔습니다.\n"
     ]
    }
   ],
   "source": [
    "log_print(\"--- 임의의 행동 시퀀스 종료 ---\")\n",
    "\n",
    "# 환경 닫기 (메모리 해제)\n",
    "env.close()\n",
    "log_print(\"환경이 닫혔습니다.\")\n",
    "\n",
    "# 로그 파일 닫기 (매우 중요!)\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cfd01c-6f5c-4104-b455-e446a0b35d19",
   "metadata": {},
   "source": [
    "네, `env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='ansi')` 이 한 줄 코드가 **`FrozenLake-v1` 환경을 설정하고 불러오는 핵심적인 부분**입니다. 각 부분이 어떤 의미를 가지는지 자세히 설명해 드릴게요.\r\n",
    "\r\n",
    "### `env = gym.make(...)`\r\n",
    "\r\n",
    "* `gym.make()`: Gymnasium 라이브러리에서 제공하는 함수로, 지정된 이름의 강화 학습 환경을 생성하는 역할을 합니다. 생성된 환경 객체는 보통 `env` 변수에 할당됩니다.\r\n",
    "\r\n",
    "### `'FrozenLake-v1'`\r\n",
    "\r\n",
    "* 이것은 우리가 사용할 **강화 학습 환경의 이름**입니다. Gymnasium은 다양한 환경을 내장하고 있으며, 각 환경은 고유한 이름을 가지고 있습니다.\r\n",
    "* `FrozenLake`는 얼어붙은 호수에서 목표 지점까지 가는 게임을 시뮬레이션하는 환경이고, `-v1`은 이 환경의 버전 번호를 나타냅니다. (버전이 다르면 환경의 설정이나 규칙이 약간 다를 수 있습니다.)\r\n",
    "\r\n",
    "### `is_slippery=False`\r\n",
    "\r\n",
    "* 이것은 `FrozenLake-v1` 환경에 특화된 **추가적인 설정(파라미터)**입니다.\r\n",
    "* **`is_slippery`**: 환경의 **\"미끄러움\"** 특성을 제어합니다.\r\n",
    "    * `True` (기본값): 에이전트가 특정 방향으로 이동을 시도해도, 1/3의 확률로 의도하지 않은 옆 방향으로 미끄러질 수 있습니다. 이것이 `FrozenLake` 환경의 핵심적인 **불확실성(확률적 요소)**입니다.\r\n",
    "    * **`False`**: 이 설정을 사용하면, 에이전트가 `상하좌우` 행동을 취했을 때 **항상 의도한 방향으로만 정확하게 이동**합니다. 즉, 미끄러짐이 없는 **결정론적인 환경**이 됩니다.\r\n",
    "* 우리가 초반에 이 설정을 `False`로 한 이유는, 복잡한 확률적 요소를 잠시 제외하고 **환경의 기본적인 작동 방식과 에이전트의 이동 경로를 더 직관적으로 이해**하기 위함입니다. 미끄러움이 없으면 미로 찾기가 훨씬 쉬워지겠죠?\r\n",
    "\r\n",
    "### `render_mode='ansi'`\r\n",
    "\r\n",
    "* 이것은 환경의 **시각화(렌더링) 방식**을 설정하는 파라미터입니다. 에이전트와 환경의 현재 상태를 우리가 눈으로 볼 수 있도록 출력하는 방식을 결정합니다.\r\n",
    "* **`'ansi'`**: 이 모드는 환경의 현재 상태를 **ANSI 이스케이프 코드**를 사용하여 텍스트 기반으로 출력합니다.\r\n",
    "    * Jupyter Lab의 출력 셀이나 일반 터미널에 **문자열 형태의 그림**처럼 표시됩니다. (예: `S F F F \\nF H F H \\nF F F H \\nH F F G`)\r\n",
    "    * **장점**: `pygame`과 같은 추가적인 그래픽 라이브러리 설치 없이도 환경의 변화를 확인할 수 있습니다. 서버 환경이나 원격 접속 환경에서 특히 유용합니다.\r\n",
    "    * **단점**: 시각적인 효과는 떨어집니다.\r\n",
    "\r\n",
    "* **`'human'` (이전 코드에서 사용)**: 이 모드는 별도의 **팝업 창**을 띄워서 환경을 그래픽적으로 보여줍니다.\r\n",
    "    * **장점**: 직관적이고 시각적으로 이해하기 좋습니다.\r\n",
    "    * **단점**: `pygame` 라이브러리가 필요하고, 일부 환경에서는 그래픽 드라이버나 X 서버 설정이 필요할 수 있어 설치가 번거로울 때가 있습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**요약하자면, 이 코드는:**\r\n",
    "\r\n",
    "\"**미끄럽지 않은** `FrozenLake-v1` 환경을 만들고, 이 환경의 현재 상태를 **텍스트 기반(ANSI)으로 화면에 표시**할 수 있도록 설정해라.\"\r\n",
    "\r\n",
    "라는 의미입니다. 이렇게 설정하면, 복잡한 그래픽 설정 없이도 환경의 변화를 실시간으로 확인하면서 강화 학습의 기본 동작을 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bf0533-3760-4768-b0b0-ef492803b90c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Environment",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
