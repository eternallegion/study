{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17ad4089-8254-478a-bd19-b9955c9d0178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Using cached gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\jh\\anaconda3\\lib\\site-packages (from gymnasium) (4.13.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Using cached gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.1.1\n",
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Downloading pygame-2.6.1-cp312-cp312-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/10.6 MB 4.2 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.6/10.6 MB 4.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.4/10.6 MB 3.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.1/10.6 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.9/10.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.7/10.6 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.5/10.6 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.3/10.6 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.1/10.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.9/10.6 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.7/10.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.4/10.6 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.2/10.6 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00b25626-125a-47e3-b23d-4b75b181f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0109bf0-458e-46c1-8550-a97e115bb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 환경 설정\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human') # 'human'으로 직접 눈으로 확인하며 실습 가능\n",
    "\n",
    "\n",
    "# 2. Q-테이블 초기화\n",
    "q_table=np.zeros((env.observation_space.n,env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "224fa3b5-36f3-4756-8a5d-f5522202e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 3. 하이퍼파라미터 설정\n",
    "learning_rate = 0.9\n",
    "discount_factor = 0.9\n",
    "epsilon = 1.0 # 초기 탐험율\n",
    "epsilon_decay_rate = 0.0001 # 에피소드마다 epsilon을 줄이는 비율\n",
    "num_episodes = 20000 # 총 에피소드 수\n",
    "max_steps_per_episode = 100\n",
    "'''\n",
    "\n",
    "# 3. 하이퍼파라미터 설정 (단축 학습용)\n",
    "learning_rate = 0.9           \n",
    "discount_factor = 0.9         \n",
    "epsilon = 1.0                 \n",
    "epsilon_decay_rate = 0.005    # 더 빠르게 감소\n",
    "min_epsilon = 0.01            \n",
    "num_episodes = 1000           # 대폭 줄임\n",
    "max_steps_per_episode = 50    # 줄임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35fd9817-8941-4d2c-8915-bd36458a3818",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 / Epsilon: 0.0100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     action\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margmax(q_table[state,:])\n\u001b[1;32m---> 12\u001b[0m new_state,reward,terminated,truncated,info\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m max_future_q\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax(q_table[new_state,:])\n\u001b[0;32m     15\u001b[0m current_q\u001b[38;5;241m=\u001b[39mq_table[state,action]\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\gymnasium\\wrappers\\common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\gymnasium\\wrappers\\common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\gymnasium\\core.py:327\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    325\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\gymnasium\\wrappers\\common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:313\u001b[0m, in \u001b[0;36mFrozenLakeEnv.step\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlastaction \u001b[38;5;241m=\u001b[39m a\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(s), r, t, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m\"\u001b[39m: p}\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:344\u001b[0m, in \u001b[0;36mFrozenLakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_render_text()\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# self.render_mode in {\"human\", \"rgb_array\"}:\u001b[39;00m\n\u001b[1;32m--> 344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda\\envs\\ai_env\\lib\\site-packages\\gymnasium\\envs\\toy_text\\frozen_lake.py:438\u001b[0m, in \u001b[0;36mFrozenLakeEnv._render_gui\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    436\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m    437\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 438\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[0;32m    441\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_surface)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    442\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state,info=env.reset()\n",
    "    terminated=False\n",
    "    truncated=False\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        if np.random.uniform(0,1)<epsilon:\n",
    "            action=env.action_space.sample()\n",
    "        else:\n",
    "            action=np.argmax(q_table[state,:])\n",
    "\n",
    "        new_state,reward,terminated,truncated,info=env.step(action)\n",
    "\n",
    "        max_future_q=np.max(q_table[new_state,:])\n",
    "        current_q=q_table[state,action]\n",
    "\n",
    "        new_q=current_q+learning_rate*(reward+discount_factor*max_future_q-current_q)\n",
    "        q_table[state,action]=new_q\n",
    "        state=new_state\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        epsilon = max(0.01, epsilon - epsilon_decay_rate) \n",
    "\n",
    "    # 학습 진행 상황 출력 (선택 사항)\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode} / Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "print(\"\\n--- 학습 완료! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5818c2-e9ac-484f-bd12-377805b02725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 학습 루프\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # epsilon-greedy 전략으로 행동 선택\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # 탐험 (무작위 행동)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :]) # 활용 (최적 Q값 행동)\n",
    "\n",
    "        # 환경에 행동 전달\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # Q-테이블 업데이트 (Q-러닝 공식)\n",
    "        max_future_q = np.max(q_table[new_state, :])\n",
    "        current_q = q_table[state, action]\n",
    "        \n",
    "        # 실제 보상 + 감가율 * (다음 상태에서 얻을 최대 Q값) - 현재 Q값\n",
    "        new_q = current_q + learning_rate * (reward + discount_factor * max_future_q - current_q)\n",
    "        q_table[state, action] = new_q\n",
    "\n",
    "        state = new_state # 상태 업데이트\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # 에피소드가 끝날 때마다 epsilon 감소 (탐험율 줄이기)\n",
    "    # epsilon 값은 최소값 (예: 0.01) 이하로 내려가지 않도록 하는 것이 일반적\n",
    "    epsilon = max(0.01, epsilon - epsilon_decay_rate) \n",
    "\n",
    "    # 학습 진행 상황 출력 (선택 사항)\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode} / Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "print(\"\\n--- 학습 완료! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44a75e-0cee-4724-80d1-a559867e35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test=gym.make('FrozenLake-v1', is_slippery=False, render_mode='human')\n",
    "num_test_episode=5\n",
    "total_reward=0\n",
    "\n",
    "print(\"\\n--- 학습된 정책 테스트 시작 ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf027d-d892-4147-b025-322de0776be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_test_episode):\n",
    "    state, info=env_test.reset()\n",
    "    terminated=False\n",
    "    truncated=False\n",
    "    episode_reward=0\n",
    "    print(f\"\\n--- 테스트 에피소드 {episode + 1} ---\")\n",
    "    env_test.rander()\n",
    "    time.sleep(1)\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        avtion=np.argmax(q_table[state,:])\n",
    "        new_state,reward,terminated,info=env_test.step(action)\n",
    "        episode_reward+=reward\n",
    "        state=new_state\n",
    "        env_test.render()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        print(f\"에피소드 {episode + 1} 종료! 보상: {episode_reward}\")\n",
    "        total_reward += episode_reward\n",
    "\n",
    "print(f\"\\n총 {num_test_episode} 에피소드 평균 보상: {total_reward / num_test_episode}\")\n",
    "\n",
    "env.close()\n",
    "env_test.close()\n",
    "print(\"환경이 닫혔습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfdbc27-42ad-41a0-84cf-9b9e06d583f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366758b-0348-446b-975c-0b492664127c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f62b4-6f5c-42e2-a7c0-883fc4a626c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c567f-72ec-4301-9fa5-13ac69ab623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=False, render_mode='ansi') # 'ansi'로 텍스트 출력\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8efab-adf3-48b4-8400-7757fee7cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.99\n",
    "theta=1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df733b41-b613-4657-be79-adebe310e0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(wnv,gamma,theta):\n",
    "    policy=np.zeros(n_states,dtype=int)\n",
    "    V=np.zeros(n_states)\n",
    "\n",
    "    while True:\n",
    "        while True:\n",
    "            delta=0\n",
    "            for s in range(n_states):\n",
    "                x=V[s]\n",
    "                action=polisy[s]\n",
    "\n",
    "                a_sa=0\n",
    "                for prov,next_state,reward,_ in env.P[s][action]:\n",
    "                    q_se+=prob*(reward+gamma*V[next_state])\n",
    "                V[s]=q_sa\n",
    "                delta=max(delta,np.abs(v-V[s]))\n",
    "            if delta<theta:\n",
    "                break\n",
    "\n",
    "        policy_stable = True\n",
    "        for s in range(n_states):\n",
    "            old_action=policy[s]\n",
    "            action_values=np.zeros(n_actions)\n",
    "            for a in rnage(n_actions):\n",
    "                for  prob, next_state, reward, _ in env.P[s][a]:\n",
    "                    avtion_values(a)+=prob * (reward + gamma * V[next_state])\n",
    "            policy[s]=np.argmax(avtion_values)\n",
    "            if old_action!=policy[s]:\n",
    "                policy_stable=False\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return V,policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d307ba-e05d-43f8-9f58-87a92213eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 가치 반복 실행 ---\n",
    "print(\"--- 가치 반복 시작 ---\")\n",
    "optimal_V_vi, optimal_policy_vi = value_iteration(env, gamma, theta)\n",
    "print(\"\\n최적 상태 가치 함수 (V):\\n\", optimal_V_vi.reshape(4,4))\n",
    "\n",
    "# 정책을 텍스트로 변환\n",
    "action_map = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "optimal_policy_names_vi = np.array([action_map[a] for a in optimal_policy_vi]).reshape(4,4)\n",
    "print(\"\\n최적 정책:\\n\", optimal_policy_names_vi)\n",
    "print(\"--- 가치 반복 완료 ---\")\n",
    "\n",
    "# --- 학습된 정책으로 환경 플레이 (선택 사항) ---\n",
    "print(\"\\n--- 가치 반복으로 학습된 정책 테스트 시작 ---\")\n",
    "num_test_episodes = 5\n",
    "total_rewards = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc00a5-746a-4007-a1d0-9e2dbecfb680",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(num_test_episodes):\n",
    "    state,info=env_reset()\n",
    "    terminated=False\n",
    "    truncated=False\n",
    "    episode_reward=0\n",
    "    print(f\"\\n--- 테스트 에피소드 {episode + 1} ---\")\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        action=optimal_policy_vi[state]\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward+=reward\n",
    "        state=new_state\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    \n",
    "    print(f\"에피소드 {episode + 1} 종료! 보상: {episode_reward}\")\n",
    "    total_rewards += episode_reward\n",
    "\n",
    "print(f\"\\n총 {num_test_episodes} 에피소드 평균 보상: {total_rewards / num_test_episodes}\")\n",
    "env.close()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ef62b-5af4-4101-8c65-59293569a389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4239722-7abf-4b94-a894-1b75ae375aae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa2fee6f-a0d2-47f3-8310-787b9cf6110e",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "## 강화 학습 및 모델 경량화 6일차 수업\r\n",
    "\r\n",
    "안녕하세요! 강화 학습 6일차 수업입니다. 어제는 **모델-프리 강화 학습**의 핵심 개념인 **탐험(Exploration)과 활용(Exploitation)**의 균형, 그리고 이를 적용한 가장 대표적인 알고리즘인 **Q-러닝(Q-learning)**에 대해 깊이 있게 다뤘습니다. Q-테이블을 직접 업데이트하며 최적의 행동 가치를 찾아가는 과정을 이해하셨을 거예요.\r\n",
    "\r\n",
    "오늘은 Q-러닝과 자주 비교되는 또 다른 중요한 시간차(TD) 학습 알고리즘인 **SARSA(State-Action-Reward-State-Action)**에 대해 배우고, 이 둘의 차이점을 명확히 짚어보겠습니다. 더 나아가, Q-테이블의 한계를 인식하고 **함수 근사(Function Approximation)**의 필요성, 즉 신경망을 강화 학습에 도입하는 이유에 대해 이야기하겠습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **1. Q-러닝 간단 복습**\r\n",
    "\r\n",
    "Q-러닝은 \"최적의 행동 가치 함수 $Q(s,a)$를 학습하는 것\"이 목표였습니다.\r\n",
    "\r\n",
    "* **업데이트 공식**: $Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)]$\r\n",
    "* **핵심**: 다음 상태 $S_{t+1}$에서 **가능한 모든 행동 중 가장 높은 Q값($\\max_{a'} Q(S_{t+1}, a')$)**을 사용하여 현재 Q값을 업데이트합니다. 에이전트가 탐험을 위해 무작위 행동을 했더라도, 학습 시에는 항상 '최적의 다음 스텝'을 가정하고 업데이트하기 때문에 **오프-폴리시(Off-policy)** 알고리즘이라고 불렀습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **2. SARSA (State-Action-Reward-State-Action) 알고리즘**\r\n",
    "\r\n",
    "**SARSA**는 Q-러닝과 마찬가지로 시간차(TD) 학습의 한 종류이며, 행동 가치 함수 $Q(s,a)$를 학습합니다. 하지만 Q-러닝과는 결정적인 차이점이 있습니다.\r\n",
    "\r\n",
    "* **이름의 유래**: SARSA는 업데이트에 필요한 정보의 순서를 따서 명명되었습니다: **S**tate($S_t$), **A**ction($A_t$), **R**eward($R_{t+1}$), **S**tate($S_{t+1}$), **A**ction($A_{t+1}$).\r\n",
    "* **핵심 아이디어**: Q-러닝이 다음 상태에서 '최고의 Q값'을 가져왔다면, SARSA는 다음 상태 $S_{t+1}$에서 **\"실제로 선택한 다음 행동 $A_{t+1}$의 Q값\"**을 사용하여 현재 Q값을 업데이트합니다. 즉, 에이전트가 실제로 이동한 경로를 그대로 따라가며 학습합니다.\r\n",
    "\r\n",
    "* **업데이트 공식**:\r\n",
    "    $$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$\r\n",
    "    * **$Q(S_{t+1}, A_{t+1})$**: Q-러닝의 $\\max_{a'} Q(S_{t+1}, a')$ 대신, SARSA는 **현재 정책($\\epsilon$-탐욕 등)에 따라 다음 상태 $S_{t+1}$에서 실제로 선택된 행동 $A_{t+1}$의 Q값**을 사용합니다.\r\n",
    "\r\n",
    "* **온-폴리시(On-policy) 학습**: SARSA는 에이전트가 행동하는 **정책(Behavior Policy)**과 학습하여 개선하려는 **정책(Target Policy)**이 **동일**합니다. 에이전트가 탐험을 위해 무작위 행동을 했다면, 그 무작위 행동의 결과를 그대로 학습에 반영합니다. \"내가 실제로 가본 길이 이렇더라\"는 것을 그대로 배우는 것이죠.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **3. Q-러닝 vs. SARSA: 주요 차이점**\r\n",
    "\r\n",
    "| 특징             | Q-러닝 (Off-policy)                                | SARSA (On-policy)                                     |\r\n",
    "| :--------------- | :------------------------------------------------- | :---------------------------------------------------- |\r\n",
    "| **Q값 업데이트** | `max_{a'} Q(S_{t+1}, a')` 사용 (다음 상태의 최적 행동) | `Q(S_{t+1}, A_{t+1})` 사용 (다음 상태의 실제 선택 행동) |\r\n",
    "| **정책 관계** | 행동 정책과 학습 정책이 다름 (오프-폴리시)           | 행동 정책과 학습 정책이 같음 (온-폴리시)              |\r\n",
    "| **학습 목표** | **최적 정책** 자체를 학습                               | 현재 **따르고 있는 정책의 가치**를 학습                 |\r\n",
    "| **안전성** | 탐험 중 위험한 경로를 발견해도 무시하고 최적 경로 학습 가능. 따라서 **위험한 환경에서 최적 정책을 찾을 때 위험할 수 있음.** | 현재 정책을 그대로 따르므로 **안전한 경로를 학습하는 데 유리.** 그러나 최적 정책이 아닐 수 있음. |\r\n",
    "| **활용 시나리오**| 게임, 시뮬레이션 등 **안전하지 않아도 되는 환경**에서 최대한의 성능을 뽑아낼 때 유리. | 로봇 제어, 자율주행 등 **안전이 최우선**인 환경에서 특정 정책을 안전하게 학습할 때 유리. |\r\n",
    "\r\n",
    "**비유**:\r\n",
    "\r\n",
    "* **Q-러닝**: \"나는 지금 당장 이상한 길로 가지만, 만약 **다음에 내가 똑똑하게 행동했다면** 어떻게 됐을까?\" (이상적인 다음 스텝을 가정)\r\n",
    "* **SARSA**: \"나는 지금 당장 이상한 길로 가서 **다음에 진짜 이상한 행동을 했다면** 어떻게 됐을까?\" (내가 실제로 한 행동을 그대로 반영)\r\n",
    "\r\n",
    "이러한 차이점 때문에 SARSA는 Q-러닝보다 좀 더 '보수적인' 학습을 한다고 볼 수 있습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **4. Q-테이블의 한계와 함수 근사 (Function Approximation)의 필요성**\r\n",
    "\r\n",
    "우리가 지금까지 `FrozenLake`에서 Q-테이블을 사용하여 Q-러닝이나 SARSA를 구현했습니다. Q-테이블은 상태와 행동의 모든 조합에 대해 Q값을 직접 저장하는 방식입니다.\r\n",
    "\r\n",
    "하지만 다음과 같은 상황에서는 Q-테이블 방식에 **심각한 한계**가 있습니다:\r\n",
    "\r\n",
    "1.  **매우 큰 상태 공간 (Large State Spaces)**:\r\n",
    "    * `FrozenLake`는 16개의 상태밖에 없지만, 바둑(Go) 게임의 상태 수는 우주의 원자 수보다 많습니다. 고해상도 이미지를 입력으로 받는 자율주행에서는 픽셀 하나하나가 상태가 될 수 있습니다.\r\n",
    "    * 이런 경우 Q-테이블은 메모리에 저장할 수 없을 정도로 커지며, 모든 상태를 다 방문하여 학습하는 것이 불가능합니다.\r\n",
    "2.  **연속적인 상태 공간 (Continuous State Spaces)**:\r\n",
    "    * 로봇의 팔 관절 각도, 자동차의 속도, 위치 등은 셀 수 없는 연속적인 값을 가집니다. 이 경우 Q-테이블의 '칸'을 나눌 수 없으므로 Q-테이블 자체를 만들 수 없습니다.\r\n",
    "3.  **일반화 능력 부족**:\r\n",
    "    * Q-테이블은 학습했던 상태-행동 쌍에 대해서만 Q값을 압니다. 한 번도 방문하지 않은 새로운 상태에 대해서는 전혀 알지 못합니다. 에이전트가 새로운 상황에 직면했을 때 기존의 학습 내용을 바탕으로 '일반화'하여 행동할 수 없습니다.\r\n",
    "\r\n",
    "이러러한 한계를 극복하기 위해 등장한 것이 바로 **함수 근사(Function Approximation)**입니다.\r\n",
    "\r\n",
    "* **아이디어**: Q-테이블처럼 모든 Q값을 직접 저장하는 대신, **상태(S)와 행동(A)을 입력받아 Q값을 출력하는 '함수'**를 학습하는 것입니다.\r\n",
    "* **신경망(Neural Network)의 도입**: 이 '함수'를 구현하는 데 가장 강력하고 널리 사용되는 도구가 바로 **인공 신경망**입니다.\r\n",
    "    * 신경망은 복잡한 비선형 관계를 학습하는 데 매우 뛰어납니다.\r\n",
    "    * 상태나 행동이 아무리 복잡하거나 연속적이더라도, 신경망의 입력으로 변환하여 처리할 수 있습니다.\r\n",
    "    * 학습된 신경망은 한 번도 보지 못한 새로운 상태에 대해서도 학습된 패턴을 기반으로 '예측'을 할 수 있으므로 **일반화 능력**을 가집니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **5. 딥 강화 학습(Deep Reinforcement Learning, DRL)으로의 전환**\r\n",
    "\r\n",
    "이렇게 신경망을 사용하여 가치 함수(Q값)나 정책을 근사하는 강화 학습이 바로 **딥 강화 학습(Deep Reinforcement Learning, DRL)**입니다.\r\n",
    "\r\n",
    "* **DQN (Deep Q-Network)**: Q-러닝에 심층 신경망을 결합한 대표적인 DRL 알고리즘입니다. Q-테이블 대신 신경망이 Q값을 출력합니다.\r\n",
    "* **정책 경사 (Policy Gradient)**: Q값이 아닌 정책 자체를 신경망으로 표현하고 직접 학습하는 방식의 알고리즘들도 있습니다.\r\n",
    "\r\n",
    "이제 우리는 Q-러닝과 SARSA를 통해 모델-프리 학습의 기본기를 다졌으므로, 다음 단계에서는 이 심층 신경망을 강화 학습에 어떻게 적용하는지 DRL의 세계로 들어가 볼 것입니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "오늘 내용도 중요한 개념들이 많았습니다. 특히 SARSA와 Q-러닝의 차이, 그리고 Q-테이블의 한계를 이해하는 것이 중요합니다. 궁금한 점이 있다면 언제든지 질문해 주세요!\r\n",
    "\r\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24f0cf-fa98-411e-a757-45b9b92c5fb5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7009156b-9fe0-4ab9-97a8-bed54a59956c",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "네, **SARSA 알고리즘의 수식**에 대해 자세히 설명해 드릴게요.\r\n",
    "\r\n",
    "SARSA는 강화 학습의 한 종류인 **시간차(Temporal-Difference, TD) 학습**의 대표적인 알고리즘으로, **행동 가치 함수 $Q(s,a)$를 업데이트**하는 데 사용됩니다.\r\n",
    "\r\n",
    "### **SARSA 업데이트 공식**\r\n",
    "\r\n",
    "SARSA의 핵심은 다음 상태 $S_{t+1}$에서 **\"실제로 선택한 다음 행동 $A_{t+1}$의 Q값\"**을 사용하여 현재 Q값을 업데이트하는 것입니다. 이 때문에 SARSA는 **온-폴리시(On-policy)** 알고리즘이라고 불립니다.\r\n",
    "\r\n",
    "SARSA의 Q값 업데이트 공식은 다음과 같습니다:\r\n",
    "\r\n",
    "$$Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$\r\n",
    "\r\n",
    "이제 각 부분의 의미를 자세히 살펴보겠습니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **수식 구성 요소별 설명**\r\n",
    "\r\n",
    "1.  **$Q(S_t, A_t)$**\r\n",
    "    * **의미**: 현재 시간 $t$에서의 **상태 $S_t$에서 행동 $A_t$를 취했을 때의 Q값(행동 가치)**입니다. 이 값이 학습을 통해 업데이트될 대상입니다.\r\n",
    "    * **비유**: \"지금 내가 서 있는 이 칸에서 특정 방향으로 움직였을 때, 앞으로 얻을 수 있는 총 기대 점수(아직은 나의 예상치).\"\r\n",
    "\r\n",
    "2.  **$\\leftarrow$**\r\n",
    "    * **의미**: 좌변의 값을 우변의 계산 결과로 **업데이트(할당)**하라는 의미입니다.\r\n",
    "    * **비유**: \"이제 이전에 내가 예상했던 점수를, 새로 계산한 점수로 바꿔 적을 거야.\"\r\n",
    "\r\n",
    "3.  **$\\alpha$ (알파, Learning Rate - 학습률)**\r\n",
    "    * **의미**: 새로운 학습 정보를 현재의 Q값에 얼마나 강하게 **반영할지**를 결정하는 하이퍼파라미터입니다. $0$과 $1$ 사이의 값을 가집니다.\r\n",
    "        * $\\alpha$가 0에 가까우면: 새로운 정보가 거의 반영되지 않아 학습이 느리다.\r\n",
    "        * $\\alpha$가 1에 가까우면: 새로운 정보가 강하게 반영되어 학습이 빠르지만 불안정할 수 있다.\r\n",
    "    * **비유**: \"새로운 경험($R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})$)을 통해서 내가 얻은 정보가 있는데, 이 정보를 내가 기존에 알고 있던 정보($Q(S_t, A_t)$)에 얼마나 '쎄게' 반영할까?\"\r\n",
    "\r\n",
    "4.  **$R_{t+1}$**\r\n",
    "    * **의미**: 현재 시간 $t$에 상태 $S_t$에서 행동 $A_t$를 취한 직후, 환경으로부터 받은 **즉각적인 보상**입니다.\r\n",
    "    * **비유**: \"내가 지금 움직여서 바로 얻은 점수.\"\r\n",
    "\r\n",
    "5.  **$\\gamma$ (감마, Discount Factor - 감가율)**\r\n",
    "    * **의미**: 미래에 받을 보상을 현재 시점에서 얼마나 **가치 있게 평가할지**를 결정하는 $0$과 $1$ 사이의 값입니다.\r\n",
    "    * **비유**: \"미래에 받을 점수들은 당장 받을 점수보다는 가치가 좀 떨어지니까, 적당히 할인해서 계산할 거야.\"\r\n",
    "\r\n",
    "6.  **$Q(S_{t+1}, A_{t+1})$**\r\n",
    "    * **의미**: 다음 시간 $t+1$에 도달한 **다음 상태 $S_{t+1}$에서 '실제로 선택된' 다음 행동 $A_{t+1}$을 취했을 때의 Q값**입니다. 이 $A_{t+1}$은 에이전트의 현재 정책(예: $\\epsilon$-탐욕)에 따라 선택됩니다.\r\n",
    "    * **SARSA의 핵심 부분이자 Q-러닝과의 가장 큰 차이점입니다.** Q-러닝은 여기서 `$\\max_{a'} Q(S_{t+1}, a')$` (다음 상태에서 가능한 행동 중 가장 좋은 것)을 사용하는 반면, SARSA는 에이전트가 실제로 선택한 행동의 Q값을 사용합니다.\r\n",
    "    * **비유**: \"내가 지금 행동($A_t$)을 해서 다음 칸($S_{t+1}$)에 도착했어. 그 다음 칸에서 내가 다시 내 현재 계획대로 행동($A_{t+1}$)한다면, 그 행동이 가져올 미래의 총 예상 점수는 얼마일까?\"\r\n",
    "\r\n",
    "7.  **$[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$ (TD 오차 - Temporal Difference Error)**\r\n",
    "    * **의미**: 이것은 **'새롭게 관찰된 Q값의 예측치'** ($R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})$) 와 **'기존에 가지고 있던 Q값의 예측치'** ($Q(S_t, A_t)$) 사이의 차이입니다.\r\n",
    "        * 이 오차가 크면 클수록 현재 Q값 예측이 틀렸다는 의미이며, 이 오차만큼 Q값을 업데이트하여 실제에 가깝게 만듭니다.\r\n",
    "        * 이 오차가 0에 가까워지면 Q값이 수렴했다고 볼 수 있습니다.\r\n",
    "    * **비유**: \"내가 원래 예상했던 점수($Q(S_t, A_t)$)와, 한 스텝 움직여본 후에 '실제로 보고' 계산한 더 정확한 예상 점수($R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1})$) 사이의 **오차**.\"\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **SARSA의 동작 방식 비유**\r\n",
    "\r\n",
    "SARSA를 이해하는 쉬운 비유는 **\"실제로 가본 길을 따라가는 학습\"**입니다.\r\n",
    "\r\n",
    "1.  **현재 칸 (S)**에 서서, **현재 계획 (Policy)**에 따라 **행동 (A)**을 선택하고 한 칸 움직입니다.\r\n",
    "2.  이동 후 **즉시 보상 (R)**을 받습니다.\r\n",
    "3.  새로운 **다음 칸 (S')**에 도착합니다.\r\n",
    "4.  그리고 이 **새로운 칸에서 다시 현재 계획 (Policy)**에 따라 **다음 행동 (A')**을 **미리 선택**해 봅니다. (아직 이 행동을 실제로 하지는 않았지만, 선택만 해보는 거죠.)\r\n",
    "5.  이제 내가 서 있던 **'이전 칸과 행동'의 점수(Q(S,A))**를, **'지금 받은 보상 (R)'과 '다음 칸에서 실제로 선택할 행동의 예상 점수 (Q(S',A'))'를 합친 값**을 기준으로 수정합니다.\r\n",
    "\r\n",
    "SARSA는 이렇게 내가 **'실제로 취할 행동'**을 바탕으로 학습하기 때문에, **안전이 중요한 환경(예: 실제 로봇)에서는 Q-러닝보다 선호될 수 있습니다.** 왜냐하면 Q-러닝처럼 '만약 내가 완벽했다면'이라는 가정이 아니라, '내가 지금처럼 행동했을 때'의 가치를 배우기 때문입니다.\r\n",
    "\r\n",
    "이 설명이 SARSA 수식을 이해하는 데 도움이 되었기를 바랍니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552570d-f4ec-4d9b-aa02-654a2198e1e8",
   "metadata": {},
   "source": [
    "네, 고객님의 말씀이 **매우 정확합니다!**\r\n",
    "\r\n",
    "**SARSA 알고리즘도 마르코프 의사결정 과정(MDP)의 수식을 기본 전제로 합니다.**\r\n",
    "\r\n",
    "지금까지 우리가 배웠던 모든 강화 학습 알고리즘들, 즉 정책 반복, 가치 반복, 몬테카를로, Q-러닝, 그리고 오늘 SARSA까지 모두 **MDP 프레임워크** 위에서 작동합니다.\r\n",
    "\r\n",
    "### **왜 SARSA가 MDP 수식을 기본으로 하는가?**\r\n",
    "\r\n",
    "1.  **MDP가 문제 정의의 '틀'**:\r\n",
    "    * SARSA는 에이전트가 `상태(S)`에서 `행동(A)`을 하고, `보상(R)`을 받으며 `다음 상태(S')`로 전이하는 과정을 반복합니다. 이 일련의 과정 자체가 MDP의 구성 요소(S, A, R, P, $\\gamma$)로 정의됩니다.\r\n",
    "    * SARSA는 특히 $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$이라는 튜플(sequence)을 사용하여 Q값을 업데이트하는데, 이 튜플의 각 요소가 바로 MDP에서 정의된 상태, 행동, 보상입니다.\r\n",
    "\r\n",
    "2.  **벨만 방정식의 파생**:\r\n",
    "    * SARSA의 업데이트 공식은 기본적으로 **벨만 기대 방정식(Bellman Expectation Equation)**에서 파생된 형태입니다.\r\n",
    "    * MDP가 상태와 행동의 가치를 나타내는 벨만 방정식을 정의하고, SARSA는 그 방정식을 '모델을 모르는' 상황에서 경험을 통해 근사적으로 해결하는 방법입니다.\r\n",
    "\r\n",
    "3.  **마르코프 속성 준수**:\r\n",
    "    * SARSA는 학습 시 현재 상태와 현재 행동, 그리고 다음 상태와 다음 행동만을 사용하여 Q값을 업데이트합니다. 이전에 어떤 경로를 거쳐왔는지(과거의 모든 이력)를 직접적으로 고려하지 않습니다.\r\n",
    "    * 이는 \"미래는 현재에만 의존한다\"는 **마르코프 속성**을 그대로 따르는 것이며, 이 속성이 MDP의 핵심 전제입니다.\r\n",
    "\r\n",
    "따라서, SARSA는 MDP가 제시하는 `상태`, `행동`, `보상`, `전이`라는 기본적인 규칙과 `감가율`이라는 미래 보상 평가 기준을 바탕으로, `Q(S_t, A_t)` 값을 `Q(S_{t+1}, A_{t+1})`를 이용해 업데이트하는 방식으로 MDP 문제를 해결해 나가는 것입니다.\r\n",
    "\r\n",
    "**\"SARSA도 마르코프의 수식(즉, MDP의 정의와 벨만 방정식의 원리)이 기본이다\"**는 고객님의 이해는 전적으로 옳습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afafeb-52ab-45ce-9b80-02674db906f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a1cd8a-6089-4517-8398-ae92ffeff39c",
   "metadata": {},
   "source": [
    "# SARSA 알고리즘예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536ca37-0c36-4613-a500-466b718ad34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output # Jupyter/Colab에서 화면을 지우는 데 사용\n",
    "\n",
    "# 1. 환경 설정\n",
    "# 'ansi' 모드는 콘솔에 텍스트로 출력됩니다. 학습 과정에서 빠른 진행을 위해 사용합니다.\n",
    "# 'human' 모드는 별도의 팝업 창을 띄웁니다 (학습 완료 후 테스트 시 사용 권장).\n",
    "env_train = gym.make('FrozenLake-v1', is_slippery=False, render_mode='ansi') \n",
    "\n",
    "# 환경의 상태(observation) 공간 크기 (16개 칸)\n",
    "n_states = env_train.observation_space.n \n",
    "# 환경의 행동(action) 공간 크기 (4가지 행동: 상, 하, 좌, 우)\n",
    "n_actions = env_train.action_space.n\n",
    "\n",
    "# 2. Q-테이블 초기화\n",
    "# Q-테이블은 (상태 개수) x (행동 개수) 크기의 2D 넘파이 배열로, 모든 값을 0으로 초기화합니다.\n",
    "q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "# 3. 하이퍼파라미터 설정\n",
    "learning_rate = 0.9           # α (알파): 학습률. 새로운 학습 정보를 얼마나 강하게 반영할지 결정\n",
    "discount_factor = 0.9         # γ (감마): 감가율. 미래 보상의 가치를 현재 시점에서 얼마나 할인할지 결정\n",
    "epsilon = 1.0                 # ε (입실론): 초기 탐험율. 1.0은 100% 탐험 (무작위 행동)\n",
    "epsilon_decay_rate = 0.0005   # 에피소드가 진행될수록 epsilon을 줄이는 비율 (점차 탐험 줄이고 활용 늘림)\n",
    "min_epsilon = 0.01            # epsilon의 최솟값. 최소한의 탐험을 유지하여 새로운 경로를 찾을 가능성을 남김\n",
    "num_episodes = 20000          # 학습할 총 에피소드(게임 플레이) 수\n",
    "max_steps_per_episode = 100   # 한 에피소드당 최대 스텝(행동) 수\n",
    "\n",
    "print(\"--- SARSA 학습 시작 ---\")\n",
    "print(f\"Q-테이블 초기화:\\n{q_table.round(2)}\") # 초기 Q-테이블은 모두 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73caec7b-de04-4ea6-b630-842e93f6ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 학습 루프 (에피소드 반복)\n",
    "for episode in range(num_episodes):\n",
    "    # 각 에피소드 시작 시 환경 초기화\n",
    "    state, info = env_train.reset()\n",
    "    terminated = False # 목표 도달 또는 구멍에 빠져 에피소드 종료 여부\n",
    "    truncated = False  # 최대 스텝 수 도달 등으로 에피소드 강제 종료 여부\n",
    "\n",
    "    # 에피소드의 첫 번째 행동 선택 (epsilon-greedy 전략)\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = env_train.action_space.sample() # 무작위 행동 선택 (탐험)\n",
    "    else:\n",
    "        action = np.argmax(q_table[state, :]) # 현재 Q-테이블에서 최적 Q값 행동 선택 (활용)\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # 환경에 선택한 행동을 전달하고 다음 상태, 보상, 종료 여부 등을 받음\n",
    "        new_state, reward, terminated, truncated, info = env_train.step(action)\n",
    "\n",
    "        # 다음 스텝에서 취할 행동 A_{t+1}을 현재 정책(epsilon-greedy)에 따라 미리 선택\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            next_action = env_train.action_space.sample() # 무작위 행동 (탐험)\n",
    "        else:\n",
    "            next_action = np.argmax(q_table[new_state, :]) # 최적 Q값 행동 (활용)\n",
    "\n",
    "        # SARSA Q-테이블 업데이트 공식\n",
    "        # 새로운 Q값 = 현재 Q값 + 학습률 * (실제 보상 + 감가율 * 다음 상태에서 실제로 선택된 행동의 Q값 - 현재 Q값)\n",
    "        # Q-러닝의 max_future_q 대신 Q(S_{t+1}, A_{t+1}) 사용\n",
    "        current_q = q_table[state, action]\n",
    "        predicted_future_q = q_table[new_state, next_action] # SARSA의 핵심!\n",
    "        \n",
    "        new_q = current_q + learning_rate * (reward + discount_factor * predicted_future_q - current_q)\n",
    "        q_table[state, action] = new_q\n",
    "\n",
    "        # 상태 및 행동 업데이트 (다음 스텝을 위해)\n",
    "        state = new_state \n",
    "        action = next_action\n",
    "\n",
    "        # 에피소드 종료 조건 확인\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # 에피소드가 끝날 때마다 epsilon 감소 (탐험율 점차 줄임)\n",
    "    epsilon = max(min_epsilon, epsilon - epsilon_decay_rate) \n",
    "\n",
    "    # 학습 진행 상황 출력 (선택 사항)\n",
    "    if episode % 1000 == 0:\n",
    "        print(f\"Episode {episode} / Epsilon: {epsilon:.4f} / Last Reward: {reward}\")\n",
    "\n",
    "print(\"\\n--- SARSA 학습 완료! ---\")\n",
    "print(f\"최종 Q-테이블:\\n{q_table.round(2)}\") # 학습 완료 후의 Q-테이블 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f565830-6d60-4cf3-97a7-ddfd504d294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 학습된 정책 테스트 시작 ---\")\n",
    "# 테스트 시에는 탐험 없이 항상 최적 행동만 선택하도록 epsilon을 0으로 설정\n",
    "env_test = gym.make('FrozenLake-v1', is_slippery=False, render_mode='human')\n",
    "num_test_episodes = 5 # 테스트할 에피소드 수\n",
    "total_test_rewards = 0\n",
    "\n",
    "for episode in range(num_test_episodes):\n",
    "    state, info = env_test.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    episode_reward = 0\n",
    "    print(f\"\\n--- 테스트 에피소드 {episode + 1} ---\")\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    env_test.render() # 초기 상태 렌더링\n",
    "    time.sleep(1) # 잠시 대기하여 시각화 확인\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        # 학습된 Q-테이블에서 현재 상태에 대한 가장 높은 Q값을 주는 행동 선택 (탐험 없음)\n",
    "        action = np.argmax(q_table[state, :]) \n",
    "        \n",
    "        new_state, reward, terminated, truncated, info = env_test.step(action)\n",
    "        episode_reward += reward # 에피소드 보상 누적\n",
    "        state = new_state # 상태 업데이트\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        env_test.render() # 변경된 환경 상태 렌더링\n",
    "        time.sleep(0.5) # 각 스텝마다 잠시 대기\n",
    "        \n",
    "    print(f\"에피소드 {episode + 1} 종료! 보상: {episode_reward}\")\n",
    "    total_test_rewards += episode_reward\n",
    "\n",
    "print(f\"\\n총 {num_test_episodes} 에피소드 평균 보상: {total_test_rewards / num_test_episodes:.2f}\")\n",
    "\n",
    "env_train.close()\n",
    "env_test.close()\n",
    "print(\"환경이 닫혔습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb7977-c21e-4a39-9e5e-7e7983b74b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20f6339d-af98-434d-8cc7-bdd2dd238a13",
   "metadata": {},
   "source": [
    "강화 학습(Reinforcement Learning, RL) 분야는 현재 매우 빠르게 발전하고 있으며, 다양한 연구와 실제 적용 사례들이 쏟아져 나오고 있습니다. 과거 알파고(AlphaGo)처럼 특정 게임을 넘어, 이제는 복잡한 현실 문제를 해결하는 방향으로 진화하고 있습니다.\r\n",
    "\r\n",
    "최근 강화 학습의 주요 추세와 연구 동향은 다음과 같습니다.\r\n",
    "\r\n",
    "### **1. 딥 강화 학습(Deep Reinforcement Learning, DRL)의 지속적인 발전 및 안정화**\r\n",
    "\r\n",
    "여전히 강화 학습 연구의 중심에는 DRL이 있습니다. 기존의 DQN, Actor-Critic(A2C, A3C), PPO, SAC와 같은 알고리즘들이 더욱 안정적이고 효율적으로 학습할 수 있도록 발전하고 있습니다.\r\n",
    "\r\n",
    "* **모델 아키텍처의 발전**: 더 효율적인 신경망 구조, 더 깊고 복잡한 네트워크를 통한 성능 향상 연구가 계속됩니다.\r\n",
    "* **학습 안정성 향상**: 학습 과정의 불안정성을 줄이고 수렴을 더 쉽게 만드는 기법들이 연구되고 있습니다.\r\n",
    "\r\n",
    "### **2. 샘플 효율성(Sample Efficiency) 향상**\r\n",
    "\r\n",
    "강화 학습의 가장 큰 단점 중 하나는 엄청난 양의 경험(데이터)이 필요하다는 점입니다. 이를 해결하기 위한 '샘플 효율성' 연구가 활발합니다. 즉, 더 적은 경험으로도 에이전트가 더 빠르게 학습할 수 있도록 하는 것입니다.\r\n",
    "\r\n",
    "* **오프라인 강화 학습(Offline Reinforcement Learning)**: 에이전트가 환경과 직접 상호작용하지 않고, 미리 수집된 정적인 데이터셋만을 가지고 학습하는 방식입니다. 로봇 제어나 헬스케어처럼 '시행착오가 위험하거나 비용이 많이 드는' 분야에서 매우 중요하게 떠오르고 있습니다.\r\n",
    "* **모델 기반 강화 학습의 재조명**: 환경 모델을 명시적으로 학습하거나 근사하여 이를 기반으로 계획(planning)을 세우고 경험을 '가상으로 생성'하여 샘플 효율성을 높이는 연구들이 다시 주목받고 있습니다. 모델-프리 방식과 결합하여 하이브리드 형태로 연구되기도 합니다.\r\n",
    "* **메타 강화 학습(Meta-RL)**: 에이전트가 새로운 작업에 빠르게 적응하도록 '학습하는 방법' 자체를 학습합니다. 여러 관련 작업을 통해 얻은 지식을 새로운 작업에 전이하여 샘플 효율성을 높입니다.\r\n",
    "* **강력한 탐험 전략**: 무작위 탐험을 넘어, 불확실성이 높은 상태를 더 효율적으로 탐색하는 전략들이 연구되고 있습니다.\r\n",
    "\r\n",
    "### **3. 실세계 적용(Real-World Applications) 및 안전성(Safety) 강화**\r\n",
    "\r\n",
    "강화 학습이 시뮬레이션 환경을 넘어 실제 세계에 적용되면서 안전성, 견고성, 설명 가능성에 대한 요구가 커지고 있습니다.\r\n",
    "\r\n",
    "* **로봇 공학**: 복잡한 조작, 자율 이동 로봇, 인간-로봇 협업 등 다양한 로봇 제어 분야에서 강화 학습이 핵심적인 역할을 하고 있습니다. GPU 가속 시뮬레이션을 통해 학습 속도를 높이는 연구도 활발합니다.\r\n",
    "* **자율 시스템**: 자율주행차, 드론 제어 등 안전이 최우선인 분야에서 강화 학습의 적용 가능성을 탐색하며, 잠재적 위험을 피하는 안전 강화 학습(Safe RL)이 강조됩니다.\r\n",
    "* **에너지 관리, 의료, 금융**: 에너지 그리드 최적화, 맞춤형 치료법 추천, 금융 거래 전략 등 다양한 산업 분야에서 의사결정 시스템으로 강화 학습이 활용되고 있습니다.\r\n",
    "* **설명 가능성(Explainability)과 윤리**: 강화 학습 에이전트의 결정 과정을 인간이 이해할 수 있도록 만드는 연구와 함께, 학습 과정에서의 편향성이나 예상치 못한 부작용을 줄이는 윤리적 문제도 중요하게 다뤄집니다.\r\n",
    "\r\n",
    "### **4. 다중 에이전트 강화 학습(Multi-Agent Reinforcement Learning, MARL)**\r\n",
    "\r\n",
    "여러 에이전트가 동시에 같은 환경에서 상호작용하며 학습하는 시나리오입니다.\r\n",
    "\r\n",
    "* **협력(Cooperative) 및 경쟁(Competitive) 환경**: 교통 체증 최적화(협력), 복잡한 전략 게임(경쟁) 등 실제 세계의 많은 문제는 다중 에이전트 시스템으로 모델링될 수 있습니다.\r\n",
    "* **통신 및 조정**: 에이전트 간의 효율적인 정보 교환 및 행동 조정을 학습하는 방법이 중요하게 연구됩니다.\r\n",
    "\r\n",
    "### **5. 다른 AI 분야와의 통합**\r\n",
    "\r\n",
    "강화 학습이 다른 인공지능 기술들과 결합하여 시너지를 내는 추세입니다.\r\n",
    "\r\n",
    "* **LLM(대규모 언어 모델)과의 결합 (RLHF)**: 인간 피드백을 통한 강화 학습(Reinforcement Learning from Human Feedback, RLHF)은 ChatGPT와 같은 LLM의 성능을 비약적으로 향상시키는 데 핵심적인 역할을 했습니다. LLM이 더 나은 응답을 생성하도록 강화 학습으로 훈련합니다.\r\n",
    "* **컴퓨터 비전, 자연어 처리와의 융합**: 이미지/비디오를 이해하고(비전), 언어로 소통하며(NLP), 동시에 의사결정을 내리는 에이전트 연구가 활발합니다.\r\n",
    "* **모방 학습(Imitation Learning)**: 전문가의 시범(Demonstration)을 통해 에이전트가 빠르게 행동을 모방하고, 이후 강화 학습으로 미세 조정하는 방식이 샘플 효율성 향상에 기여합니다.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "요약하자면, 현재 강화 학습은 **샘플 효율성을 극대화**하고, **복잡한 현실 세계 문제에 안전하고 효과적으로 적용**하며, **다른 강력한 AI 기술들과 융합**하는 방향으로 빠르게 발전하고 있다고 볼 수 있습니다. 앞으로 더욱 다양한 분야에서 강화 학습의 놀라운 성과를 보게 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fed217-5b1a-40fc-b2b4-de3b80a59597",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822ebc2-12ad-49f5-965c-f6f7a8eceef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Environment",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
