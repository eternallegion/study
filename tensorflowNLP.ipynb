{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ef35c7-d609-4be8-bad0-a9090f72207c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n",
      "==============================\n",
      "11314 11314 20 11314\n",
      "==============================\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "==============================\n",
      "7\n",
      "==============================\n",
      "rec.autos\n",
      "==============================\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "nw_dt=fetch_20newsgroups(subset='train')\n",
    "print(nw_dt.keys())\n",
    "print('==='*10)\n",
    "print (len(nw_dt.data), len(nw_dt.filenames), len(nw_dt.target_names), len(nw_dt.target))\n",
    "print('==='*10)\n",
    "print(nw_dt.target_names)\n",
    "print('==='*10)\n",
    "print(nw_dt.target[0])  #첫번째\n",
    "print('==='*10)\n",
    "print(nw_dt.target_names[7]) #8번째\n",
    "print('==='*10)\n",
    "print(nw_dt.data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c760ace-b498-4ed7-88b9-f752920b7d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "#나이브 베이즈 분류\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dtmvec= CountVectorizer()\n",
    "X_train_dtm=dtmvec.fit_transform(nw_dt.data)\n",
    "print(X_train_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "584a3c81-fbdb-4967-86cf-f07dc42f75bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "tfidf_tr=TfidfTransformer()\n",
    "tfidfv=tfidf_tr.fit_transform(X_train_dtm)\n",
    "print(tfidfv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc41753-c7f7-4b63-9d2b-4eb3bdf1dafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod=MultinomialNB()\n",
    "mod.fit(tfidfv,nw_dt.target)\n",
    "\n",
    "MultinomialNB(alpha=1.0,class_prior=None,fit_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c137bb-9158-4014-80a9-6be2d60e276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7738980350504514\n"
     ]
    }
   ],
   "source": [
    "nw_dt_test=fetch_20newsgroups(subset='test', shuffle=True)\n",
    "X_text_dtm=dtmvec.transform(nw_dt_test.data)\n",
    "\n",
    "tfidfv_test=tfidf_tr.transform(X_text_dtm)\n",
    "pred=mod.predict(tfidfv_test)\n",
    "\n",
    "print(\"정확도:\", accuracy_score(nw_dt_test.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4973356-4baf-425a-a19b-5e98ccd0597c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0129b7d4-6438-4899-a376-52913ba6f130",
   "metadata": {},
   "source": [
    "# 네이버 쇼핑 리뷰 감성 분류하기(Naver Shopping Review Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e2c71-fe0d-4ec5-b2f0-9e4bf8ceb4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73fa937-b799-4487-aca8-e4fd56e49863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eternal/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "2024-01-22 19:01:04.912614: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-22 19:01:04.928361: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-22 19:01:04.928381: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-22 19:01:04.928869: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-22 19:01:04.931625: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 19:01:05.336205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import pandas as ps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as mp\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9acd3a6f-9649-47c6-8b7e-ee32af9e2400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "199908\n",
      "False\n",
      "훈련용 리뷰의 개수 : 149931\n",
      "테스트용 리뷰의 개수 : 49977\n"
     ]
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"ratings_total.txt\")\n",
    "\n",
    "total_data=ps.read_table('ratings_total.txt',names=['ratings','reviews'])\n",
    "\n",
    "print(len(total_data))\n",
    "total_data[:5]\n",
    "\n",
    "total_data['label']=np.select([total_data.ratings>3],[1],default=0)\n",
    "total_data[:5]\n",
    "\n",
    "total_data['ratings'].nunique(),total_data['reviews'].nunique(),total_data['label'].nunique()\n",
    "\n",
    "total_data.drop_duplicates(subset=['reviews'],inplace=True)\n",
    "print(len(total_data))\n",
    "print(total_data.isnull().values.any())\n",
    "\n",
    "\n",
    "train_data,test_data=train_test_split(total_data,test_size=0.25,random_state=42)\n",
    "print('훈련용 리뷰의 개수 :', len(train_data))\n",
    "print('테스트용 리뷰의 개수 :', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ca7e88-0125-4143-8cb0-e07f2fea5b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  count\n",
      "0      0  74918\n",
      "1      1  75013\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['label'].value_counts().plot(kind='bar')\n",
    "print(train_data.groupby('label').size().reset_index(name = 'count'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06ef3ac-3e63-4cd5-80e9-97096c174d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings    0\n",
      "reviews    0\n",
      "label      0\n",
      "dtype: int64\n",
      "전처리 후 테스트용 샘플의 개수 : 49977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54520/1002105688.py:8: FutureWarning: Series.replace without 'value' and with non-dict-like 'to_replace' is deprecated and will raise in a future version. Explicitly specify the new values instead.\n",
      "  test_data['reviews'].replace(',np.nan,inplace=True')\n"
     ]
    }
   ],
   "source": [
    "train_data['reviews']=train_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train_data['reviews'].replace(' ',np.nan,inplace=True)\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "test_data.drop_duplicates(subset=['reviews'],inplace=True)\n",
    "test_data['reviews']=test_data['reviews'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "test_data['reviews'].replace(',np.nan,inplace=True')\n",
    "test_data=test_data.dropna(how='any')\n",
    "\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895dec5-a86c-4d40-8280-70028ebe03a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1af34bb5-94cb-49bf-a718-7f13feded7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['와', '이런', '것', '도', '상품', '이', '라고', '차라리', '내', '가', '만드', '는', '게', '나을', '뻔']\n"
     ]
    }
   ],
   "source": [
    "mecab=Mecab()\n",
    "print(mecab.morphs('와 이런 것도 상품이라고 차라리 내가 만드는 게 나을 뻔'))\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
    "\n",
    "train_data['tokenized']=train_data['reviews'].apply(mecab.morphs)\n",
    "train_data['tokenized']=train_data['tokenized'].apply(lambda x:[item for item in x if item not in stopwords])\n",
    "\n",
    "test_data['tokenized']=test_data['reviews'].apply(mecab.morphs)\n",
    "test_data['tokenized']=test_data['tokenized'].apply(lambda x:[item for item in x if item not in stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e88e9-d517-49fa-afe9-0e80fdbf88bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe210bd3-e489-4a90-abcd-99e9aabfd206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 77185), ('네요', 31942), ('는데', 20241), ('안', 19757), ('어요', 15232), ('있', 13203), ('너무', 12984), ('했', 11888), ('좋', 9804), ('배송', 9681), ('..', 9660), ('같', 9005), ('어', 8886), ('구매', 8872), ('거', 8868), ('아요', 8679), ('없', 8676), ('습니다', 8443), ('그냥', 8355), ('되', 8349)]\n",
      "============================================================\n",
      "[('좋', 39455), ('.', 35659), ('아요', 21278), ('네요', 19912), ('어요', 19359), ('잘', 18608), ('구매', 16165), ('습니다', 13340), ('있', 12391), ('배송', 12274), ('!', 12007), ('는데', 11685), ('했', 10157), ('~', 9980), ('합니다', 9825), ('먹', 9643), ('재', 9268), ('너무', 8388), ('같', 7870), ('만족', 7250)]\n"
     ]
    }
   ],
   "source": [
    "negative=np.hstack(train_data[train_data.label==0]['tokenized'].values)\n",
    "positive=np.hstack(train_data[train_data.label==1]['tokenized'].values)\n",
    "\n",
    "negative_count=Counter(negative)\n",
    "print(negative_count.most_common(20))\n",
    "\n",
    "print('==='*20)\n",
    "positive_count=Counter(positive)\n",
    "print(positive_count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6195247-eb12-486f-b86e-f1a53a465d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 리뷰의 평균 길이 : 14.788569981203258\n",
      "부정 리뷰의 평균 길이 : 18.835380015483594\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,(ax1,ax2) = mp.subplots(1,2,figsize=(10,5))\n",
    "text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\n",
    "ax1.hist(text_len, color='red')\n",
    "ax1.set_title('Positive Reviews')\n",
    "ax1.set_xlabel('length of samples')\n",
    "ax1.set_ylabel('number of samples')\n",
    "print('긍정 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "\n",
    "text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))\n",
    "ax2.hist(text_len, color='blue')\n",
    "ax2.set_title('Negative Reviews')\n",
    "fig.suptitle('Words in texts')\n",
    "ax2.set_xlabel('length of samples')\n",
    "ax2.set_ylabel('number of samples')\n",
    "print('부정 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "mp.show()\n",
    "\n",
    "x_train=train_data['tokenized'].values\n",
    "y_train=train_data['label'].values\n",
    "x_test=test_data['tokenized'].values\n",
    "y_test=test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2322a4df-ee6e-46b8-beb2-ce12522cc31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 42361\n",
      "등장 빈도가 1번 이하인 희귀 단어의 수: 1\n",
      "단어 집합에서 희귀 단어의 비율: 0.0023606619296050613\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 0.7818860486485715\n",
      "단어 집합의 크기 : 42362\n",
      "[[72, 146, 2083, 313, 14803, 278, 79, 7, 254, 182, 148, 835, 3046, 659, 3, 85, 68, 219, 44, 1402, 168, 4, 7], [509, 2795, 2, 2, 8834, 2686, 2, 2, 2543, 353, 3017, 266, 2424, 41, 495, 3], [50, 27, 881, 104, 39, 2425, 171, 8, 11, 8343, 5, 2, 1375, 34, 149, 335, 46, 64, 171, 149, 8, 1980, 3, 2, 4629, 121, 177, 1443, 292, 45, 33, 130, 147, 2, 4629], [337, 57, 21], [152, 3461, 452, 14804, 2005, 105, 289, 2939, 68, 51, 137, 2978, 27, 26]]\n",
      "[[15, 740, 803, 125, 198, 264, 14], [353, 4058, 68, 4159, 1705], [13, 77, 3, 54, 176, 4, 30, 16, 7, 398, 538, 304, 18, 101, 120, 611, 64, 8, 3], [23, 10, 60, 2, 564, 4, 729, 4, 14, 2], [750, 50, 437, 17, 4303, 16, 547, 63, 36, 6, 2924, 2324, 1157, 70, 233, 239, 837, 7613, 94]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "threshold=2\n",
    "total_cnt=len(tokenizer.word_index)\n",
    "rate_cnt=0\n",
    "total_freq=0\n",
    "rare_freq=0\n",
    "\n",
    "for key,value in tokenizer.word_counts.items():\n",
    "    total_freq=total_freq+value\n",
    "    if(value<threshold):\n",
    "        rare_cnt=rare_cnt=+1\n",
    "        rare_freq=rare_freq+value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "# 0번 패딩 토큰과 1번 OOV 토큰을 고려하여 +2\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer=Tokenizer(vocab_size,oov_token='OOV')\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train=tokenizer.texts_to_sequences(x_train)\n",
    "x_test=tokenizer.texts_to_sequences(x_test)\n",
    "print(x_train[:5])\n",
    "print(x_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97b55c-56af-402b-9fd1-1e3ba1eb86ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e24924bd-9606-4a2d-ab6c-845bc451bddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 87\n",
      "리뷰의 평균 길이 : 16.81069291874262\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(review) for review in x_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, x_train))/len(x_train))\n",
    "mp.hist([len(review) for review in x_train], bins=50)\n",
    "mp.xlabel('length of samples')\n",
    "mp.ylabel('number of samples')\n",
    "mp.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdc394fb-2121-40cc-b95d-e20e021f55b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 80 이하인 샘플의 비율: 99.99866605305107\n"
     ]
    }
   ],
   "source": [
    "def BTL(max_len,nest_list):\n",
    "    count=0\n",
    "    for sentence in nest_list:\n",
    "        if (len(sentence)<=max_len):\n",
    "            count=count+1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (count / len(nest_list))*100))\n",
    "\n",
    "max_len = 80\n",
    "BTL(max_len, x_train)\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22984f87-af3c-49ef-9f0c-4fe0bb8429aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d0e9e61-e33f-483a-83e5-48826b21ca66",
   "metadata": {},
   "source": [
    "# GRU로 네이버 쇼핑 리뷰 감성 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f24cf06b-bcbd-4ad6-90ce-de8f74cb19d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 19:01:18.795850: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-22 19:01:18.823799: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-22 19:01:18.823908: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-22 19:01:18.825545: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-22 19:01:18.825610: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-22 19:01:18.825651: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-22 19:01:18.871540: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-22 19:01:18.871614: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-22 19:01:18.871664: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-22 19:01:18.871706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21779 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 19:01:19.718951: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/1875 [..............................] - ETA: 26:44 - loss: 0.6919 - acc: 0.5469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 19:01:19.937109: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fcbe84afbb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-22 19:01:19.937128: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-01-22 19:01:19.939769: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1705917679.971227   54677 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2860 - acc: 0.8909\n",
      "Epoch 1: val_acc improved from -inf to 0.91420, saving model to best_model.h5\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.2860 - acc: 0.8909 - val_loss: 0.2397 - val_acc: 0.9142\n",
      "Epoch 2/1500\n",
      "  27/1875 [..............................] - ETA: 7s - loss: 0.2596 - acc: 0.9115"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eternal/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2238 - acc: 0.9205\n",
      "Epoch 2: val_acc improved from 0.91420 to 0.92093, saving model to best_model.h5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2238 - acc: 0.9205 - val_loss: 0.2182 - val_acc: 0.9209\n",
      "Epoch 3/1500\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.2018 - acc: 0.9290\n",
      "Epoch 3: val_acc improved from 0.92093 to 0.92270, saving model to best_model.h5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2018 - acc: 0.9290 - val_loss: 0.2133 - val_acc: 0.9227\n",
      "Epoch 4/1500\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1862 - acc: 0.9357\n",
      "Epoch 4: val_acc did not improve from 0.92270\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1862 - acc: 0.9357 - val_loss: 0.2164 - val_acc: 0.9223\n",
      "Epoch 5/1500\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1732 - acc: 0.9415\n",
      "Epoch 5: val_acc did not improve from 0.92270\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1732 - acc: 0.9415 - val_loss: 0.2160 - val_acc: 0.9212\n",
      "Epoch 6/1500\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1612 - acc: 0.9459\n",
      "Epoch 6: val_acc did not improve from 0.92270\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1612 - acc: 0.9459 - val_loss: 0.2231 - val_acc: 0.9214\n",
      "Epoch 7/1500\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.1495 - acc: 0.9508\n",
      "Epoch 7: val_acc did not improve from 0.92270\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1495 - acc: 0.9508 - val_loss: 0.2164 - val_acc: 0.9216\n",
      "Epoch 7: early stopping\n",
      "1562/1562 [==============================] - 2s 1ms/step - loss: 0.2184 - acc: 0.9223\n",
      "\n",
      " 테스트 정확도: 0.9223\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding,Dense,GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "embedding_dim=100\n",
    "hidden_dim=128\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,embedding_dim))\n",
    "model.add(GRU(hidden_dim))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "es=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=4)\n",
    "mc=ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(x_train,y_train,epochs=1500,callbacks=[es,mc],batch_size=64,validation_split=0.2)\n",
    "load_model=load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (load_model.evaluate(x_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17707854-6123-465d-9dc6-64c45a3ce5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_pred(N_sentence):\n",
    "    N_sentence=re.sub(r'[^ㄱ-ㅎ ㅏ-ㅑ 가-힣]',' ',N_sentence)\n",
    "    N_sentence=mecab.morphs(N_sentence)\n",
    "    N_sentence=[word for word in N_sentence if not word in stopwords]\n",
    "    encode=tokenizer.texts_to_sequences([N_sentence])\n",
    "    pad_new=pad_sequences(encode,maxlen=max_len)\n",
    "\n",
    "    score=float(load_model.predict(pad_new))\n",
    "    if(score>0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "883349a0-221a-405c-99d4-a8d22ce5e7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 111ms/step\n",
      "97.43% 확률로 긍정 리뷰입니다.\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "98.87% 확률로 부정 리뷰입니다.\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "96.01% 확률로 긍정 리뷰입니다.\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "92.25% 확률로 부정 리뷰입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_54520/425926545.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  score=float(load_model.predict(pad_new))\n"
     ]
    }
   ],
   "source": [
    "sentence_pred('이 상품 진짜 좋아요... 저는 강추합니다. 대박')\n",
    "sentence_pred('진짜 배송도 늦고 개짜증나네요. 뭐 이런 걸 상품이라고 만듬?')\n",
    "sentence_pred('판매자님... 너무 짱이에요.. 대박나삼')\n",
    "sentence_pred('ㅁㄴㅇㄻㄴㅇㄻㄴㅇ리뷰쓰기도 귀찮아')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfe98e7-210f-4626-8ad2-9fa41f100ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49ce7a-543a-4097-8820-e566f55cb4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7b77cd6-3b33-475e-afcf-c9fab9f28389",
   "metadata": {},
   "source": [
    "# BiLSTM으로 한국어 스팀 리뷰 감성 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "00fb0dc9-459e-400c-99b3-acd7f1edc845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 리뷰 개수 : 100000\n",
      "   label                               reviews\n",
      "0      0                             노래가 너무 적음\n",
      "1      0       돌겠네 진짜. 황숙아, 어크 공장 그만 돌려라. 죽는다.\n",
      "2      1      막노동 체험판 막노동 하는사람인데 장비를 내가 사야돼 뭐지\n",
      "3      1  차악!차악!!차악!!! 정말 이래서 왕국을 되찾을 수 있는거야??\n",
      "4      1   시간 때우기에 좋음.. 도전과제는 50시간이면 다 깰 수 있어요\n",
      "총 샘플의 수 : 99892\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as mp\n",
    "import urllib.request\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/steam.txt\", filename=\"steam.txt\")\n",
    "\n",
    "total_data=ps.read_table(\"steam.txt\",names=['label','reviews'])\n",
    "print('전체 리뷰 개수 :',len(total_data)) \n",
    "\n",
    "print(total_data[:5])\n",
    "\n",
    "total_data['reviews'].nunique(),total_data['label'].nunique()\n",
    "\n",
    "total_data.drop_duplicates(subset=['reviews'],inplace=True)\n",
    "print('총 샘플의 수 :',len(total_data))\n",
    "\n",
    "print(total_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad379aa-9658-402b-bdb2-35b3795a33b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7441142d-20c5-4bfb-a0a5-47f47ed97e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74919\n",
      "24973\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data=train_test_split(total_data,test_size=0.25,random_state=42)\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa0fb2-601a-4ab5-b690-a39027ee108b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4167c765-c9aa-41fe-b7f8-693509c5a151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  count\n",
      "0      0  37376\n",
      "1      1  37543\n",
      "label\n",
      "0    37376\n",
      "1    37543\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['label'].value_counts().plot(kind='bar')\n",
    "#test_data['label'].value_counts().plot(kind='bar')\n",
    "print(train_data.groupby('label').size().reset_index(name='count'))#\n",
    "print(train_data.groupby('label').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506b28e-6c6e-4a69-a96d-204f166b934c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1a538764-9313-45b9-959e-f4b282271bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label      0\n",
      "reviews    0\n",
      "dtype: int64\n",
      "전처리 후 테스트용 샘플의 개수 : 24973\n"
     ]
    }
   ],
   "source": [
    "train_data['reviews']=train_data['reviews'].str.replace('[^ㄱ-ㅎ ㅏ-ㅣ 가-힣]',' ')\n",
    "train_data['reviews'].replace(' ',np.nan,inplace=True)\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "test_data.drop_duplicates(subset=['reviews'],inplace=True)\n",
    "test_data['reviews']=test_data['reviews'].str.replace('[ㄱ-ㅎ,ㅏ-ㅣ,가-힣]',' ')\n",
    "test_data['reviews'].replace('',np.nan,inplace=True)\n",
    "test_data=test_data.dropna(how='any')\n",
    "\n",
    "print('전처리 후 테스트용 샘플의 개수 :',len(test_data))\n",
    "\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '게임', '겜', '되', '음', '면']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "36e3c40c-2b9f-4e28-8e4d-1bb459291ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab=Mecab()\n",
    "\n",
    "train_data['tokenized']=train_data['reviews'].apply(mecab.morphs)\n",
    "train_data['tokenized']=train_data['tokenized'].apply(lambda x:[item for item in x if item not in stopwords])\n",
    "\n",
    "test_data['tokenized']=test_data['reviews'].apply(mecab.morphs)\n",
    "test_data['tokenized']=test_data['tokenized'].apply(lambda x:[item for item in x if item not in stopwords])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50e3cc-6831-44cb-9998-698f136b1890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "76aa1916-dd92-4bd2-b62f-952e89e1b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 36401), ('안', 8137), ('없', 7140), ('는데', 5743), ('있', 5688), ('..', 5158), (',', 4692), ('같', 4249), ('로', 4174), ('?', 4139), ('할', 3917), ('거', 3915), ('나', 3840), ('해', 3670), ('너무', 3519), ('기', 3355), ('으로', 3352), ('했', 3277), ('어', 3159), ('습니다', 2957)]\n",
      "=============================================\n",
      "[('.', 36364), ('있', 9980), ('좋', 6544), ('!', 5894), (',', 5394), ('습니다', 5161), ('재밌', 4996), ('할', 4853), ('지만', 4689), ('해', 4379), ('없', 4153), ('로', 3913), ('으로', 3901), ('보', 3848), ('수', 3830), ('는데', 3782), ('기', 3599), ('..', 3478), ('안', 3379), ('것', 3368)]\n",
      "긍정 리뷰의 평균 길이 : 17.333723996484032\n",
      "부정 리뷰의 평균 길이 : 17.591770119863014\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "negative=np.hstack(train_data[train_data.label==0]['tokenized'].values)\n",
    "positive=np.hstack(train_data[train_data.label==1]['tokenized'].values)\n",
    "\n",
    "negative_count=Counter(negative)\n",
    "positive_count=Counter(positive)\n",
    "print(negative_count.most_common(20))\n",
    "print('==='*15)\n",
    "print(positive_count.most_common(20))\n",
    "\n",
    "\n",
    "fig,(ax1,ax2) = mp.subplots(1,2,figsize=(10,5))\n",
    "text_len = train_data[train_data['label']==1]['tokenized'].map(lambda x: len(x))\n",
    "ax1.hist(text_len, color='red')\n",
    "ax1.set_title('Positive Reviews')\n",
    "ax1.set_xlabel('length of samples')\n",
    "ax1.set_ylabel('number of samples')\n",
    "print('긍정 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "\n",
    "text_len = train_data[train_data['label']==0]['tokenized'].map(lambda x: len(x))\n",
    "ax2.hist(text_len, color='blue')\n",
    "ax2.set_title('Negative Reviews')\n",
    "fig.suptitle('Words in texts')\n",
    "ax2.set_xlabel('length of samples')\n",
    "ax2.set_ylabel('number of samples')\n",
    "print('부정 리뷰의 평균 길이 :', np.mean(text_len))\n",
    "mp.show()\n",
    "\n",
    "\n",
    "x_train=train_data['tokenized'].values\n",
    "y_train=train_data['label'].values\n",
    "x_test=test_data['tokenized'].values\n",
    "y_test=test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d581d-1a82-4dfe-bb0e-f00d06c4c49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c15e2a2a-5ddd-45d6-aac0-b9ebc7ebc25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 37464\n",
      "등장 빈도가 1번 이하인 희귀 단어의 수: 16606\n",
      "단어 집합에서 희귀 단어의 비율: 44.325218876788384\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 1.2693098519418775\n",
      "단어 집합의 크기 : 20860\n",
      "[[496, 670, 9, 40, 95, 168, 2594, 641, 31, 11, 78, 54, 256, 122, 44, 52, 60], [47, 170, 326, 23, 26, 72, 3790, 1046, 8, 65, 2, 90], [9, 21, 1564, 5]]\n",
      "[[774, 39, 20, 472, 60, 2, 116, 150, 103, 6898, 527], [4811, 732, 882, 757, 730, 125, 5, 1892, 109, 406, 3729, 84, 600, 2024, 4572, 285, 188, 69, 30, 395, 40, 95, 1037, 24, 6719, 6855], [1883, 849, 585, 56, 28, 2, 385]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "threshold=2\n",
    "total_cnt=len(tokenizer.word_index)\n",
    "rare_cnt=0\n",
    "total_freq=0\n",
    "rare_freq=0\n",
    "\n",
    "for key,value in tokenizer.word_counts.items():\n",
    "    total_freq=total_freq+value\n",
    "    if (value<threshold):\n",
    "        rare_cnt=rare_cnt+1\n",
    "        rare_freq=rare_freq+value\n",
    "\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 2\n",
    "print('단어 집합의 크기 :',vocab_size)\n",
    "\n",
    "tokenizer=Tokenizer(vocab_size,oov_token='OOV')\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "x_train=tokenizer.texts_to_sequences(x_train)\n",
    "x_test=tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "print(x_train[:3])\n",
    "print(x_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b67a52-4065-451f-9aa0-b86cf4867623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4db8687d-29e6-46e7-adf1-a762644c547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 83\n",
      "리뷰의 평균 길이 : 17.46245945621271\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 99.93993512994034\n"
     ]
    }
   ],
   "source": [
    "print('리뷰의 최대 길이 :',max(len(review) for review in x_train))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, x_train))/len(x_train))\n",
    "mp.hist([len(review) for review in x_train], bins=50)\n",
    "mp.xlabel('length of samples')\n",
    "mp.ylabel('number of samples')\n",
    "mp.show()\n",
    "\n",
    "\n",
    "def BTL(max_len,nest_list):\n",
    "    count=0\n",
    "    for sentence in nest_list:\n",
    "        if (len(sentence)<=max_len):\n",
    "            count=count+1\n",
    "    print(max_len,(count/len(nest_list))*100)\n",
    "\n",
    "max_len=60\n",
    "BTL(max_len,x_train)\n",
    "\n",
    "x_train=pad_sequences(x_train,maxlen=max_len)\n",
    "x_test=pad_sequences(x_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7f31db7b-fea4-4653-8b5e-746f7f22f84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.5972 - acc: 0.6682\n",
      "Epoch 1: val_loss improved from -inf to 0.50898, saving model to test_model.h5\n",
      "235/235 [==============================] - 6s 23ms/step - loss: 0.5958 - acc: 0.6694 - val_loss: 0.5090 - val_acc: 0.7599\n",
      "Epoch 2/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.4739 - acc: 0.7785\n",
      "Epoch 2: val_loss did not improve from 0.50898\n",
      "235/235 [==============================] - 3s 11ms/step - loss: 0.4738 - acc: 0.7786 - val_loss: 0.4706 - val_acc: 0.7834\n",
      "Epoch 3/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.4387 - acc: 0.8004\n",
      "Epoch 3: val_loss did not improve from 0.50898\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.4387 - acc: 0.8004 - val_loss: 0.4690 - val_acc: 0.7836\n",
      "Epoch 4/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.4172 - acc: 0.8143\n",
      "Epoch 4: val_loss did not improve from 0.50898\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.4172 - acc: 0.8142 - val_loss: 0.4855 - val_acc: 0.7683\n",
      "Epoch 5/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.3996 - acc: 0.8255\n",
      "Epoch 5: val_loss did not improve from 0.50898\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.3998 - acc: 0.8253 - val_loss: 0.4704 - val_acc: 0.7827\n",
      "Epoch 6/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8349\n",
      "Epoch 6: val_loss improved from 0.50898 to 0.51690, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3832 - acc: 0.8353 - val_loss: 0.5169 - val_acc: 0.7768\n",
      "Epoch 7/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.3673 - acc: 0.8437\n",
      "Epoch 7: val_loss did not improve from 0.51690\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3673 - acc: 0.8437 - val_loss: 0.4794 - val_acc: 0.7774\n",
      "Epoch 7: early stopping\n",
      "<keras.src.engine.sequential.Sequential object at 0x7fcdf4b531c0>\n"
     ]
    }
   ],
   "source": [
    "#조기종료 존재\n",
    "import re\n",
    "import os\n",
    "from tensorflow.keras.layers import Embedding,Dense,LSTM,Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "embedding_dim=100\n",
    "hidden_dim=128\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size,embedding_dim))\n",
    "\n",
    "model.add(Bidirectional(LSTM(hidden)))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "es=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=4) #조기종료\n",
    "mc=ModelCheckpoint('test_model.h5',moniter='val_acc',mode='max',verbose=1,save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(x_train,y_train,epochs=1100,callbacks=[mc,es],batch_size=256,validation_split=0.2)\n",
    "\n",
    "load_model=load_model('test_model.h5')\n",
    "print(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5ad0896e-bcd8-458f-8b4c-cd7d33c77eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.5895 - acc: 0.6787\n",
      "Epoch 1: val_loss improved from -inf to 0.50612, saving model to test_model.h5\n",
      "235/235 [==============================] - 6s 23ms/step - loss: 0.5895 - acc: 0.6787 - val_loss: 0.5061 - val_acc: 0.7579\n",
      "Epoch 2/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.4684 - acc: 0.7808\n",
      "Epoch 2: val_loss did not improve from 0.50612\n",
      "235/235 [==============================] - 2s 10ms/step - loss: 0.4684 - acc: 0.7808 - val_loss: 0.4855 - val_acc: 0.7740\n",
      "Epoch 3/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.4358 - acc: 0.8024\n",
      "Epoch 3: val_loss did not improve from 0.50612\n",
      "235/235 [==============================] - 2s 8ms/step - loss: 0.4358 - acc: 0.8024 - val_loss: 0.4647 - val_acc: 0.7865\n",
      "Epoch 4/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.4148 - acc: 0.8166\n",
      "Epoch 4: val_loss did not improve from 0.50612\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 0.4148 - acc: 0.8166 - val_loss: 0.4707 - val_acc: 0.7814\n",
      "Epoch 5/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.3981 - acc: 0.8264\n",
      "Epoch 5: val_loss did not improve from 0.50612\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.3978 - acc: 0.8265 - val_loss: 0.4704 - val_acc: 0.7864\n",
      "Epoch 6/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.3822 - acc: 0.8356\n",
      "Epoch 6: val_loss improved from 0.50612 to 0.50984, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 6ms/step - loss: 0.3822 - acc: 0.8356 - val_loss: 0.5098 - val_acc: 0.7763\n",
      "Epoch 7/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.3656 - acc: 0.8451\n",
      "Epoch 7: val_loss did not improve from 0.50984\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3656 - acc: 0.8451 - val_loss: 0.4717 - val_acc: 0.7828\n",
      "Epoch 8/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.3481 - acc: 0.8550\n",
      "Epoch 8: val_loss did not improve from 0.50984\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3481 - acc: 0.8550 - val_loss: 0.4754 - val_acc: 0.7787\n",
      "Epoch 9/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.3308 - acc: 0.8656\n",
      "Epoch 9: val_loss improved from 0.50984 to 0.52026, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3308 - acc: 0.8656 - val_loss: 0.5203 - val_acc: 0.7744\n",
      "Epoch 10/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.3137 - acc: 0.8740\n",
      "Epoch 10: val_loss did not improve from 0.52026\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3137 - acc: 0.8740 - val_loss: 0.5135 - val_acc: 0.7753\n",
      "Epoch 11/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2973 - acc: 0.8805\n",
      "Epoch 11: val_loss did not improve from 0.52026\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2973 - acc: 0.8805 - val_loss: 0.5183 - val_acc: 0.7691\n",
      "Epoch 12/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2806 - acc: 0.8892\n",
      "Epoch 12: val_loss improved from 0.52026 to 0.55018, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2806 - acc: 0.8892 - val_loss: 0.5502 - val_acc: 0.7658\n",
      "Epoch 13/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2648 - acc: 0.8973\n",
      "Epoch 13: val_loss improved from 0.55018 to 0.65032, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2648 - acc: 0.8973 - val_loss: 0.6503 - val_acc: 0.7533\n",
      "Epoch 14/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2487 - acc: 0.9033\n",
      "Epoch 14: val_loss did not improve from 0.65032\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2487 - acc: 0.9033 - val_loss: 0.6130 - val_acc: 0.7657\n",
      "Epoch 15/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2338 - acc: 0.9101\n",
      "Epoch 15: val_loss did not improve from 0.65032\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2338 - acc: 0.9101 - val_loss: 0.6291 - val_acc: 0.7631\n",
      "Epoch 16/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2194 - acc: 0.9158\n",
      "Epoch 16: val_loss improved from 0.65032 to 0.65653, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2194 - acc: 0.9158 - val_loss: 0.6565 - val_acc: 0.7565\n",
      "Epoch 17/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2058 - acc: 0.9227\n",
      "Epoch 17: val_loss improved from 0.65653 to 0.71147, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2058 - acc: 0.9227 - val_loss: 0.7115 - val_acc: 0.7630\n",
      "Epoch 18/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1925 - acc: 0.9273\n",
      "Epoch 18: val_loss did not improve from 0.71147\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1925 - acc: 0.9273 - val_loss: 0.6970 - val_acc: 0.7516\n",
      "Epoch 19/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1805 - acc: 0.9332\n",
      "Epoch 19: val_loss improved from 0.71147 to 0.73797, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1805 - acc: 0.9332 - val_loss: 0.7380 - val_acc: 0.7660\n",
      "Epoch 20/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1692 - acc: 0.9373\n",
      "Epoch 20: val_loss improved from 0.73797 to 0.80693, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1692 - acc: 0.9373 - val_loss: 0.8069 - val_acc: 0.7595\n",
      "Epoch 21/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1599 - acc: 0.9411\n",
      "Epoch 21: val_loss did not improve from 0.80693\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1599 - acc: 0.9411 - val_loss: 0.8055 - val_acc: 0.7524\n",
      "Epoch 22/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1492 - acc: 0.9450\n",
      "Epoch 22: val_loss improved from 0.80693 to 0.85867, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1492 - acc: 0.9450 - val_loss: 0.8587 - val_acc: 0.7517\n",
      "Epoch 23/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.1402 - acc: 0.9489\n",
      "Epoch 23: val_loss improved from 0.85867 to 0.87932, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1411 - acc: 0.9484 - val_loss: 0.8793 - val_acc: 0.7569\n",
      "Epoch 24/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1336 - acc: 0.9512\n",
      "Epoch 24: val_loss did not improve from 0.87932\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1336 - acc: 0.9512 - val_loss: 0.8767 - val_acc: 0.7437\n",
      "Epoch 25/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1252 - acc: 0.9552\n",
      "Epoch 25: val_loss improved from 0.87932 to 0.99527, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1252 - acc: 0.9552 - val_loss: 0.9953 - val_acc: 0.7304\n",
      "Epoch 26/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1191 - acc: 0.9566\n",
      "Epoch 26: val_loss did not improve from 0.99527\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1191 - acc: 0.9566 - val_loss: 0.9658 - val_acc: 0.7557\n",
      "Epoch 27/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1115 - acc: 0.9602\n",
      "Epoch 27: val_loss did not improve from 0.99527\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1115 - acc: 0.9602 - val_loss: 0.9836 - val_acc: 0.7501\n",
      "Epoch 28/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1066 - acc: 0.9622\n",
      "Epoch 28: val_loss improved from 0.99527 to 1.00992, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.1066 - acc: 0.9622 - val_loss: 1.0099 - val_acc: 0.7527\n",
      "Epoch 29/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.1013 - acc: 0.9638\n",
      "Epoch 29: val_loss improved from 1.00992 to 1.03648, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1013 - acc: 0.9638 - val_loss: 1.0365 - val_acc: 0.7509\n",
      "Epoch 30/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0955 - acc: 0.9663\n",
      "Epoch 30: val_loss improved from 1.03648 to 1.09548, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0955 - acc: 0.9663 - val_loss: 1.0955 - val_acc: 0.7286\n",
      "Epoch 31/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0896 - acc: 0.9680\n",
      "Epoch 31: val_loss improved from 1.09548 to 1.17088, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0896 - acc: 0.9680 - val_loss: 1.1709 - val_acc: 0.7449\n",
      "Epoch 32/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0843 - acc: 0.9699\n",
      "Epoch 32: val_loss did not improve from 1.17088\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0846 - acc: 0.9695 - val_loss: 1.1667 - val_acc: 0.7329\n",
      "Epoch 33/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0804 - acc: 0.9715\n",
      "Epoch 33: val_loss improved from 1.17088 to 1.22068, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0804 - acc: 0.9715 - val_loss: 1.2207 - val_acc: 0.7389\n",
      "Epoch 34/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0767 - acc: 0.9729\n",
      "Epoch 34: val_loss improved from 1.22068 to 1.23710, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0768 - acc: 0.9729 - val_loss: 1.2371 - val_acc: 0.7419\n",
      "Epoch 35/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0743 - acc: 0.9740\n",
      "Epoch 35: val_loss improved from 1.23710 to 1.27047, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0743 - acc: 0.9740 - val_loss: 1.2705 - val_acc: 0.7365\n",
      "Epoch 36/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9756\n",
      "Epoch 36: val_loss improved from 1.27047 to 1.37134, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0699 - acc: 0.9755 - val_loss: 1.3713 - val_acc: 0.7277\n",
      "Epoch 37/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0668 - acc: 0.9766\n",
      "Epoch 37: val_loss did not improve from 1.37134\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0663 - acc: 0.9767 - val_loss: 1.2991 - val_acc: 0.7387\n",
      "Epoch 38/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9780\n",
      "Epoch 38: val_loss improved from 1.37134 to 1.39964, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0631 - acc: 0.9780 - val_loss: 1.3996 - val_acc: 0.7365\n",
      "Epoch 39/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0586 - acc: 0.9796\n",
      "Epoch 39: val_loss improved from 1.39964 to 1.48031, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0586 - acc: 0.9796 - val_loss: 1.4803 - val_acc: 0.7351\n",
      "Epoch 40/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0575 - acc: 0.9798\n",
      "Epoch 40: val_loss did not improve from 1.48031\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0575 - acc: 0.9798 - val_loss: 1.4105 - val_acc: 0.7286\n",
      "Epoch 41/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0549 - acc: 0.9806\n",
      "Epoch 41: val_loss improved from 1.48031 to 1.49049, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0549 - acc: 0.9806 - val_loss: 1.4905 - val_acc: 0.7393\n",
      "Epoch 42/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0525 - acc: 0.9820\n",
      "Epoch 42: val_loss improved from 1.49049 to 1.51405, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0525 - acc: 0.9820 - val_loss: 1.5141 - val_acc: 0.7222\n",
      "Epoch 43/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0504 - acc: 0.9829\n",
      "Epoch 43: val_loss did not improve from 1.51405\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0504 - acc: 0.9829 - val_loss: 1.4888 - val_acc: 0.7387\n",
      "Epoch 44/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9832\n",
      "Epoch 44: val_loss improved from 1.51405 to 1.54911, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0484 - acc: 0.9832 - val_loss: 1.5491 - val_acc: 0.7318\n",
      "Epoch 45/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9839\n",
      "Epoch 45: val_loss improved from 1.54911 to 1.61802, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0456 - acc: 0.9839 - val_loss: 1.6180 - val_acc: 0.7256\n",
      "Epoch 46/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0440 - acc: 0.9845\n",
      "Epoch 46: val_loss improved from 1.61802 to 1.64259, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0440 - acc: 0.9845 - val_loss: 1.6426 - val_acc: 0.7320\n",
      "Epoch 47/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0426 - acc: 0.9850\n",
      "Epoch 47: val_loss did not improve from 1.64259\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0426 - acc: 0.9850 - val_loss: 1.6174 - val_acc: 0.7320\n",
      "Epoch 48/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0409 - acc: 0.9860\n",
      "Epoch 48: val_loss improved from 1.64259 to 1.71339, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0409 - acc: 0.9860 - val_loss: 1.7134 - val_acc: 0.7287\n",
      "Epoch 49/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0396 - acc: 0.9864\n",
      "Epoch 49: val_loss did not improve from 1.71339\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0396 - acc: 0.9864 - val_loss: 1.6699 - val_acc: 0.7374\n",
      "Epoch 50/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0390 - acc: 0.9865\n",
      "Epoch 50: val_loss did not improve from 1.71339\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0390 - acc: 0.9865 - val_loss: 1.7102 - val_acc: 0.7203\n",
      "Epoch 51/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0359 - acc: 0.9878\n",
      "Epoch 51: val_loss improved from 1.71339 to 1.80042, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0365 - acc: 0.9875 - val_loss: 1.8004 - val_acc: 0.7231\n",
      "Epoch 52/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0346 - acc: 0.9885\n",
      "Epoch 52: val_loss improved from 1.80042 to 1.83029, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0346 - acc: 0.9885 - val_loss: 1.8303 - val_acc: 0.7318\n",
      "Epoch 53/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9886\n",
      "Epoch 53: val_loss did not improve from 1.83029\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0339 - acc: 0.9886 - val_loss: 1.7096 - val_acc: 0.7232\n",
      "Epoch 54/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0332 - acc: 0.9890\n",
      "Epoch 54: val_loss did not improve from 1.83029\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0332 - acc: 0.9890 - val_loss: 1.8037 - val_acc: 0.7298\n",
      "Epoch 55/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0302 - acc: 0.9900\n",
      "Epoch 55: val_loss improved from 1.83029 to 1.87254, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0302 - acc: 0.9900 - val_loss: 1.8725 - val_acc: 0.7284\n",
      "Epoch 56/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9894\n",
      "Epoch 56: val_loss did not improve from 1.87254\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0310 - acc: 0.9894 - val_loss: 1.8517 - val_acc: 0.7305\n",
      "Epoch 57/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0291 - acc: 0.9903\n",
      "Epoch 57: val_loss improved from 1.87254 to 1.90289, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0291 - acc: 0.9903 - val_loss: 1.9029 - val_acc: 0.7252\n",
      "Epoch 58/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0280 - acc: 0.9906\n",
      "Epoch 58: val_loss improved from 1.90289 to 2.13203, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0280 - acc: 0.9906 - val_loss: 2.1320 - val_acc: 0.7088\n",
      "Epoch 59/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0269 - acc: 0.9909\n",
      "Epoch 59: val_loss did not improve from 2.13203\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0269 - acc: 0.9909 - val_loss: 2.0257 - val_acc: 0.7262\n",
      "Epoch 60/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0272 - acc: 0.9907\n",
      "Epoch 60: val_loss did not improve from 2.13203\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0272 - acc: 0.9907 - val_loss: 1.9850 - val_acc: 0.7262\n",
      "Epoch 61/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0262 - acc: 0.9913\n",
      "Epoch 61: val_loss did not improve from 2.13203\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0262 - acc: 0.9913 - val_loss: 2.0243 - val_acc: 0.7275\n",
      "Epoch 62/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0248 - acc: 0.9916\n",
      "Epoch 62: val_loss did not improve from 2.13203\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0248 - acc: 0.9916 - val_loss: 2.0210 - val_acc: 0.7303\n",
      "Epoch 63/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9919\n",
      "Epoch 63: val_loss did not improve from 2.13203\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0238 - acc: 0.9918 - val_loss: 2.0899 - val_acc: 0.7216\n",
      "Epoch 64/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0238 - acc: 0.9921\n",
      "Epoch 64: val_loss improved from 2.13203 to 2.16821, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0238 - acc: 0.9921 - val_loss: 2.1682 - val_acc: 0.7246\n",
      "Epoch 65/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0225 - acc: 0.9923\n",
      "Epoch 65: val_loss did not improve from 2.16821\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0225 - acc: 0.9923 - val_loss: 2.1113 - val_acc: 0.7256\n",
      "Epoch 66/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0223 - acc: 0.9923\n",
      "Epoch 66: val_loss did not improve from 2.16821\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0223 - acc: 0.9923 - val_loss: 2.0552 - val_acc: 0.7292\n",
      "Epoch 67/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0209 - acc: 0.9927\n",
      "Epoch 67: val_loss improved from 2.16821 to 2.24358, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0209 - acc: 0.9927 - val_loss: 2.2436 - val_acc: 0.7280\n",
      "Epoch 68/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0212 - acc: 0.9930\n",
      "Epoch 68: val_loss did not improve from 2.24358\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0212 - acc: 0.9930 - val_loss: 2.1598 - val_acc: 0.7283\n",
      "Epoch 69/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0204 - acc: 0.9928\n",
      "Epoch 69: val_loss did not improve from 2.24358\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0204 - acc: 0.9928 - val_loss: 2.1501 - val_acc: 0.7278\n",
      "Epoch 70/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0200 - acc: 0.9933\n",
      "Epoch 70: val_loss improved from 2.24358 to 2.24885, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0200 - acc: 0.9933 - val_loss: 2.2488 - val_acc: 0.7258\n",
      "Epoch 71/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0198 - acc: 0.9933\n",
      "Epoch 71: val_loss did not improve from 2.24885\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0198 - acc: 0.9933 - val_loss: 2.1985 - val_acc: 0.7209\n",
      "Epoch 72/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0193 - acc: 0.9933\n",
      "Epoch 72: val_loss improved from 2.24885 to 2.25090, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0193 - acc: 0.9933 - val_loss: 2.2509 - val_acc: 0.7236\n",
      "Epoch 73/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9936\n",
      "Epoch 73: val_loss did not improve from 2.25090\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0188 - acc: 0.9936 - val_loss: 2.1593 - val_acc: 0.7266\n",
      "Epoch 74/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9940\n",
      "Epoch 74: val_loss did not improve from 2.25090\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0180 - acc: 0.9937 - val_loss: 2.2036 - val_acc: 0.7294\n",
      "Epoch 75/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0173 - acc: 0.9941\n",
      "Epoch 75: val_loss improved from 2.25090 to 2.32309, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0173 - acc: 0.9941 - val_loss: 2.3231 - val_acc: 0.7308\n",
      "Epoch 76/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0176 - acc: 0.9940\n",
      "Epoch 76: val_loss did not improve from 2.32309\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0175 - acc: 0.9940 - val_loss: 2.2290 - val_acc: 0.7330\n",
      "Epoch 77/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0168 - acc: 0.9942\n",
      "Epoch 77: val_loss did not improve from 2.32309\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0168 - acc: 0.9942 - val_loss: 2.2467 - val_acc: 0.7252\n",
      "Epoch 78/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0159 - acc: 0.9944\n",
      "Epoch 78: val_loss improved from 2.32309 to 2.41309, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0159 - acc: 0.9944 - val_loss: 2.4131 - val_acc: 0.7276\n",
      "Epoch 79/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9944\n",
      "Epoch 79: val_loss did not improve from 2.41309\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0166 - acc: 0.9944 - val_loss: 2.2635 - val_acc: 0.7302\n",
      "Epoch 80/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0161 - acc: 0.9942\n",
      "Epoch 80: val_loss did not improve from 2.41309\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0161 - acc: 0.9942 - val_loss: 2.2612 - val_acc: 0.7288\n",
      "Epoch 81/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9950\n",
      "Epoch 81: val_loss did not improve from 2.41309\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0158 - acc: 0.9949 - val_loss: 2.3154 - val_acc: 0.7168\n",
      "Epoch 82/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0147 - acc: 0.9949\n",
      "Epoch 82: val_loss did not improve from 2.41309\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0148 - acc: 0.9949 - val_loss: 2.3215 - val_acc: 0.7262\n",
      "Epoch 83/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0142 - acc: 0.9949\n",
      "Epoch 83: val_loss did not improve from 2.41309\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0142 - acc: 0.9949 - val_loss: 2.4096 - val_acc: 0.7261\n",
      "Epoch 84/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0146 - acc: 0.9950\n",
      "Epoch 84: val_loss did not improve from 2.41309\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0146 - acc: 0.9950 - val_loss: 2.4102 - val_acc: 0.7201\n",
      "Epoch 85/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0141 - acc: 0.9951\n",
      "Epoch 85: val_loss improved from 2.41309 to 2.45925, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0141 - acc: 0.9951 - val_loss: 2.4593 - val_acc: 0.7246\n",
      "Epoch 86/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0134 - acc: 0.9950\n",
      "Epoch 86: val_loss improved from 2.45925 to 2.58920, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0134 - acc: 0.9950 - val_loss: 2.5892 - val_acc: 0.7188\n",
      "Epoch 87/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 87: val_loss did not improve from 2.58920\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0137 - acc: 0.9954 - val_loss: 2.4324 - val_acc: 0.7240\n",
      "Epoch 88/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0130 - acc: 0.9953\n",
      "Epoch 88: val_loss did not improve from 2.58920\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0130 - acc: 0.9953 - val_loss: 2.5025 - val_acc: 0.7198\n",
      "Epoch 89/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0131 - acc: 0.9953\n",
      "Epoch 89: val_loss did not improve from 2.58920\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0131 - acc: 0.9953 - val_loss: 2.4970 - val_acc: 0.7205\n",
      "Epoch 90/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0120 - acc: 0.9959\n",
      "Epoch 90: val_loss improved from 2.58920 to 2.61691, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0120 - acc: 0.9959 - val_loss: 2.6169 - val_acc: 0.7226\n",
      "Epoch 91/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0122 - acc: 0.9958\n",
      "Epoch 91: val_loss did not improve from 2.61691\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0122 - acc: 0.9958 - val_loss: 2.4858 - val_acc: 0.7262\n",
      "Epoch 92/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0120 - acc: 0.9957\n",
      "Epoch 92: val_loss did not improve from 2.61691\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0120 - acc: 0.9957 - val_loss: 2.5975 - val_acc: 0.7172\n",
      "Epoch 93/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0125 - acc: 0.9958\n",
      "Epoch 93: val_loss did not improve from 2.61691\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0125 - acc: 0.9958 - val_loss: 2.4863 - val_acc: 0.7242\n",
      "Epoch 94/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0111 - acc: 0.9962\n",
      "Epoch 94: val_loss improved from 2.61691 to 2.62450, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0111 - acc: 0.9961 - val_loss: 2.6245 - val_acc: 0.7255\n",
      "Epoch 95/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0110 - acc: 0.9958\n",
      "Epoch 95: val_loss improved from 2.62450 to 2.64108, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0110 - acc: 0.9958 - val_loss: 2.6411 - val_acc: 0.7210\n",
      "Epoch 96/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0114 - acc: 0.9960\n",
      "Epoch 96: val_loss did not improve from 2.64108\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0114 - acc: 0.9960 - val_loss: 2.6223 - val_acc: 0.7212\n",
      "Epoch 97/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0114 - acc: 0.9961\n",
      "Epoch 97: val_loss did not improve from 2.64108\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0114 - acc: 0.9961 - val_loss: 2.5431 - val_acc: 0.7232\n",
      "Epoch 98/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0100 - acc: 0.9964\n",
      "Epoch 98: val_loss improved from 2.64108 to 2.75707, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0100 - acc: 0.9964 - val_loss: 2.7571 - val_acc: 0.7240\n",
      "Epoch 99/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9962\n",
      "Epoch 99: val_loss did not improve from 2.75707\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0107 - acc: 0.9961 - val_loss: 2.6610 - val_acc: 0.7207\n",
      "Epoch 100/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0106 - acc: 0.9963\n",
      "Epoch 100: val_loss did not improve from 2.75707\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0107 - acc: 0.9963 - val_loss: 2.6498 - val_acc: 0.7228\n",
      "Epoch 101/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0111 - acc: 0.9958\n",
      "Epoch 101: val_loss did not improve from 2.75707\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0111 - acc: 0.9958 - val_loss: 2.5784 - val_acc: 0.7256\n",
      "Epoch 102/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9963\n",
      "Epoch 102: val_loss did not improve from 2.75707\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0103 - acc: 0.9962 - val_loss: 2.6869 - val_acc: 0.7254\n",
      "Epoch 103/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0103 - acc: 0.9961\n",
      "Epoch 103: val_loss did not improve from 2.75707\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0103 - acc: 0.9961 - val_loss: 2.6715 - val_acc: 0.7209\n",
      "Epoch 104/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0095 - acc: 0.9965\n",
      "Epoch 104: val_loss did not improve from 2.75707\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0096 - acc: 0.9965 - val_loss: 2.5921 - val_acc: 0.7282\n",
      "Epoch 105/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0112 - acc: 0.9961\n",
      "Epoch 105: val_loss did not improve from 2.75707\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0112 - acc: 0.9961 - val_loss: 2.6447 - val_acc: 0.7229\n",
      "Epoch 106/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9966\n",
      "Epoch 106: val_loss did not improve from 2.75707\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0087 - acc: 0.9966 - val_loss: 2.7455 - val_acc: 0.7218\n",
      "Epoch 107/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0088 - acc: 0.9966\n",
      "Epoch 107: val_loss did not improve from 2.75707\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0088 - acc: 0.9966 - val_loss: 2.6855 - val_acc: 0.7246\n",
      "Epoch 108/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0088 - acc: 0.9965\n",
      "Epoch 108: val_loss improved from 2.75707 to 2.81424, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0088 - acc: 0.9965 - val_loss: 2.8142 - val_acc: 0.7190\n",
      "Epoch 109/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0083 - acc: 0.9968\n",
      "Epoch 109: val_loss did not improve from 2.81424\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0083 - acc: 0.9968 - val_loss: 2.7874 - val_acc: 0.7209\n",
      "Epoch 110/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0088 - acc: 0.9964\n",
      "Epoch 110: val_loss did not improve from 2.81424\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0088 - acc: 0.9964 - val_loss: 2.7141 - val_acc: 0.7201\n",
      "Epoch 111/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0082 - acc: 0.9969\n",
      "Epoch 111: val_loss did not improve from 2.81424\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0082 - acc: 0.9969 - val_loss: 2.7397 - val_acc: 0.7219\n",
      "Epoch 112/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0084 - acc: 0.9969\n",
      "Epoch 112: val_loss improved from 2.81424 to 2.83283, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0084 - acc: 0.9969 - val_loss: 2.8328 - val_acc: 0.7145\n",
      "Epoch 113/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9972\n",
      "Epoch 113: val_loss improved from 2.83283 to 2.83789, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0076 - acc: 0.9972 - val_loss: 2.8379 - val_acc: 0.7248\n",
      "Epoch 114/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9970\n",
      "Epoch 114: val_loss did not improve from 2.83789\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0080 - acc: 0.9970 - val_loss: 2.7711 - val_acc: 0.7242\n",
      "Epoch 115/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9968\n",
      "Epoch 115: val_loss improved from 2.83789 to 2.84001, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0076 - acc: 0.9968 - val_loss: 2.8400 - val_acc: 0.7213\n",
      "Epoch 116/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9969\n",
      "Epoch 116: val_loss improved from 2.84001 to 2.84571, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0080 - acc: 0.9969 - val_loss: 2.8457 - val_acc: 0.7255\n",
      "Epoch 117/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9974\n",
      "Epoch 117: val_loss improved from 2.84571 to 2.85819, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0074 - acc: 0.9973 - val_loss: 2.8582 - val_acc: 0.7212\n",
      "Epoch 118/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0069 - acc: 0.9972\n",
      "Epoch 118: val_loss improved from 2.85819 to 2.89707, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0069 - acc: 0.9972 - val_loss: 2.8971 - val_acc: 0.7185\n",
      "Epoch 119/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0072 - acc: 0.9974\n",
      "Epoch 119: val_loss did not improve from 2.89707\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0071 - acc: 0.9974 - val_loss: 2.8049 - val_acc: 0.7210\n",
      "Epoch 120/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9975\n",
      "Epoch 120: val_loss improved from 2.89707 to 2.95075, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0063 - acc: 0.9975 - val_loss: 2.9508 - val_acc: 0.7228\n",
      "Epoch 121/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9972\n",
      "Epoch 121: val_loss did not improve from 2.95075\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0076 - acc: 0.9972 - val_loss: 2.9314 - val_acc: 0.7216\n",
      "Epoch 122/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0063 - acc: 0.9974\n",
      "Epoch 122: val_loss improved from 2.95075 to 2.98540, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0063 - acc: 0.9974 - val_loss: 2.9854 - val_acc: 0.7292\n",
      "Epoch 123/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9977\n",
      "Epoch 123: val_loss improved from 2.98540 to 2.99323, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0059 - acc: 0.9977 - val_loss: 2.9932 - val_acc: 0.7268\n",
      "Epoch 124/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0069 - acc: 0.9972\n",
      "Epoch 124: val_loss did not improve from 2.99323\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0069 - acc: 0.9972 - val_loss: 2.8240 - val_acc: 0.7250\n",
      "Epoch 125/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0070 - acc: 0.9974\n",
      "Epoch 125: val_loss did not improve from 2.99323\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0069 - acc: 0.9974 - val_loss: 2.8297 - val_acc: 0.7271\n",
      "Epoch 126/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0065 - acc: 0.9976\n",
      "Epoch 126: val_loss did not improve from 2.99323\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0065 - acc: 0.9976 - val_loss: 2.8804 - val_acc: 0.7277\n",
      "Epoch 127/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0060 - acc: 0.9978\n",
      "Epoch 127: val_loss did not improve from 2.99323\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0060 - acc: 0.9978 - val_loss: 2.9313 - val_acc: 0.7273\n",
      "Epoch 128/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9975\n",
      "Epoch 128: val_loss did not improve from 2.99323\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0062 - acc: 0.9975 - val_loss: 2.9551 - val_acc: 0.7244\n",
      "Epoch 129/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0066 - acc: 0.9975\n",
      "Epoch 129: val_loss did not improve from 2.99323\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0066 - acc: 0.9975 - val_loss: 2.9529 - val_acc: 0.7231\n",
      "Epoch 130/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9977\n",
      "Epoch 130: val_loss improved from 2.99323 to 3.09446, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0058 - acc: 0.9977 - val_loss: 3.0945 - val_acc: 0.7224\n",
      "Epoch 131/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0054 - acc: 0.9981\n",
      "Epoch 131: val_loss did not improve from 3.09446\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0054 - acc: 0.9981 - val_loss: 3.0553 - val_acc: 0.7256\n",
      "Epoch 132/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0053 - acc: 0.9980\n",
      "Epoch 132: val_loss did not improve from 3.09446\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0055 - acc: 0.9979 - val_loss: 2.9899 - val_acc: 0.7230\n",
      "Epoch 133/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9976\n",
      "Epoch 133: val_loss did not improve from 3.09446\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0060 - acc: 0.9976 - val_loss: 2.9510 - val_acc: 0.7242\n",
      "Epoch 134/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0049 - acc: 0.9981\n",
      "Epoch 134: val_loss did not improve from 3.09446\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0051 - acc: 0.9981 - val_loss: 3.0937 - val_acc: 0.7280\n",
      "Epoch 135/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9978\n",
      "Epoch 135: val_loss improved from 3.09446 to 3.10643, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0053 - acc: 0.9978 - val_loss: 3.1064 - val_acc: 0.7258\n",
      "Epoch 136/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0056 - acc: 0.9980\n",
      "Epoch 136: val_loss improved from 3.10643 to 3.13907, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0056 - acc: 0.9980 - val_loss: 3.1391 - val_acc: 0.7268\n",
      "Epoch 137/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0047 - acc: 0.9982\n",
      "Epoch 137: val_loss improved from 3.13907 to 3.20859, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0047 - acc: 0.9982 - val_loss: 3.2086 - val_acc: 0.7270\n",
      "Epoch 138/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0047 - acc: 0.9981\n",
      "Epoch 138: val_loss improved from 3.20859 to 3.23087, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - acc: 0.9981 - val_loss: 3.2309 - val_acc: 0.7244\n",
      "Epoch 139/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9981\n",
      "Epoch 139: val_loss did not improve from 3.23087\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - acc: 0.9981 - val_loss: 3.2098 - val_acc: 0.7301\n",
      "Epoch 140/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0047 - acc: 0.9980\n",
      "Epoch 140: val_loss improved from 3.23087 to 3.25458, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - acc: 0.9980 - val_loss: 3.2546 - val_acc: 0.7250\n",
      "Epoch 141/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0047 - acc: 0.9982\n",
      "Epoch 141: val_loss improved from 3.25458 to 3.27927, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - acc: 0.9982 - val_loss: 3.2793 - val_acc: 0.7240\n",
      "Epoch 142/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0045 - acc: 0.9981\n",
      "Epoch 142: val_loss improved from 3.27927 to 3.31820, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - acc: 0.9980 - val_loss: 3.3182 - val_acc: 0.7209\n",
      "Epoch 143/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0043 - acc: 0.9982\n",
      "Epoch 143: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0043 - acc: 0.9982 - val_loss: 3.3022 - val_acc: 0.7195\n",
      "Epoch 144/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9981\n",
      "Epoch 144: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0053 - acc: 0.9981 - val_loss: 3.2481 - val_acc: 0.7240\n",
      "Epoch 145/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9979\n",
      "Epoch 145: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0057 - acc: 0.9979 - val_loss: 3.1295 - val_acc: 0.7250\n",
      "Epoch 146/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9978\n",
      "Epoch 146: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0052 - acc: 0.9978 - val_loss: 3.1542 - val_acc: 0.7272\n",
      "Epoch 147/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9980\n",
      "Epoch 147: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0053 - acc: 0.9980 - val_loss: 3.2030 - val_acc: 0.7189\n",
      "Epoch 148/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9976\n",
      "Epoch 148: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0057 - acc: 0.9976 - val_loss: 3.1178 - val_acc: 0.7254\n",
      "Epoch 149/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9978\n",
      "Epoch 149: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0055 - acc: 0.9978 - val_loss: 3.0656 - val_acc: 0.7256\n",
      "Epoch 150/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9980\n",
      "Epoch 150: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0049 - acc: 0.9980 - val_loss: 3.1711 - val_acc: 0.7248\n",
      "Epoch 151/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0054 - acc: 0.9980\n",
      "Epoch 151: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0054 - acc: 0.9980 - val_loss: 3.1149 - val_acc: 0.7288\n",
      "Epoch 152/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9980\n",
      "Epoch 152: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0050 - acc: 0.9980 - val_loss: 3.0839 - val_acc: 0.7262\n",
      "Epoch 153/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.9981\n",
      "Epoch 153: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0050 - acc: 0.9981 - val_loss: 3.1027 - val_acc: 0.7230\n",
      "Epoch 154/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0043 - acc: 0.9983\n",
      "Epoch 154: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0043 - acc: 0.9983 - val_loss: 3.1361 - val_acc: 0.7259\n",
      "Epoch 155/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9983\n",
      "Epoch 155: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 3.1920 - val_acc: 0.7283\n",
      "Epoch 156/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0040 - acc: 0.9983\n",
      "Epoch 156: val_loss did not improve from 3.31820\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 3.2940 - val_acc: 0.7263\n",
      "Epoch 157/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9984\n",
      "Epoch 157: val_loss improved from 3.31820 to 3.34446, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0038 - acc: 0.9984 - val_loss: 3.3445 - val_acc: 0.7274\n",
      "Epoch 158/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9983\n",
      "Epoch 158: val_loss improved from 3.34446 to 3.35777, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0039 - acc: 0.9983 - val_loss: 3.3578 - val_acc: 0.7274\n",
      "Epoch 159/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0041 - acc: 0.9982\n",
      "Epoch 159: val_loss improved from 3.35777 to 3.36110, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 3.3611 - val_acc: 0.7240\n",
      "Epoch 160/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9984\n",
      "Epoch 160: val_loss did not improve from 3.36110\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9984 - val_loss: 3.3513 - val_acc: 0.7324\n",
      "Epoch 161/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0037 - acc: 0.9983\n",
      "Epoch 161: val_loss improved from 3.36110 to 3.41526, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0037 - acc: 0.9983 - val_loss: 3.4153 - val_acc: 0.7290\n",
      "Epoch 162/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9984\n",
      "Epoch 162: val_loss improved from 3.41526 to 3.45297, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9983 - val_loss: 3.4530 - val_acc: 0.7259\n",
      "Epoch 163/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9983\n",
      "Epoch 163: val_loss did not improve from 3.45297\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 3.4327 - val_acc: 0.7248\n",
      "Epoch 164/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0051 - acc: 0.9980\n",
      "Epoch 164: val_loss did not improve from 3.45297\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0051 - acc: 0.9980 - val_loss: 3.2037 - val_acc: 0.7266\n",
      "Epoch 165/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9980\n",
      "Epoch 165: val_loss did not improve from 3.45297\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - acc: 0.9980 - val_loss: 3.3359 - val_acc: 0.7216\n",
      "Epoch 166/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9980\n",
      "Epoch 166: val_loss did not improve from 3.45297\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0052 - acc: 0.9980 - val_loss: 3.1461 - val_acc: 0.7243\n",
      "Epoch 167/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0047 - acc: 0.9982\n",
      "Epoch 167: val_loss improved from 3.45297 to 3.45949, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0046 - acc: 0.9982 - val_loss: 3.4595 - val_acc: 0.7176\n",
      "Epoch 168/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0041 - acc: 0.9985\n",
      "Epoch 168: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0041 - acc: 0.9985 - val_loss: 3.3443 - val_acc: 0.7225\n",
      "Epoch 169/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0053 - acc: 0.9981\n",
      "Epoch 169: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0053 - acc: 0.9981 - val_loss: 3.1766 - val_acc: 0.7227\n",
      "Epoch 170/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9983\n",
      "Epoch 170: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 3.2455 - val_acc: 0.7244\n",
      "Epoch 171/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0051 - acc: 0.9979\n",
      "Epoch 171: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0051 - acc: 0.9979 - val_loss: 3.1164 - val_acc: 0.7216\n",
      "Epoch 172/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0046 - acc: 0.9982\n",
      "Epoch 172: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0046 - acc: 0.9982 - val_loss: 3.1473 - val_acc: 0.7274\n",
      "Epoch 173/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9984\n",
      "Epoch 173: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 3.1788 - val_acc: 0.7267\n",
      "Epoch 174/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0038 - acc: 0.9985\n",
      "Epoch 174: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0038 - acc: 0.9985 - val_loss: 3.2782 - val_acc: 0.7258\n",
      "Epoch 175/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9982\n",
      "Epoch 175: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0047 - acc: 0.9982 - val_loss: 3.2487 - val_acc: 0.7270\n",
      "Epoch 176/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0045 - acc: 0.9981\n",
      "Epoch 176: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - acc: 0.9981 - val_loss: 3.2254 - val_acc: 0.7198\n",
      "Epoch 177/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9983\n",
      "Epoch 177: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0037 - acc: 0.9984 - val_loss: 3.2943 - val_acc: 0.7248\n",
      "Epoch 178/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0035 - acc: 0.9985\n",
      "Epoch 178: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9985 - val_loss: 3.2890 - val_acc: 0.7294\n",
      "Epoch 179/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9983\n",
      "Epoch 179: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 3.2171 - val_acc: 0.7294\n",
      "Epoch 180/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9984\n",
      "Epoch 180: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0039 - acc: 0.9984 - val_loss: 3.2016 - val_acc: 0.7274\n",
      "Epoch 181/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9986\n",
      "Epoch 181: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9986 - val_loss: 3.2826 - val_acc: 0.7272\n",
      "Epoch 182/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0034 - acc: 0.9986\n",
      "Epoch 182: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 3.3422 - val_acc: 0.7238\n",
      "Epoch 183/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9986\n",
      "Epoch 183: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9986 - val_loss: 3.3651 - val_acc: 0.7268\n",
      "Epoch 184/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9987\n",
      "Epoch 184: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9987 - val_loss: 3.4112 - val_acc: 0.7269\n",
      "Epoch 185/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9986\n",
      "Epoch 185: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0032 - acc: 0.9986 - val_loss: 3.4560 - val_acc: 0.7242\n",
      "Epoch 186/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9987\n",
      "Epoch 186: val_loss did not improve from 3.45949\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9987 - val_loss: 3.4252 - val_acc: 0.7260\n",
      "Epoch 187/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9986\n",
      "Epoch 187: val_loss improved from 3.45949 to 3.48346, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0033 - acc: 0.9986 - val_loss: 3.4835 - val_acc: 0.7280\n",
      "Epoch 188/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9985\n",
      "Epoch 188: val_loss did not improve from 3.48346\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 3.4828 - val_acc: 0.7254\n",
      "Epoch 189/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9984\n",
      "Epoch 189: val_loss improved from 3.48346 to 3.48372, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9985 - val_loss: 3.4837 - val_acc: 0.7276\n",
      "Epoch 190/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9985\n",
      "Epoch 190: val_loss improved from 3.48372 to 3.50353, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 3.5035 - val_acc: 0.7270\n",
      "Epoch 191/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0034 - acc: 0.9985\n",
      "Epoch 191: val_loss improved from 3.50353 to 3.54214, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 3.5421 - val_acc: 0.7267\n",
      "Epoch 192/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9985\n",
      "Epoch 192: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 3.5347 - val_acc: 0.7250\n",
      "Epoch 193/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9984\n",
      "Epoch 193: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9984 - val_loss: 3.3720 - val_acc: 0.7232\n",
      "Epoch 194/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9984\n",
      "Epoch 194: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9984 - val_loss: 3.3665 - val_acc: 0.7300\n",
      "Epoch 195/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0043 - acc: 0.9983\n",
      "Epoch 195: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0043 - acc: 0.9983 - val_loss: 3.4016 - val_acc: 0.7190\n",
      "Epoch 196/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0043 - acc: 0.9981\n",
      "Epoch 196: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0043 - acc: 0.9981 - val_loss: 3.3732 - val_acc: 0.7261\n",
      "Epoch 197/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0044 - acc: 0.9982\n",
      "Epoch 197: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - acc: 0.9982 - val_loss: 3.2850 - val_acc: 0.7290\n",
      "Epoch 198/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0051 - acc: 0.9980\n",
      "Epoch 198: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0050 - acc: 0.9980 - val_loss: 3.2760 - val_acc: 0.7270\n",
      "Epoch 199/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9984\n",
      "Epoch 199: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9984 - val_loss: 3.3347 - val_acc: 0.7270\n",
      "Epoch 200/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9983\n",
      "Epoch 200: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0044 - acc: 0.9983 - val_loss: 3.3425 - val_acc: 0.7195\n",
      "Epoch 201/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0036 - acc: 0.9984\n",
      "Epoch 201: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0036 - acc: 0.9984 - val_loss: 3.2692 - val_acc: 0.7269\n",
      "Epoch 202/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9982\n",
      "Epoch 202: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9982 - val_loss: 3.3209 - val_acc: 0.7266\n",
      "Epoch 203/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9983\n",
      "Epoch 203: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0037 - acc: 0.9983 - val_loss: 3.3464 - val_acc: 0.7218\n",
      "Epoch 204/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0035 - acc: 0.9984\n",
      "Epoch 204: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9984 - val_loss: 3.3741 - val_acc: 0.7274\n",
      "Epoch 205/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9987\n",
      "Epoch 205: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0032 - acc: 0.9986 - val_loss: 3.3753 - val_acc: 0.7252\n",
      "Epoch 206/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9986\n",
      "Epoch 206: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 3.4426 - val_acc: 0.7276\n",
      "Epoch 207/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0031 - acc: 0.9987\n",
      "Epoch 207: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 3.4429 - val_acc: 0.7306\n",
      "Epoch 208/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9986\n",
      "Epoch 208: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9986 - val_loss: 3.4535 - val_acc: 0.7282\n",
      "Epoch 209/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9983\n",
      "Epoch 209: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 3.5035 - val_acc: 0.7264\n",
      "Epoch 210/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9985\n",
      "Epoch 210: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 3.4318 - val_acc: 0.7286\n",
      "Epoch 211/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0031 - acc: 0.9985\n",
      "Epoch 211: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0031 - acc: 0.9985 - val_loss: 3.4416 - val_acc: 0.7279\n",
      "Epoch 212/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9985\n",
      "Epoch 212: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 3.5301 - val_acc: 0.7265\n",
      "Epoch 213/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9984\n",
      "Epoch 213: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0033 - acc: 0.9984 - val_loss: 3.3903 - val_acc: 0.7300\n",
      "Epoch 214/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9984\n",
      "Epoch 214: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 3.4709 - val_acc: 0.7267\n",
      "Epoch 215/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9985\n",
      "Epoch 215: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 3.4028 - val_acc: 0.7332\n",
      "Epoch 216/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9984\n",
      "Epoch 216: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9984 - val_loss: 3.4724 - val_acc: 0.7250\n",
      "Epoch 217/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9983\n",
      "Epoch 217: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 3.3636 - val_acc: 0.7223\n",
      "Epoch 218/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9987\n",
      "Epoch 218: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9986 - val_loss: 3.3606 - val_acc: 0.7272\n",
      "Epoch 219/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9986\n",
      "Epoch 219: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9986 - val_loss: 3.3615 - val_acc: 0.7277\n",
      "Epoch 220/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9986\n",
      "Epoch 220: val_loss did not improve from 3.54214\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9985 - val_loss: 3.4995 - val_acc: 0.7278\n",
      "Epoch 221/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9986\n",
      "Epoch 221: val_loss improved from 3.54214 to 3.55924, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 3.5592 - val_acc: 0.7226\n",
      "Epoch 222/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9986\n",
      "Epoch 222: val_loss improved from 3.55924 to 3.57936, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 3.5794 - val_acc: 0.7260\n",
      "Epoch 223/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9985\n",
      "Epoch 223: val_loss improved from 3.57936 to 3.58667, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 3.5867 - val_acc: 0.7273\n",
      "Epoch 224/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9985\n",
      "Epoch 224: val_loss improved from 3.58667 to 3.65052, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9984 - val_loss: 3.6505 - val_acc: 0.7255\n",
      "Epoch 225/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9986\n",
      "Epoch 225: val_loss did not improve from 3.65052\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 3.6380 - val_acc: 0.7288\n",
      "Epoch 226/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9985\n",
      "Epoch 226: val_loss improved from 3.65052 to 3.66535, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 3.6653 - val_acc: 0.7278\n",
      "Epoch 227/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9986\n",
      "Epoch 227: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9984 - val_loss: 3.6023 - val_acc: 0.7266\n",
      "Epoch 228/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9984\n",
      "Epoch 228: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 3.4811 - val_acc: 0.7270\n",
      "Epoch 229/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9982\n",
      "Epoch 229: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - acc: 0.9982 - val_loss: 3.4759 - val_acc: 0.7248\n",
      "Epoch 230/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9983\n",
      "Epoch 230: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0038 - acc: 0.9983 - val_loss: 3.4367 - val_acc: 0.7279\n",
      "Epoch 231/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9984\n",
      "Epoch 231: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 3.4413 - val_acc: 0.7296\n",
      "Epoch 232/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0036 - acc: 0.9982\n",
      "Epoch 232: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9982 - val_loss: 3.4919 - val_acc: 0.7255\n",
      "Epoch 233/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9985\n",
      "Epoch 233: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9985 - val_loss: 3.4960 - val_acc: 0.7250\n",
      "Epoch 234/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9987\n",
      "Epoch 234: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9987 - val_loss: 3.3761 - val_acc: 0.7284\n",
      "Epoch 235/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9985\n",
      "Epoch 235: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 3.3878 - val_acc: 0.7292\n",
      "Epoch 236/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9985\n",
      "Epoch 236: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9985 - val_loss: 3.5419 - val_acc: 0.7242\n",
      "Epoch 237/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9984\n",
      "Epoch 237: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9984 - val_loss: 3.5037 - val_acc: 0.7274\n",
      "Epoch 238/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9983\n",
      "Epoch 238: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9984 - val_loss: 3.4072 - val_acc: 0.7283\n",
      "Epoch 239/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9982\n",
      "Epoch 239: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9982 - val_loss: 3.3045 - val_acc: 0.7351\n",
      "Epoch 240/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9984\n",
      "Epoch 240: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 3.3924 - val_acc: 0.7218\n",
      "Epoch 241/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9985\n",
      "Epoch 241: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 3.3647 - val_acc: 0.7284\n",
      "Epoch 242/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9987\n",
      "Epoch 242: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9988 - val_loss: 3.4354 - val_acc: 0.7254\n",
      "Epoch 243/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0033 - acc: 0.9984\n",
      "Epoch 243: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9984 - val_loss: 3.4092 - val_acc: 0.7288\n",
      "Epoch 244/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9985\n",
      "Epoch 244: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 3.3723 - val_acc: 0.7271\n",
      "Epoch 245/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0030 - acc: 0.9986\n",
      "Epoch 245: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0030 - acc: 0.9986 - val_loss: 3.4972 - val_acc: 0.7246\n",
      "Epoch 246/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9985\n",
      "Epoch 246: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 3.4920 - val_acc: 0.7264\n",
      "Epoch 247/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0029 - acc: 0.9985\n",
      "Epoch 247: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 3.5334 - val_acc: 0.7283\n",
      "Epoch 248/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9985\n",
      "Epoch 248: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 3.5707 - val_acc: 0.7272\n",
      "Epoch 249/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 249: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 3.6079 - val_acc: 0.7254\n",
      "Epoch 250/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0087 - acc: 0.9981\n",
      "Epoch 250: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0085 - acc: 0.9981 - val_loss: 3.5099 - val_acc: 0.7286\n",
      "Epoch 251/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9987\n",
      "Epoch 251: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9987 - val_loss: 3.5487 - val_acc: 0.7261\n",
      "Epoch 252/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9984\n",
      "Epoch 252: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0038 - acc: 0.9984 - val_loss: 3.4336 - val_acc: 0.7260\n",
      "Epoch 253/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9987\n",
      "Epoch 253: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9987 - val_loss: 3.5545 - val_acc: 0.7292\n",
      "Epoch 254/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9983\n",
      "Epoch 254: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9984 - val_loss: 3.4882 - val_acc: 0.7274\n",
      "Epoch 255/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9984\n",
      "Epoch 255: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 3.3717 - val_acc: 0.7270\n",
      "Epoch 256/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9983\n",
      "Epoch 256: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9984 - val_loss: 3.4225 - val_acc: 0.7253\n",
      "Epoch 257/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9985\n",
      "Epoch 257: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 3.4944 - val_acc: 0.7242\n",
      "Epoch 258/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0031 - acc: 0.9984\n",
      "Epoch 258: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 3.4903 - val_acc: 0.7254\n",
      "Epoch 259/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 259: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 3.5446 - val_acc: 0.7292\n",
      "Epoch 260/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 260: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.5273 - val_acc: 0.7267\n",
      "Epoch 261/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 261: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 3.4898 - val_acc: 0.7306\n",
      "Epoch 262/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9986\n",
      "Epoch 262: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9986 - val_loss: 3.5617 - val_acc: 0.7273\n",
      "Epoch 263/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 263: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 3.6056 - val_acc: 0.7263\n",
      "Epoch 264/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9985\n",
      "Epoch 264: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.6311 - val_acc: 0.7260\n",
      "Epoch 265/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9985\n",
      "Epoch 265: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 3.6307 - val_acc: 0.7282\n",
      "Epoch 266/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0029 - acc: 0.9985\n",
      "Epoch 266: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 3.6418 - val_acc: 0.7258\n",
      "Epoch 267/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 267: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.6606 - val_acc: 0.7266\n",
      "Epoch 268/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9986\n",
      "Epoch 268: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.6170 - val_acc: 0.7261\n",
      "Epoch 269/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0028 - acc: 0.9986\n",
      "Epoch 269: val_loss did not improve from 3.66535\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0028 - acc: 0.9986 - val_loss: 3.6474 - val_acc: 0.7272\n",
      "Epoch 270/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 270: val_loss improved from 3.66535 to 3.67064, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.6706 - val_acc: 0.7276\n",
      "Epoch 271/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9986\n",
      "Epoch 271: val_loss improved from 3.67064 to 3.70411, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.7041 - val_acc: 0.7275\n",
      "Epoch 272/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9987\n",
      "Epoch 272: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0028 - acc: 0.9986 - val_loss: 3.6952 - val_acc: 0.7246\n",
      "Epoch 273/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9983\n",
      "Epoch 273: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9983 - val_loss: 3.6082 - val_acc: 0.7282\n",
      "Epoch 274/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9986\n",
      "Epoch 274: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 3.4510 - val_acc: 0.7270\n",
      "Epoch 275/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9985\n",
      "Epoch 275: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9984 - val_loss: 3.5132 - val_acc: 0.7268\n",
      "Epoch 276/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 276: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 3.5483 - val_acc: 0.7297\n",
      "Epoch 277/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9986\n",
      "Epoch 277: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.5702 - val_acc: 0.7271\n",
      "Epoch 278/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9986\n",
      "Epoch 278: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9986 - val_loss: 3.6016 - val_acc: 0.7259\n",
      "Epoch 279/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9984\n",
      "Epoch 279: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 3.5912 - val_acc: 0.7272\n",
      "Epoch 280/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9985\n",
      "Epoch 280: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9985 - val_loss: 3.5823 - val_acc: 0.7232\n",
      "Epoch 281/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9983\n",
      "Epoch 281: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9983 - val_loss: 3.5924 - val_acc: 0.7268\n",
      "Epoch 282/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9983\n",
      "Epoch 282: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9983 - val_loss: 3.4791 - val_acc: 0.7227\n",
      "Epoch 283/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9981\n",
      "Epoch 283: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0046 - acc: 0.9981 - val_loss: 3.3947 - val_acc: 0.7284\n",
      "Epoch 284/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0040 - acc: 0.9981\n",
      "Epoch 284: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9981 - val_loss: 3.3531 - val_acc: 0.7267\n",
      "Epoch 285/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9983\n",
      "Epoch 285: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9982 - val_loss: 3.3024 - val_acc: 0.7300\n",
      "Epoch 286/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9981\n",
      "Epoch 286: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0043 - acc: 0.9981 - val_loss: 3.3932 - val_acc: 0.7242\n",
      "Epoch 287/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9985\n",
      "Epoch 287: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9985 - val_loss: 3.2787 - val_acc: 0.7260\n",
      "Epoch 288/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0031 - acc: 0.9983\n",
      "Epoch 288: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9983 - val_loss: 3.3640 - val_acc: 0.7292\n",
      "Epoch 289/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 289: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.4350 - val_acc: 0.7268\n",
      "Epoch 290/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 290: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 3.4972 - val_acc: 0.7288\n",
      "Epoch 291/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 291: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 3.5654 - val_acc: 0.7257\n",
      "Epoch 292/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 292: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.5437 - val_acc: 0.7281\n",
      "Epoch 293/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9985\n",
      "Epoch 293: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.5521 - val_acc: 0.7274\n",
      "Epoch 294/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0028 - acc: 0.9984\n",
      "Epoch 294: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 3.5547 - val_acc: 0.7264\n",
      "Epoch 295/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9986\n",
      "Epoch 295: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9986 - val_loss: 3.5969 - val_acc: 0.7280\n",
      "Epoch 296/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 296: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.6293 - val_acc: 0.7261\n",
      "Epoch 297/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9987\n",
      "Epoch 297: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9987 - val_loss: 3.6174 - val_acc: 0.7282\n",
      "Epoch 298/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9987\n",
      "Epoch 298: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9987 - val_loss: 3.6426 - val_acc: 0.7263\n",
      "Epoch 299/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 299: val_loss did not improve from 3.70411\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.6607 - val_acc: 0.7270\n",
      "Epoch 300/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 300: val_loss improved from 3.70411 to 3.70438, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.7044 - val_acc: 0.7256\n",
      "Epoch 301/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 301: val_loss did not improve from 3.70438\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 3.6610 - val_acc: 0.7238\n",
      "Epoch 302/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 302: val_loss did not improve from 3.70438\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9984 - val_loss: 3.6677 - val_acc: 0.7260\n",
      "Epoch 303/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 303: val_loss did not improve from 3.70438\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.6872 - val_acc: 0.7253\n",
      "Epoch 304/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 304: val_loss improved from 3.70438 to 3.70451, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.7045 - val_acc: 0.7230\n",
      "Epoch 305/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9987\n",
      "Epoch 305: val_loss did not improve from 3.70451\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.6791 - val_acc: 0.7264\n",
      "Epoch 306/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 306: val_loss improved from 3.70451 to 3.73545, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.7355 - val_acc: 0.7229\n",
      "Epoch 307/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 307: val_loss did not improve from 3.73545\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 3.7345 - val_acc: 0.7225\n",
      "Epoch 308/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 308: val_loss did not improve from 3.73545\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.7314 - val_acc: 0.7251\n",
      "Epoch 309/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 309: val_loss improved from 3.73545 to 3.78190, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.7819 - val_acc: 0.7252\n",
      "Epoch 310/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 310: val_loss did not improve from 3.78190\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.7650 - val_acc: 0.7264\n",
      "Epoch 311/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 311: val_loss improved from 3.78190 to 3.78520, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.7852 - val_acc: 0.7252\n",
      "Epoch 312/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9983\n",
      "Epoch 312: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0038 - acc: 0.9983 - val_loss: 3.5857 - val_acc: 0.7274\n",
      "Epoch 313/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9981\n",
      "Epoch 313: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0048 - acc: 0.9981 - val_loss: 3.3951 - val_acc: 0.7247\n",
      "Epoch 314/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9983\n",
      "Epoch 314: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9983 - val_loss: 3.4136 - val_acc: 0.7245\n",
      "Epoch 315/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 315: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.5055 - val_acc: 0.7244\n",
      "Epoch 316/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9985\n",
      "Epoch 316: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 3.4258 - val_acc: 0.7266\n",
      "Epoch 317/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9985\n",
      "Epoch 317: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 3.4080 - val_acc: 0.7234\n",
      "Epoch 318/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9982\n",
      "Epoch 318: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0038 - acc: 0.9982 - val_loss: 3.3413 - val_acc: 0.7281\n",
      "Epoch 319/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9981\n",
      "Epoch 319: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0037 - acc: 0.9981 - val_loss: 3.3791 - val_acc: 0.7224\n",
      "Epoch 320/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9983\n",
      "Epoch 320: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0039 - acc: 0.9983 - val_loss: 3.3019 - val_acc: 0.7278\n",
      "Epoch 321/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9985\n",
      "Epoch 321: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.3301 - val_acc: 0.7312\n",
      "Epoch 322/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9985\n",
      "Epoch 322: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.3871 - val_acc: 0.7286\n",
      "Epoch 323/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 323: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.4195 - val_acc: 0.7288\n",
      "Epoch 324/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 324: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.4725 - val_acc: 0.7276\n",
      "Epoch 325/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 325: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.4753 - val_acc: 0.7278\n",
      "Epoch 326/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 326: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9984 - val_loss: 3.5148 - val_acc: 0.7273\n",
      "Epoch 327/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 327: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.5851 - val_acc: 0.7280\n",
      "Epoch 328/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 328: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 3.5716 - val_acc: 0.7286\n",
      "Epoch 329/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9987\n",
      "Epoch 329: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9987 - val_loss: 3.5932 - val_acc: 0.7282\n",
      "Epoch 330/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9987\n",
      "Epoch 330: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9987 - val_loss: 3.6130 - val_acc: 0.7292\n",
      "Epoch 331/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 331: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 3.5359 - val_acc: 0.7262\n",
      "Epoch 332/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 332: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9987 - val_loss: 3.5865 - val_acc: 0.7275\n",
      "Epoch 333/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 333: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 3.6237 - val_acc: 0.7295\n",
      "Epoch 334/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 334: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.6360 - val_acc: 0.7306\n",
      "Epoch 335/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 335: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9984 - val_loss: 3.6612 - val_acc: 0.7267\n",
      "Epoch 336/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 336: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.6618 - val_acc: 0.7266\n",
      "Epoch 337/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 337: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.6364 - val_acc: 0.7292\n",
      "Epoch 338/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 338: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.6916 - val_acc: 0.7278\n",
      "Epoch 339/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 339: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.7035 - val_acc: 0.7260\n",
      "Epoch 340/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 340: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.7240 - val_acc: 0.7261\n",
      "Epoch 341/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 341: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.6768 - val_acc: 0.7278\n",
      "Epoch 342/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 342: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.7158 - val_acc: 0.7260\n",
      "Epoch 343/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9984\n",
      "Epoch 343: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0031 - acc: 0.9984 - val_loss: 3.5278 - val_acc: 0.7262\n",
      "Epoch 344/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9985\n",
      "Epoch 344: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 3.5641 - val_acc: 0.7301\n",
      "Epoch 345/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9984\n",
      "Epoch 345: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9984 - val_loss: 3.5838 - val_acc: 0.7284\n",
      "Epoch 346/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9986\n",
      "Epoch 346: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9986 - val_loss: 3.6231 - val_acc: 0.7259\n",
      "Epoch 347/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9981\n",
      "Epoch 347: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 3.5194 - val_acc: 0.7307\n",
      "Epoch 348/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9983\n",
      "Epoch 348: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0038 - acc: 0.9983 - val_loss: 3.4862 - val_acc: 0.7247\n",
      "Epoch 349/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9983\n",
      "Epoch 349: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 3.2791 - val_acc: 0.7302\n",
      "Epoch 350/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9983\n",
      "Epoch 350: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9983 - val_loss: 3.3900 - val_acc: 0.7270\n",
      "Epoch 351/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9983\n",
      "Epoch 351: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 3.4312 - val_acc: 0.7278\n",
      "Epoch 352/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 352: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.4123 - val_acc: 0.7293\n",
      "Epoch 353/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 353: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.5156 - val_acc: 0.7245\n",
      "Epoch 354/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 354: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.5442 - val_acc: 0.7256\n",
      "Epoch 355/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 355: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 3.5711 - val_acc: 0.7257\n",
      "Epoch 356/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 356: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9987 - val_loss: 3.5978 - val_acc: 0.7258\n",
      "Epoch 357/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9984\n",
      "Epoch 357: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9984 - val_loss: 3.5849 - val_acc: 0.7280\n",
      "Epoch 358/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 358: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.6305 - val_acc: 0.7284\n",
      "Epoch 359/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 359: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 3.6012 - val_acc: 0.7296\n",
      "Epoch 360/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 360: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.5939 - val_acc: 0.7272\n",
      "Epoch 361/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 361: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.6411 - val_acc: 0.7288\n",
      "Epoch 362/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 362: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.6453 - val_acc: 0.7258\n",
      "Epoch 363/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 363: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.6757 - val_acc: 0.7264\n",
      "Epoch 364/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9983\n",
      "Epoch 364: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9984 - val_loss: 3.6663 - val_acc: 0.7260\n",
      "Epoch 365/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 365: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.6847 - val_acc: 0.7256\n",
      "Epoch 366/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 366: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.6778 - val_acc: 0.7265\n",
      "Epoch 367/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 367: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6529 - val_acc: 0.7304\n",
      "Epoch 368/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 368: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.6860 - val_acc: 0.7274\n",
      "Epoch 369/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 369: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.7159 - val_acc: 0.7258\n",
      "Epoch 370/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 370: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.7257 - val_acc: 0.7282\n",
      "Epoch 371/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 371: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.7617 - val_acc: 0.7290\n",
      "Epoch 372/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 372: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.7591 - val_acc: 0.7263\n",
      "Epoch 373/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 373: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 3.7415 - val_acc: 0.7258\n",
      "Epoch 374/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9985\n",
      "Epoch 374: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.6598 - val_acc: 0.7242\n",
      "Epoch 375/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0042 - acc: 0.9981\n",
      "Epoch 375: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0042 - acc: 0.9981 - val_loss: 3.5668 - val_acc: 0.7289\n",
      "Epoch 376/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9983\n",
      "Epoch 376: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 3.4217 - val_acc: 0.7257\n",
      "Epoch 377/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9985\n",
      "Epoch 377: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 3.5432 - val_acc: 0.7271\n",
      "Epoch 378/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 378: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 3.5345 - val_acc: 0.7257\n",
      "Epoch 379/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 379: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.5622 - val_acc: 0.7244\n",
      "Epoch 380/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 380: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.5193 - val_acc: 0.7277\n",
      "Epoch 381/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 381: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.5651 - val_acc: 0.7260\n",
      "Epoch 382/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 382: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.4311 - val_acc: 0.7289\n",
      "Epoch 383/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 383: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 3.5630 - val_acc: 0.7241\n",
      "Epoch 384/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 384: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.5906 - val_acc: 0.7278\n",
      "Epoch 385/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 385: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.6266 - val_acc: 0.7253\n",
      "Epoch 386/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 386: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 3.6147 - val_acc: 0.7251\n",
      "Epoch 387/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9936\n",
      "Epoch 387: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0312 - acc: 0.9937 - val_loss: 3.3752 - val_acc: 0.7310\n",
      "Epoch 388/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9981\n",
      "Epoch 388: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0042 - acc: 0.9982 - val_loss: 3.2619 - val_acc: 0.7275\n",
      "Epoch 389/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9985\n",
      "Epoch 389: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.3947 - val_acc: 0.7262\n",
      "Epoch 390/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 390: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.4243 - val_acc: 0.7278\n",
      "Epoch 391/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 391: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.4531 - val_acc: 0.7265\n",
      "Epoch 392/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 392: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9984 - val_loss: 3.4826 - val_acc: 0.7298\n",
      "Epoch 393/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 393: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6034 - val_acc: 0.7250\n",
      "Epoch 394/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9984\n",
      "Epoch 394: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9984 - val_loss: 3.5931 - val_acc: 0.7272\n",
      "Epoch 395/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 395: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 3.6194 - val_acc: 0.7280\n",
      "Epoch 396/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 396: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6088 - val_acc: 0.7258\n",
      "Epoch 397/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 397: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7015 - val_acc: 0.7266\n",
      "Epoch 398/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 398: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.7028 - val_acc: 0.7264\n",
      "Epoch 399/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 399: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 3.6622 - val_acc: 0.7282\n",
      "Epoch 400/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9984\n",
      "Epoch 400: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9984 - val_loss: 3.7094 - val_acc: 0.7257\n",
      "Epoch 401/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 401: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6996 - val_acc: 0.7256\n",
      "Epoch 402/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 402: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 3.7124 - val_acc: 0.7269\n",
      "Epoch 403/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 403: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.7341 - val_acc: 0.7262\n",
      "Epoch 404/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 404: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.7304 - val_acc: 0.7262\n",
      "Epoch 405/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 405: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.7531 - val_acc: 0.7248\n",
      "Epoch 406/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 406: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.7548 - val_acc: 0.7240\n",
      "Epoch 407/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 407: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0024 - acc: 0.9987 - val_loss: 3.7304 - val_acc: 0.7260\n",
      "Epoch 408/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 408: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.7585 - val_acc: 0.7273\n",
      "Epoch 409/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 409: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7600 - val_acc: 0.7241\n",
      "Epoch 410/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9984\n",
      "Epoch 410: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 3.6226 - val_acc: 0.7246\n",
      "Epoch 411/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 411: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 3.5249 - val_acc: 0.7281\n",
      "Epoch 412/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 412: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.6049 - val_acc: 0.7292\n",
      "Epoch 413/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9988\n",
      "Epoch 413: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9988 - val_loss: 3.6551 - val_acc: 0.7258\n",
      "Epoch 414/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 414: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.6699 - val_acc: 0.7267\n",
      "Epoch 415/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 415: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6939 - val_acc: 0.7270\n",
      "Epoch 416/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 416: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.6858 - val_acc: 0.7262\n",
      "Epoch 417/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 417: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.7259 - val_acc: 0.7260\n",
      "Epoch 418/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 418: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.7080 - val_acc: 0.7262\n",
      "Epoch 419/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 419: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.7389 - val_acc: 0.7248\n",
      "Epoch 420/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 420: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.7414 - val_acc: 0.7235\n",
      "Epoch 421/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 421: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7329 - val_acc: 0.7242\n",
      "Epoch 422/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 422: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.7671 - val_acc: 0.7236\n",
      "Epoch 423/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 423: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.7756 - val_acc: 0.7263\n",
      "Epoch 424/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 424: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.7453 - val_acc: 0.7260\n",
      "Epoch 425/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 425: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.7577 - val_acc: 0.7258\n",
      "Epoch 426/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9988\n",
      "Epoch 426: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9988 - val_loss: 3.7703 - val_acc: 0.7249\n",
      "Epoch 427/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0030 - acc: 0.9985\n",
      "Epoch 427: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 3.7034 - val_acc: 0.7286\n",
      "Epoch 428/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9980\n",
      "Epoch 428: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0039 - acc: 0.9980 - val_loss: 3.5843 - val_acc: 0.7248\n",
      "Epoch 429/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9981\n",
      "Epoch 429: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0037 - acc: 0.9981 - val_loss: 3.4968 - val_acc: 0.7284\n",
      "Epoch 430/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0039 - acc: 0.9982\n",
      "Epoch 430: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0040 - acc: 0.9981 - val_loss: 3.4289 - val_acc: 0.7268\n",
      "Epoch 431/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9983\n",
      "Epoch 431: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9983 - val_loss: 3.4993 - val_acc: 0.7254\n",
      "Epoch 432/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9982\n",
      "Epoch 432: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9982 - val_loss: 3.4682 - val_acc: 0.7224\n",
      "Epoch 433/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0031 - acc: 0.9982\n",
      "Epoch 433: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9982 - val_loss: 3.3170 - val_acc: 0.7267\n",
      "Epoch 434/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0035 - acc: 0.9985\n",
      "Epoch 434: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9984 - val_loss: 3.3078 - val_acc: 0.7305\n",
      "Epoch 435/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 435: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 3.4699 - val_acc: 0.7252\n",
      "Epoch 436/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 436: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 3.3840 - val_acc: 0.7268\n",
      "Epoch 437/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9986\n",
      "Epoch 437: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 3.4773 - val_acc: 0.7274\n",
      "Epoch 438/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 438: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 3.5000 - val_acc: 0.7265\n",
      "Epoch 439/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 439: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.4975 - val_acc: 0.7248\n",
      "Epoch 440/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 440: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.5404 - val_acc: 0.7278\n",
      "Epoch 441/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 441: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6076 - val_acc: 0.7263\n",
      "Epoch 442/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 442: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.5974 - val_acc: 0.7248\n",
      "Epoch 443/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 443: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6640 - val_acc: 0.7257\n",
      "Epoch 444/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9984\n",
      "Epoch 444: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9984 - val_loss: 3.6457 - val_acc: 0.7297\n",
      "Epoch 445/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 445: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6673 - val_acc: 0.7240\n",
      "Epoch 446/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 446: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6731 - val_acc: 0.7249\n",
      "Epoch 447/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 447: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6648 - val_acc: 0.7243\n",
      "Epoch 448/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 448: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6829 - val_acc: 0.7266\n",
      "Epoch 449/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9984\n",
      "Epoch 449: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9984 - val_loss: 3.6998 - val_acc: 0.7260\n",
      "Epoch 450/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 450: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.7272 - val_acc: 0.7246\n",
      "Epoch 451/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 451: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.7139 - val_acc: 0.7282\n",
      "Epoch 452/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 452: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9984 - val_loss: 3.6739 - val_acc: 0.7264\n",
      "Epoch 453/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 453: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7394 - val_acc: 0.7260\n",
      "Epoch 454/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 454: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7508 - val_acc: 0.7239\n",
      "Epoch 455/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 455: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6625 - val_acc: 0.7276\n",
      "Epoch 456/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 456: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9987 - val_loss: 3.7027 - val_acc: 0.7283\n",
      "Epoch 457/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 457: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7442 - val_acc: 0.7262\n",
      "Epoch 458/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 458: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7588 - val_acc: 0.7261\n",
      "Epoch 459/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 459: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7185 - val_acc: 0.7264\n",
      "Epoch 460/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 460: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7518 - val_acc: 0.7270\n",
      "Epoch 461/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 461: val_loss did not improve from 3.78520\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.7662 - val_acc: 0.7286\n",
      "Epoch 462/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 462: val_loss improved from 3.78520 to 3.80198, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.8020 - val_acc: 0.7247\n",
      "Epoch 463/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 463: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.6894 - val_acc: 0.7281\n",
      "Epoch 464/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 464: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6580 - val_acc: 0.7256\n",
      "Epoch 465/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9983\n",
      "Epoch 465: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9983 - val_loss: 3.5146 - val_acc: 0.7272\n",
      "Epoch 466/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 466: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9983 - val_loss: 3.5460 - val_acc: 0.7300\n",
      "Epoch 467/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9983\n",
      "Epoch 467: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9983 - val_loss: 3.5027 - val_acc: 0.7290\n",
      "Epoch 468/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9985\n",
      "Epoch 468: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 3.5002 - val_acc: 0.7262\n",
      "Epoch 469/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 469: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 3.4700 - val_acc: 0.7294\n",
      "Epoch 470/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 470: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.5154 - val_acc: 0.7279\n",
      "Epoch 471/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 471: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.5209 - val_acc: 0.7252\n",
      "Epoch 472/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 472: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.5647 - val_acc: 0.7266\n",
      "Epoch 473/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 473: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.5884 - val_acc: 0.7288\n",
      "Epoch 474/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 474: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9987 - val_loss: 3.6143 - val_acc: 0.7270\n",
      "Epoch 475/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9984\n",
      "Epoch 475: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9984 - val_loss: 3.6171 - val_acc: 0.7282\n",
      "Epoch 476/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 476: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.5785 - val_acc: 0.7300\n",
      "Epoch 477/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 477: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9985 - val_loss: 3.5660 - val_acc: 0.7247\n",
      "Epoch 478/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0034 - acc: 0.9982\n",
      "Epoch 478: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9982 - val_loss: 3.5185 - val_acc: 0.7253\n",
      "Epoch 479/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9983\n",
      "Epoch 479: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9982 - val_loss: 3.3910 - val_acc: 0.7275\n",
      "Epoch 480/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9983\n",
      "Epoch 480: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9983 - val_loss: 3.4011 - val_acc: 0.7307\n",
      "Epoch 481/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9985\n",
      "Epoch 481: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9985 - val_loss: 3.4116 - val_acc: 0.7297\n",
      "Epoch 482/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 482: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.4748 - val_acc: 0.7281\n",
      "Epoch 483/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 483: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.4742 - val_acc: 0.7301\n",
      "Epoch 484/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 484: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.5150 - val_acc: 0.7256\n",
      "Epoch 485/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 485: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.5685 - val_acc: 0.7289\n",
      "Epoch 486/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 486: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.5901 - val_acc: 0.7298\n",
      "Epoch 487/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 487: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.6006 - val_acc: 0.7282\n",
      "Epoch 488/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 488: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6275 - val_acc: 0.7273\n",
      "Epoch 489/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 489: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6242 - val_acc: 0.7270\n",
      "Epoch 490/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 490: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6261 - val_acc: 0.7272\n",
      "Epoch 491/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 491: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6245 - val_acc: 0.7282\n",
      "Epoch 492/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 492: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6599 - val_acc: 0.7279\n",
      "Epoch 493/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 493: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6901 - val_acc: 0.7260\n",
      "Epoch 494/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 494: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7020 - val_acc: 0.7274\n",
      "Epoch 495/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 495: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.7115 - val_acc: 0.7277\n",
      "Epoch 496/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 496: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6914 - val_acc: 0.7265\n",
      "Epoch 497/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 497: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6807 - val_acc: 0.7280\n",
      "Epoch 498/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9984\n",
      "Epoch 498: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9984 - val_loss: 3.7182 - val_acc: 0.7286\n",
      "Epoch 499/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 499: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6913 - val_acc: 0.7268\n",
      "Epoch 500/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 500: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7168 - val_acc: 0.7298\n",
      "Epoch 501/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 501: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.7477 - val_acc: 0.7284\n",
      "Epoch 502/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 502: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9987 - val_loss: 3.6978 - val_acc: 0.7276\n",
      "Epoch 503/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 503: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.7546 - val_acc: 0.7286\n",
      "Epoch 504/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 504: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.7112 - val_acc: 0.7277\n",
      "Epoch 505/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0033 - acc: 0.9982\n",
      "Epoch 505: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9982 - val_loss: 3.5456 - val_acc: 0.7267\n",
      "Epoch 506/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9982\n",
      "Epoch 506: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9982 - val_loss: 3.5555 - val_acc: 0.7273\n",
      "Epoch 507/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9983\n",
      "Epoch 507: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9982 - val_loss: 3.5044 - val_acc: 0.7272\n",
      "Epoch 508/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0043 - acc: 0.9983\n",
      "Epoch 508: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0042 - acc: 0.9983 - val_loss: 3.3990 - val_acc: 0.7243\n",
      "Epoch 509/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0040 - acc: 0.9981\n",
      "Epoch 509: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9981 - val_loss: 3.4110 - val_acc: 0.7244\n",
      "Epoch 510/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9983\n",
      "Epoch 510: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9982 - val_loss: 3.4584 - val_acc: 0.7248\n",
      "Epoch 511/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9985\n",
      "Epoch 511: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 3.3748 - val_acc: 0.7264\n",
      "Epoch 512/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 512: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.4980 - val_acc: 0.7250\n",
      "Epoch 513/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 513: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.5162 - val_acc: 0.7274\n",
      "Epoch 514/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 514: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9984 - val_loss: 3.5392 - val_acc: 0.7252\n",
      "Epoch 515/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 515: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.5804 - val_acc: 0.7264\n",
      "Epoch 516/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 516: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6083 - val_acc: 0.7275\n",
      "Epoch 517/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 517: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.6084 - val_acc: 0.7264\n",
      "Epoch 518/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 518: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.5988 - val_acc: 0.7246\n",
      "Epoch 519/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 519: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6425 - val_acc: 0.7270\n",
      "Epoch 520/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 520: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.6821 - val_acc: 0.7241\n",
      "Epoch 521/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 521: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6675 - val_acc: 0.7254\n",
      "Epoch 522/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 522: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6913 - val_acc: 0.7278\n",
      "Epoch 523/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 523: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6865 - val_acc: 0.7269\n",
      "Epoch 524/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 524: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6564 - val_acc: 0.7291\n",
      "Epoch 525/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 525: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6257 - val_acc: 0.7282\n",
      "Epoch 526/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 526: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6497 - val_acc: 0.7252\n",
      "Epoch 527/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 527: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6815 - val_acc: 0.7269\n",
      "Epoch 528/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 528: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6755 - val_acc: 0.7268\n",
      "Epoch 529/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 529: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.6966 - val_acc: 0.7288\n",
      "Epoch 530/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 530: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6880 - val_acc: 0.7270\n",
      "Epoch 531/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 531: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6384 - val_acc: 0.7295\n",
      "Epoch 532/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 532: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.6290 - val_acc: 0.7263\n",
      "Epoch 533/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 533: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6427 - val_acc: 0.7263\n",
      "Epoch 534/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 534: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.6766 - val_acc: 0.7290\n",
      "Epoch 535/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 535: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6933 - val_acc: 0.7296\n",
      "Epoch 536/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9984\n",
      "Epoch 536: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9984 - val_loss: 3.6742 - val_acc: 0.7294\n",
      "Epoch 537/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 537: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7080 - val_acc: 0.7265\n",
      "Epoch 538/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 538: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6779 - val_acc: 0.7256\n",
      "Epoch 539/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 539: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.7054 - val_acc: 0.7294\n",
      "Epoch 540/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9984\n",
      "Epoch 540: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9984 - val_loss: 3.7417 - val_acc: 0.7272\n",
      "Epoch 541/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 541: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7189 - val_acc: 0.7286\n",
      "Epoch 542/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 542: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7400 - val_acc: 0.7266\n",
      "Epoch 543/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0096 - acc: 0.9971\n",
      "Epoch 543: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0092 - acc: 0.9971 - val_loss: 3.4368 - val_acc: 0.7260\n",
      "Epoch 544/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0034 - acc: 0.9982\n",
      "Epoch 544: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9982 - val_loss: 3.3975 - val_acc: 0.7280\n",
      "Epoch 545/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9983\n",
      "Epoch 545: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9982 - val_loss: 3.5469 - val_acc: 0.7263\n",
      "Epoch 546/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9982\n",
      "Epoch 546: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9982 - val_loss: 3.4060 - val_acc: 0.7304\n",
      "Epoch 547/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0028 - acc: 0.9985\n",
      "Epoch 547: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.4014 - val_acc: 0.7255\n",
      "Epoch 548/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9982\n",
      "Epoch 548: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9983 - val_loss: 3.4929 - val_acc: 0.7263\n",
      "Epoch 549/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9983\n",
      "Epoch 549: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0045 - acc: 0.9983 - val_loss: 3.3696 - val_acc: 0.7250\n",
      "Epoch 550/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9984\n",
      "Epoch 550: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9984 - val_loss: 3.5664 - val_acc: 0.7266\n",
      "Epoch 551/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 551: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.5143 - val_acc: 0.7259\n",
      "Epoch 552/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 552: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.5284 - val_acc: 0.7257\n",
      "Epoch 553/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 553: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.5744 - val_acc: 0.7262\n",
      "Epoch 554/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 554: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6214 - val_acc: 0.7278\n",
      "Epoch 555/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 555: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6065 - val_acc: 0.7256\n",
      "Epoch 556/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 556: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.5966 - val_acc: 0.7288\n",
      "Epoch 557/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 557: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6394 - val_acc: 0.7279\n",
      "Epoch 558/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 558: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6227 - val_acc: 0.7288\n",
      "Epoch 559/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0044 - acc: 0.9983\n",
      "Epoch 559: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - acc: 0.9983 - val_loss: 3.4420 - val_acc: 0.7330\n",
      "Epoch 560/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9986\n",
      "Epoch 560: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 3.4933 - val_acc: 0.7276\n",
      "Epoch 561/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 561: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.5165 - val_acc: 0.7272\n",
      "Epoch 562/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9984\n",
      "Epoch 562: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9984 - val_loss: 3.2774 - val_acc: 0.7224\n",
      "Epoch 563/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 563: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.4015 - val_acc: 0.7282\n",
      "Epoch 564/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 564: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.4847 - val_acc: 0.7279\n",
      "Epoch 565/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 565: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.5642 - val_acc: 0.7274\n",
      "Epoch 566/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9984\n",
      "Epoch 566: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.5491 - val_acc: 0.7296\n",
      "Epoch 567/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 567: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.5778 - val_acc: 0.7282\n",
      "Epoch 568/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 568: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6148 - val_acc: 0.7268\n",
      "Epoch 569/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9988\n",
      "Epoch 569: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9988 - val_loss: 3.5759 - val_acc: 0.7268\n",
      "Epoch 570/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 570: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.5936 - val_acc: 0.7241\n",
      "Epoch 571/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 571: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.6140 - val_acc: 0.7258\n",
      "Epoch 572/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9984\n",
      "Epoch 572: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9984 - val_loss: 3.6258 - val_acc: 0.7244\n",
      "Epoch 573/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 573: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6721 - val_acc: 0.7274\n",
      "Epoch 574/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 574: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6691 - val_acc: 0.7257\n",
      "Epoch 575/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 575: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6754 - val_acc: 0.7290\n",
      "Epoch 576/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 576: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6821 - val_acc: 0.7280\n",
      "Epoch 577/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 577: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7067 - val_acc: 0.7273\n",
      "Epoch 578/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 578: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6678 - val_acc: 0.7313\n",
      "Epoch 579/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 579: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7051 - val_acc: 0.7267\n",
      "Epoch 580/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 580: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6734 - val_acc: 0.7261\n",
      "Epoch 581/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 581: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.7079 - val_acc: 0.7263\n",
      "Epoch 582/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 582: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7086 - val_acc: 0.7280\n",
      "Epoch 583/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 583: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7245 - val_acc: 0.7230\n",
      "Epoch 584/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 584: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6950 - val_acc: 0.7272\n",
      "Epoch 585/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 585: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6967 - val_acc: 0.7255\n",
      "Epoch 586/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 586: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7043 - val_acc: 0.7244\n",
      "Epoch 587/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 587: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7446 - val_acc: 0.7272\n",
      "Epoch 588/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 588: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7413 - val_acc: 0.7254\n",
      "Epoch 589/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 589: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7441 - val_acc: 0.7253\n",
      "Epoch 590/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 590: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 3.7612 - val_acc: 0.7248\n",
      "Epoch 591/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 591: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7547 - val_acc: 0.7276\n",
      "Epoch 592/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0228 - acc: 0.9948\n",
      "Epoch 592: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0223 - acc: 0.9948 - val_loss: 3.4753 - val_acc: 0.7268\n",
      "Epoch 593/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0033 - acc: 0.9983\n",
      "Epoch 593: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 3.4212 - val_acc: 0.7284\n",
      "Epoch 594/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 594: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.5523 - val_acc: 0.7266\n",
      "Epoch 595/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9985\n",
      "Epoch 595: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9985 - val_loss: 3.4088 - val_acc: 0.7269\n",
      "Epoch 596/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0029 - acc: 0.9985\n",
      "Epoch 596: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9985 - val_loss: 3.4769 - val_acc: 0.7255\n",
      "Epoch 597/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9983\n",
      "Epoch 597: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 3.4136 - val_acc: 0.7269\n",
      "Epoch 598/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 598: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.4325 - val_acc: 0.7276\n",
      "Epoch 599/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 599: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.4859 - val_acc: 0.7274\n",
      "Epoch 600/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 600: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.5406 - val_acc: 0.7268\n",
      "Epoch 601/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 601: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.5349 - val_acc: 0.7284\n",
      "Epoch 602/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 602: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.5854 - val_acc: 0.7282\n",
      "Epoch 603/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 603: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6704 - val_acc: 0.7247\n",
      "Epoch 604/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9988\n",
      "Epoch 604: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.6365 - val_acc: 0.7257\n",
      "Epoch 605/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 605: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6213 - val_acc: 0.7298\n",
      "Epoch 606/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
      "Epoch 606: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6507 - val_acc: 0.7276\n",
      "Epoch 607/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 607: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 3.6787 - val_acc: 0.7250\n",
      "Epoch 608/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 608: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.5993 - val_acc: 0.7266\n",
      "Epoch 609/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 609: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.5793 - val_acc: 0.7256\n",
      "Epoch 610/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 610: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6799 - val_acc: 0.7270\n",
      "Epoch 611/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 611: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6669 - val_acc: 0.7273\n",
      "Epoch 612/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 612: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6967 - val_acc: 0.7259\n",
      "Epoch 613/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 613: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 3.6679 - val_acc: 0.7286\n",
      "Epoch 614/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 614: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6846 - val_acc: 0.7268\n",
      "Epoch 615/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 615: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7142 - val_acc: 0.7283\n",
      "Epoch 616/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 616: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7254 - val_acc: 0.7241\n",
      "Epoch 617/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 617: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7291 - val_acc: 0.7276\n",
      "Epoch 618/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 618: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6798 - val_acc: 0.7251\n",
      "Epoch 619/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 619: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7488 - val_acc: 0.7260\n",
      "Epoch 620/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 620: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7459 - val_acc: 0.7268\n",
      "Epoch 621/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 621: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7343 - val_acc: 0.7267\n",
      "Epoch 622/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 622: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7142 - val_acc: 0.7275\n",
      "Epoch 623/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 623: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6676 - val_acc: 0.7257\n",
      "Epoch 624/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0030 - acc: 0.9983\n",
      "Epoch 624: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9983 - val_loss: 3.5669 - val_acc: 0.7293\n",
      "Epoch 625/1100\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9981\n",
      "Epoch 625: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9981 - val_loss: 3.5386 - val_acc: 0.7263\n",
      "Epoch 626/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 626: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 3.7467 - val_acc: 0.7248\n",
      "Epoch 627/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0041 - acc: 0.9981\n",
      "Epoch 627: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0041 - acc: 0.9981 - val_loss: 3.5058 - val_acc: 0.7228\n",
      "Epoch 628/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0032 - acc: 0.9984\n",
      "Epoch 628: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9984 - val_loss: 3.4666 - val_acc: 0.7234\n",
      "Epoch 629/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 629: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9984 - val_loss: 3.4923 - val_acc: 0.7268\n",
      "Epoch 630/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 630: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.4178 - val_acc: 0.7242\n",
      "Epoch 631/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 631: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.4058 - val_acc: 0.7284\n",
      "Epoch 632/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 632: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.5463 - val_acc: 0.7244\n",
      "Epoch 633/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 633: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.5571 - val_acc: 0.7269\n",
      "Epoch 634/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 634: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.5878 - val_acc: 0.7271\n",
      "Epoch 635/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 635: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 3.5963 - val_acc: 0.7268\n",
      "Epoch 636/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 636: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6155 - val_acc: 0.7272\n",
      "Epoch 637/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 637: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6187 - val_acc: 0.7288\n",
      "Epoch 638/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 638: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.5798 - val_acc: 0.7297\n",
      "Epoch 639/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 639: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6530 - val_acc: 0.7275\n",
      "Epoch 640/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 640: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6742 - val_acc: 0.7272\n",
      "Epoch 641/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 641: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6664 - val_acc: 0.7273\n",
      "Epoch 642/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 642: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6933 - val_acc: 0.7274\n",
      "Epoch 643/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 643: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7052 - val_acc: 0.7272\n",
      "Epoch 644/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 644: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7059 - val_acc: 0.7286\n",
      "Epoch 645/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 645: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6965 - val_acc: 0.7289\n",
      "Epoch 646/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 646: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7276 - val_acc: 0.7290\n",
      "Epoch 647/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 647: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7417 - val_acc: 0.7268\n",
      "Epoch 648/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0039 - acc: 0.9981\n",
      "Epoch 648: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 3.6373 - val_acc: 0.7294\n",
      "Epoch 649/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 649: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6427 - val_acc: 0.7256\n",
      "Epoch 650/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0031 - acc: 0.9983\n",
      "Epoch 650: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9983 - val_loss: 3.6043 - val_acc: 0.7250\n",
      "Epoch 651/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 651: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.5463 - val_acc: 0.7252\n",
      "Epoch 652/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 652: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6171 - val_acc: 0.7244\n",
      "Epoch 653/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9984\n",
      "Epoch 653: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9983 - val_loss: 3.5022 - val_acc: 0.7252\n",
      "Epoch 654/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 654: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 3.5797 - val_acc: 0.7244\n",
      "Epoch 655/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9984\n",
      "Epoch 655: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9984 - val_loss: 3.5750 - val_acc: 0.7242\n",
      "Epoch 656/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9985\n",
      "Epoch 656: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9985 - val_loss: 3.4871 - val_acc: 0.7293\n",
      "Epoch 657/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 657: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.5816 - val_acc: 0.7282\n",
      "Epoch 658/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9988\n",
      "Epoch 658: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9988 - val_loss: 3.6389 - val_acc: 0.7277\n",
      "Epoch 659/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 659: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6475 - val_acc: 0.7301\n",
      "Epoch 660/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 660: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6883 - val_acc: 0.7273\n",
      "Epoch 661/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 661: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6783 - val_acc: 0.7258\n",
      "Epoch 662/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 662: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6735 - val_acc: 0.7273\n",
      "Epoch 663/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 663: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7084 - val_acc: 0.7280\n",
      "Epoch 664/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 664: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7090 - val_acc: 0.7274\n",
      "Epoch 665/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 665: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7360 - val_acc: 0.7283\n",
      "Epoch 666/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 666: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7438 - val_acc: 0.7258\n",
      "Epoch 667/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 667: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7101 - val_acc: 0.7291\n",
      "Epoch 668/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 668: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 3.7507 - val_acc: 0.7273\n",
      "Epoch 669/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 669: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7482 - val_acc: 0.7273\n",
      "Epoch 670/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 670: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7519 - val_acc: 0.7258\n",
      "Epoch 671/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 671: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7648 - val_acc: 0.7282\n",
      "Epoch 672/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 672: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7872 - val_acc: 0.7272\n",
      "Epoch 673/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 673: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7537 - val_acc: 0.7280\n",
      "Epoch 674/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 674: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.7511 - val_acc: 0.7282\n",
      "Epoch 675/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 675: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.8002 - val_acc: 0.7252\n",
      "Epoch 676/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9984\n",
      "Epoch 676: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0026 - acc: 0.9984 - val_loss: 3.5924 - val_acc: 0.7288\n",
      "Epoch 677/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 677: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 3.6668 - val_acc: 0.7265\n",
      "Epoch 678/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9983\n",
      "Epoch 678: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9983 - val_loss: 3.6416 - val_acc: 0.7231\n",
      "Epoch 679/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 679: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 3.5001 - val_acc: 0.7294\n",
      "Epoch 680/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0037 - acc: 0.9983\n",
      "Epoch 680: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0037 - acc: 0.9983 - val_loss: 3.5134 - val_acc: 0.7308\n",
      "Epoch 681/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0033 - acc: 0.9982\n",
      "Epoch 681: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0032 - acc: 0.9982 - val_loss: 3.4956 - val_acc: 0.7263\n",
      "Epoch 682/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 682: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0027 - acc: 0.9984 - val_loss: 3.4794 - val_acc: 0.7298\n",
      "Epoch 683/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 683: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.5749 - val_acc: 0.7308\n",
      "Epoch 684/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 684: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.5677 - val_acc: 0.7280\n",
      "Epoch 685/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 685: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.5914 - val_acc: 0.7299\n",
      "Epoch 686/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 686: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.5809 - val_acc: 0.7292\n",
      "Epoch 687/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 687: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6118 - val_acc: 0.7262\n",
      "Epoch 688/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 688: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6273 - val_acc: 0.7280\n",
      "Epoch 689/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 689: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.6618 - val_acc: 0.7267\n",
      "Epoch 690/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 690: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6691 - val_acc: 0.7298\n",
      "Epoch 691/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 691: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6611 - val_acc: 0.7272\n",
      "Epoch 692/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 692: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6726 - val_acc: 0.7256\n",
      "Epoch 693/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 693: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7188 - val_acc: 0.7290\n",
      "Epoch 694/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 694: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7154 - val_acc: 0.7274\n",
      "Epoch 695/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 695: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7094 - val_acc: 0.7255\n",
      "Epoch 696/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 696: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7068 - val_acc: 0.7266\n",
      "Epoch 697/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 697: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7034 - val_acc: 0.7271\n",
      "Epoch 698/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 698: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7011 - val_acc: 0.7288\n",
      "Epoch 699/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 699: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9984 - val_loss: 3.6835 - val_acc: 0.7292\n",
      "Epoch 700/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 700: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.5140 - val_acc: 0.7283\n",
      "Epoch 701/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 701: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9987 - val_loss: 3.5987 - val_acc: 0.7278\n",
      "Epoch 702/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9986\n",
      "Epoch 702: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.6216 - val_acc: 0.7291\n",
      "Epoch 703/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 703: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7007 - val_acc: 0.7292\n",
      "Epoch 704/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 704: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6748 - val_acc: 0.7294\n",
      "Epoch 705/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 705: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 3.6757 - val_acc: 0.7270\n",
      "Epoch 706/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 706: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7333 - val_acc: 0.7290\n",
      "Epoch 707/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 707: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7059 - val_acc: 0.7284\n",
      "Epoch 708/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 708: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7428 - val_acc: 0.7303\n",
      "Epoch 709/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 709: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7373 - val_acc: 0.7283\n",
      "Epoch 710/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 710: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6831 - val_acc: 0.7299\n",
      "Epoch 711/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 711: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7289 - val_acc: 0.7257\n",
      "Epoch 712/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 712: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7668 - val_acc: 0.7256\n",
      "Epoch 713/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 713: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7593 - val_acc: 0.7263\n",
      "Epoch 714/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 714: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7610 - val_acc: 0.7280\n",
      "Epoch 715/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 715: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7845 - val_acc: 0.7270\n",
      "Epoch 716/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 716: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7485 - val_acc: 0.7266\n",
      "Epoch 717/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 717: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7948 - val_acc: 0.7270\n",
      "Epoch 718/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 718: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7551 - val_acc: 0.7289\n",
      "Epoch 719/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 719: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7814 - val_acc: 0.7234\n",
      "Epoch 720/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 720: val_loss did not improve from 3.80198\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7668 - val_acc: 0.7278\n",
      "Epoch 721/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 721: val_loss improved from 3.80198 to 3.80420, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 3.8042 - val_acc: 0.7249\n",
      "Epoch 722/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 722: val_loss improved from 3.80420 to 3.81266, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.8127 - val_acc: 0.7275\n",
      "Epoch 723/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 723: val_loss did not improve from 3.81266\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7944 - val_acc: 0.7256\n",
      "Epoch 724/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 724: val_loss did not improve from 3.81266\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7972 - val_acc: 0.7286\n",
      "Epoch 725/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 725: val_loss improved from 3.81266 to 3.83484, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8348 - val_acc: 0.7295\n",
      "Epoch 726/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 726: val_loss improved from 3.83484 to 3.85473, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.8547 - val_acc: 0.7256\n",
      "Epoch 727/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 727: val_loss did not improve from 3.85473\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.8213 - val_acc: 0.7284\n",
      "Epoch 728/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 728: val_loss did not improve from 3.85473\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.8112 - val_acc: 0.7282\n",
      "Epoch 729/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 729: val_loss improved from 3.85473 to 3.86835, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8683 - val_acc: 0.7292\n",
      "Epoch 730/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 730: val_loss did not improve from 3.86835\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.8365 - val_acc: 0.7238\n",
      "Epoch 731/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 731: val_loss did not improve from 3.86835\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9984 - val_loss: 3.8446 - val_acc: 0.7282\n",
      "Epoch 732/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 732: val_loss did not improve from 3.86835\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.8416 - val_acc: 0.7284\n",
      "Epoch 733/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 733: val_loss did not improve from 3.86835\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.8670 - val_acc: 0.7240\n",
      "Epoch 734/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 734: val_loss improved from 3.86835 to 3.86930, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9987 - val_loss: 3.8693 - val_acc: 0.7260\n",
      "Epoch 735/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 735: val_loss did not improve from 3.86930\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8677 - val_acc: 0.7298\n",
      "Epoch 736/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 736: val_loss improved from 3.86930 to 3.87813, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8781 - val_acc: 0.7267\n",
      "Epoch 737/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 737: val_loss did not improve from 3.87813\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8612 - val_acc: 0.7271\n",
      "Epoch 738/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 738: val_loss improved from 3.87813 to 3.88212, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.8821 - val_acc: 0.7282\n",
      "Epoch 739/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 739: val_loss did not improve from 3.88212\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8586 - val_acc: 0.7270\n",
      "Epoch 740/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9987\n",
      "Epoch 740: val_loss improved from 3.88212 to 3.88537, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.8854 - val_acc: 0.7246\n",
      "Epoch 741/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 741: val_loss improved from 3.88537 to 3.90938, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.9094 - val_acc: 0.7242\n",
      "Epoch 742/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 742: val_loss did not improve from 3.90938\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.8900 - val_acc: 0.7269\n",
      "Epoch 743/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 743: val_loss did not improve from 3.90938\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8696 - val_acc: 0.7268\n",
      "Epoch 744/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9988\n",
      "Epoch 744: val_loss did not improve from 3.90938\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9988 - val_loss: 3.8880 - val_acc: 0.7284\n",
      "Epoch 745/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 745: val_loss did not improve from 3.90938\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.8948 - val_acc: 0.7284\n",
      "Epoch 746/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 746: val_loss did not improve from 3.90938\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8999 - val_acc: 0.7250\n",
      "Epoch 747/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 747: val_loss improved from 3.90938 to 3.94504, saving model to test_model.h5\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.9450 - val_acc: 0.7258\n",
      "Epoch 748/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 748: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.9323 - val_acc: 0.7248\n",
      "Epoch 749/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 749: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.9355 - val_acc: 0.7240\n",
      "Epoch 750/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 750: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.8542 - val_acc: 0.7268\n",
      "Epoch 751/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9980\n",
      "Epoch 751: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0044 - acc: 0.9980 - val_loss: 3.5665 - val_acc: 0.7274\n",
      "Epoch 752/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0036 - acc: 0.9983\n",
      "Epoch 752: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9984 - val_loss: 3.5902 - val_acc: 0.7264\n",
      "Epoch 753/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9978\n",
      "Epoch 753: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0050 - acc: 0.9978 - val_loss: 3.4830 - val_acc: 0.7265\n",
      "Epoch 754/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 754: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9983 - val_loss: 3.4441 - val_acc: 0.7258\n",
      "Epoch 755/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9982\n",
      "Epoch 755: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0038 - acc: 0.9981 - val_loss: 3.4038 - val_acc: 0.7258\n",
      "Epoch 756/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9983\n",
      "Epoch 756: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9983 - val_loss: 3.5156 - val_acc: 0.7234\n",
      "Epoch 757/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9986\n",
      "Epoch 757: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9986 - val_loss: 3.5254 - val_acc: 0.7242\n",
      "Epoch 758/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 758: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.4741 - val_acc: 0.7274\n",
      "Epoch 759/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 759: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.5446 - val_acc: 0.7264\n",
      "Epoch 760/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0025 - acc: 0.9984\n",
      "Epoch 760: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9984 - val_loss: 3.5895 - val_acc: 0.7251\n",
      "Epoch 761/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0030 - acc: 0.9983\n",
      "Epoch 761: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9983 - val_loss: 3.4194 - val_acc: 0.7249\n",
      "Epoch 762/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0024 - acc: 0.9984\n",
      "Epoch 762: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0024 - acc: 0.9984 - val_loss: 3.4561 - val_acc: 0.7306\n",
      "Epoch 763/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 763: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.5616 - val_acc: 0.7279\n",
      "Epoch 764/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 764: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.5840 - val_acc: 0.7278\n",
      "Epoch 765/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 765: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6270 - val_acc: 0.7293\n",
      "Epoch 766/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 766: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6362 - val_acc: 0.7286\n",
      "Epoch 767/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 767: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6063 - val_acc: 0.7288\n",
      "Epoch 768/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 768: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6565 - val_acc: 0.7293\n",
      "Epoch 769/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 769: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6808 - val_acc: 0.7271\n",
      "Epoch 770/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 770: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6754 - val_acc: 0.7270\n",
      "Epoch 771/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 771: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6656 - val_acc: 0.7304\n",
      "Epoch 772/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 772: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6875 - val_acc: 0.7305\n",
      "Epoch 773/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 773: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.7034 - val_acc: 0.7276\n",
      "Epoch 774/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 774: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7085 - val_acc: 0.7275\n",
      "Epoch 775/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 775: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7304 - val_acc: 0.7252\n",
      "Epoch 776/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 776: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7274 - val_acc: 0.7282\n",
      "Epoch 777/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 777: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7524 - val_acc: 0.7266\n",
      "Epoch 778/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 778: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7390 - val_acc: 0.7254\n",
      "Epoch 779/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 779: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.7472 - val_acc: 0.7262\n",
      "Epoch 780/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 780: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7290 - val_acc: 0.7262\n",
      "Epoch 781/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 781: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7578 - val_acc: 0.7248\n",
      "Epoch 782/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 782: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7710 - val_acc: 0.7250\n",
      "Epoch 783/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 783: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7620 - val_acc: 0.7251\n",
      "Epoch 784/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 784: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7668 - val_acc: 0.7273\n",
      "Epoch 785/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 785: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7592 - val_acc: 0.7258\n",
      "Epoch 786/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 786: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7773 - val_acc: 0.7260\n",
      "Epoch 787/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 787: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7973 - val_acc: 0.7242\n",
      "Epoch 788/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 788: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8125 - val_acc: 0.7258\n",
      "Epoch 789/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0037 - acc: 0.9983\n",
      "Epoch 789: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0037 - acc: 0.9983 - val_loss: 3.5672 - val_acc: 0.7300\n",
      "Epoch 790/1100\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 790: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 3.5394 - val_acc: 0.7315\n",
      "Epoch 791/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 791: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9984 - val_loss: 3.4555 - val_acc: 0.7302\n",
      "Epoch 792/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 792: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.4996 - val_acc: 0.7288\n",
      "Epoch 793/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 793: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6117 - val_acc: 0.7255\n",
      "Epoch 794/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 794: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6289 - val_acc: 0.7272\n",
      "Epoch 795/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 795: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6440 - val_acc: 0.7282\n",
      "Epoch 796/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 796: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6403 - val_acc: 0.7272\n",
      "Epoch 797/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 797: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6829 - val_acc: 0.7284\n",
      "Epoch 798/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 798: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6831 - val_acc: 0.7284\n",
      "Epoch 799/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 799: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6806 - val_acc: 0.7286\n",
      "Epoch 800/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 800: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.7037 - val_acc: 0.7256\n",
      "Epoch 801/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 801: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7242 - val_acc: 0.7271\n",
      "Epoch 802/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 802: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7449 - val_acc: 0.7283\n",
      "Epoch 803/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 803: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7377 - val_acc: 0.7281\n",
      "Epoch 804/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 804: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7197 - val_acc: 0.7266\n",
      "Epoch 805/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 805: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7215 - val_acc: 0.7271\n",
      "Epoch 806/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 806: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7489 - val_acc: 0.7260\n",
      "Epoch 807/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 807: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7642 - val_acc: 0.7266\n",
      "Epoch 808/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 808: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7578 - val_acc: 0.7254\n",
      "Epoch 809/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 809: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7742 - val_acc: 0.7252\n",
      "Epoch 810/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 810: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7503 - val_acc: 0.7239\n",
      "Epoch 811/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9987\n",
      "Epoch 811: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9987 - val_loss: 3.6798 - val_acc: 0.7256\n",
      "Epoch 812/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 812: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7122 - val_acc: 0.7276\n",
      "Epoch 813/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 813: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7266 - val_acc: 0.7263\n",
      "Epoch 814/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 814: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.7367 - val_acc: 0.7266\n",
      "Epoch 815/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 815: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7471 - val_acc: 0.7256\n",
      "Epoch 816/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 816: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7761 - val_acc: 0.7291\n",
      "Epoch 817/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 817: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7777 - val_acc: 0.7246\n",
      "Epoch 818/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9984\n",
      "Epoch 818: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9984 - val_loss: 3.7952 - val_acc: 0.7262\n",
      "Epoch 819/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 819: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7648 - val_acc: 0.7258\n",
      "Epoch 820/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 820: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7733 - val_acc: 0.7268\n",
      "Epoch 821/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 821: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7894 - val_acc: 0.7272\n",
      "Epoch 822/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 822: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8031 - val_acc: 0.7287\n",
      "Epoch 823/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 823: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7955 - val_acc: 0.7268\n",
      "Epoch 824/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 824: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7967 - val_acc: 0.7266\n",
      "Epoch 825/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 825: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8242 - val_acc: 0.7268\n",
      "Epoch 826/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 826: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8226 - val_acc: 0.7258\n",
      "Epoch 827/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 827: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8322 - val_acc: 0.7266\n",
      "Epoch 828/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 828: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.8171 - val_acc: 0.7241\n",
      "Epoch 829/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9984\n",
      "Epoch 829: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9984 - val_loss: 3.8472 - val_acc: 0.7272\n",
      "Epoch 830/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 830: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.8813 - val_acc: 0.7249\n",
      "Epoch 831/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 831: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8865 - val_acc: 0.7260\n",
      "Epoch 832/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 832: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8984 - val_acc: 0.7232\n",
      "Epoch 833/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 833: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9986 - val_loss: 3.8224 - val_acc: 0.7268\n",
      "Epoch 834/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9981\n",
      "Epoch 834: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 3.6473 - val_acc: 0.7270\n",
      "Epoch 835/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9976\n",
      "Epoch 835: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0069 - acc: 0.9976 - val_loss: 3.4890 - val_acc: 0.7289\n",
      "Epoch 836/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9982\n",
      "Epoch 836: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0037 - acc: 0.9982 - val_loss: 3.5231 - val_acc: 0.7274\n",
      "Epoch 837/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9983\n",
      "Epoch 837: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0031 - acc: 0.9983 - val_loss: 3.3472 - val_acc: 0.7286\n",
      "Epoch 838/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 838: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9984 - val_loss: 3.4387 - val_acc: 0.7278\n",
      "Epoch 839/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9983\n",
      "Epoch 839: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9983 - val_loss: 3.4364 - val_acc: 0.7244\n",
      "Epoch 840/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0029 - acc: 0.9982\n",
      "Epoch 840: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9982 - val_loss: 3.5172 - val_acc: 0.7254\n",
      "Epoch 841/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9984\n",
      "Epoch 841: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9984 - val_loss: 3.4715 - val_acc: 0.7259\n",
      "Epoch 842/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 842: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.4610 - val_acc: 0.7288\n",
      "Epoch 843/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 843: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.5682 - val_acc: 0.7273\n",
      "Epoch 844/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 844: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.5566 - val_acc: 0.7271\n",
      "Epoch 845/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 845: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6031 - val_acc: 0.7245\n",
      "Epoch 846/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
      "Epoch 846: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9988 - val_loss: 3.6193 - val_acc: 0.7233\n",
      "Epoch 847/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 847: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6671 - val_acc: 0.7252\n",
      "Epoch 848/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 848: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6273 - val_acc: 0.7259\n",
      "Epoch 849/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 849: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6656 - val_acc: 0.7229\n",
      "Epoch 850/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 850: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.6908 - val_acc: 0.7240\n",
      "Epoch 851/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 851: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6883 - val_acc: 0.7250\n",
      "Epoch 852/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 852: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7320 - val_acc: 0.7262\n",
      "Epoch 853/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 853: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6961 - val_acc: 0.7254\n",
      "Epoch 854/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 854: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7115 - val_acc: 0.7230\n",
      "Epoch 855/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 855: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7221 - val_acc: 0.7223\n",
      "Epoch 856/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 856: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7400 - val_acc: 0.7220\n",
      "Epoch 857/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 857: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7226 - val_acc: 0.7219\n",
      "Epoch 858/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 858: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7496 - val_acc: 0.7219\n",
      "Epoch 859/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 859: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 3.7196 - val_acc: 0.7242\n",
      "Epoch 860/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 860: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7521 - val_acc: 0.7248\n",
      "Epoch 861/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 861: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7672 - val_acc: 0.7252\n",
      "Epoch 862/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 862: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7921 - val_acc: 0.7250\n",
      "Epoch 863/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 863: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8048 - val_acc: 0.7247\n",
      "Epoch 864/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 864: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7445 - val_acc: 0.7256\n",
      "Epoch 865/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 865: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.8076 - val_acc: 0.7236\n",
      "Epoch 866/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 866: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7757 - val_acc: 0.7242\n",
      "Epoch 867/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 867: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7888 - val_acc: 0.7248\n",
      "Epoch 868/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 868: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7754 - val_acc: 0.7240\n",
      "Epoch 869/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 869: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7962 - val_acc: 0.7233\n",
      "Epoch 870/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 870: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7957 - val_acc: 0.7236\n",
      "Epoch 871/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 871: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8056 - val_acc: 0.7238\n",
      "Epoch 872/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 872: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8177 - val_acc: 0.7260\n",
      "Epoch 873/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 873: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.8291 - val_acc: 0.7233\n",
      "Epoch 874/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 874: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8013 - val_acc: 0.7256\n",
      "Epoch 875/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 875: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8089 - val_acc: 0.7258\n",
      "Epoch 876/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 876: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8294 - val_acc: 0.7238\n",
      "Epoch 877/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 877: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8443 - val_acc: 0.7254\n",
      "Epoch 878/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9983\n",
      "Epoch 878: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9984 - val_loss: 3.8616 - val_acc: 0.7254\n",
      "Epoch 879/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 879: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8570 - val_acc: 0.7235\n",
      "Epoch 880/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 880: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8479 - val_acc: 0.7250\n",
      "Epoch 881/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 881: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8573 - val_acc: 0.7225\n",
      "Epoch 882/1100\n",
      "224/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 882: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.8801 - val_acc: 0.7240\n",
      "Epoch 883/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 883: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8857 - val_acc: 0.7234\n",
      "Epoch 884/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 884: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.8345 - val_acc: 0.7245\n",
      "Epoch 885/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0042 - acc: 0.9981\n",
      "Epoch 885: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0043 - acc: 0.9981 - val_loss: 3.4917 - val_acc: 0.7253\n",
      "Epoch 886/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0040 - acc: 0.9980\n",
      "Epoch 886: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0040 - acc: 0.9980 - val_loss: 3.4296 - val_acc: 0.7276\n",
      "Epoch 887/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0033 - acc: 0.9982\n",
      "Epoch 887: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9982 - val_loss: 3.3290 - val_acc: 0.7271\n",
      "Epoch 888/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 888: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.4835 - val_acc: 0.7274\n",
      "Epoch 889/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0025 - acc: 0.9987\n",
      "Epoch 889: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9987 - val_loss: 3.5622 - val_acc: 0.7252\n",
      "Epoch 890/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 890: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.5457 - val_acc: 0.7268\n",
      "Epoch 891/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 891: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.5720 - val_acc: 0.7260\n",
      "Epoch 892/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 892: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6123 - val_acc: 0.7286\n",
      "Epoch 893/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 893: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.5906 - val_acc: 0.7270\n",
      "Epoch 894/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 894: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6574 - val_acc: 0.7253\n",
      "Epoch 895/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9986\n",
      "Epoch 895: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9986 - val_loss: 3.6479 - val_acc: 0.7237\n",
      "Epoch 896/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9985\n",
      "Epoch 896: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9985 - val_loss: 3.6301 - val_acc: 0.7272\n",
      "Epoch 897/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 897: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6496 - val_acc: 0.7259\n",
      "Epoch 898/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 898: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6654 - val_acc: 0.7258\n",
      "Epoch 899/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 899: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6812 - val_acc: 0.7262\n",
      "Epoch 900/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 900: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.6981 - val_acc: 0.7249\n",
      "Epoch 901/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 901: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7106 - val_acc: 0.7240\n",
      "Epoch 902/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 902: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7233 - val_acc: 0.7232\n",
      "Epoch 903/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 903: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7406 - val_acc: 0.7240\n",
      "Epoch 904/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 904: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7449 - val_acc: 0.7253\n",
      "Epoch 905/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 905: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7326 - val_acc: 0.7260\n",
      "Epoch 906/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 906: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6895 - val_acc: 0.7242\n",
      "Epoch 907/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 907: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7373 - val_acc: 0.7246\n",
      "Epoch 908/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 908: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7329 - val_acc: 0.7248\n",
      "Epoch 909/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 909: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.7553 - val_acc: 0.7250\n",
      "Epoch 910/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 910: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6270 - val_acc: 0.7250\n",
      "Epoch 911/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9984\n",
      "Epoch 911: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9984 - val_loss: 3.7265 - val_acc: 0.7234\n",
      "Epoch 912/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 912: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7349 - val_acc: 0.7238\n",
      "Epoch 913/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 913: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7489 - val_acc: 0.7241\n",
      "Epoch 914/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 914: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7534 - val_acc: 0.7249\n",
      "Epoch 915/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 915: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7579 - val_acc: 0.7246\n",
      "Epoch 916/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 916: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7496 - val_acc: 0.7244\n",
      "Epoch 917/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9984\n",
      "Epoch 917: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9984 - val_loss: 3.7918 - val_acc: 0.7226\n",
      "Epoch 918/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 918: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7819 - val_acc: 0.7232\n",
      "Epoch 919/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 919: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7740 - val_acc: 0.7252\n",
      "Epoch 920/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 920: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8131 - val_acc: 0.7232\n",
      "Epoch 921/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 921: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8101 - val_acc: 0.7238\n",
      "Epoch 922/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 922: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8384 - val_acc: 0.7228\n",
      "Epoch 923/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 923: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8010 - val_acc: 0.7241\n",
      "Epoch 924/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 924: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8368 - val_acc: 0.7239\n",
      "Epoch 925/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 925: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8346 - val_acc: 0.7246\n",
      "Epoch 926/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 926: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8438 - val_acc: 0.7243\n",
      "Epoch 927/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 927: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.8552 - val_acc: 0.7248\n",
      "Epoch 928/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 928: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8546 - val_acc: 0.7248\n",
      "Epoch 929/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0032 - acc: 0.9985\n",
      "Epoch 929: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0034 - acc: 0.9984 - val_loss: 3.4538 - val_acc: 0.7288\n",
      "Epoch 930/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9982\n",
      "Epoch 930: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9982 - val_loss: 3.5404 - val_acc: 0.7236\n",
      "Epoch 931/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9984\n",
      "Epoch 931: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9984 - val_loss: 3.6031 - val_acc: 0.7220\n",
      "Epoch 932/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0040 - acc: 0.9983\n",
      "Epoch 932: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0039 - acc: 0.9983 - val_loss: 3.4985 - val_acc: 0.7236\n",
      "Epoch 933/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0028 - acc: 0.9983\n",
      "Epoch 933: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9983 - val_loss: 3.5723 - val_acc: 0.7242\n",
      "Epoch 934/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9983\n",
      "Epoch 934: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0035 - acc: 0.9983 - val_loss: 3.4733 - val_acc: 0.7274\n",
      "Epoch 935/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 935: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9985 - val_loss: 3.4992 - val_acc: 0.7273\n",
      "Epoch 936/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 936: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.5310 - val_acc: 0.7258\n",
      "Epoch 937/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 937: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6181 - val_acc: 0.7242\n",
      "Epoch 938/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 938: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.6263 - val_acc: 0.7256\n",
      "Epoch 939/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 939: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6547 - val_acc: 0.7241\n",
      "Epoch 940/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 940: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6338 - val_acc: 0.7262\n",
      "Epoch 941/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 941: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7049 - val_acc: 0.7249\n",
      "Epoch 942/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 942: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6932 - val_acc: 0.7233\n",
      "Epoch 943/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 943: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6859 - val_acc: 0.7248\n",
      "Epoch 944/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 944: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7299 - val_acc: 0.7237\n",
      "Epoch 945/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 945: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7428 - val_acc: 0.7236\n",
      "Epoch 946/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
      "Epoch 946: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9988 - val_loss: 3.6164 - val_acc: 0.7257\n",
      "Epoch 947/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9986\n",
      "Epoch 947: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6587 - val_acc: 0.7223\n",
      "Epoch 948/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 948: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6311 - val_acc: 0.7256\n",
      "Epoch 949/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9985\n",
      "Epoch 949: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6541 - val_acc: 0.7260\n",
      "Epoch 950/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 950: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9986 - val_loss: 3.6372 - val_acc: 0.7244\n",
      "Epoch 951/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 951: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6645 - val_acc: 0.7267\n",
      "Epoch 952/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 952: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6984 - val_acc: 0.7249\n",
      "Epoch 953/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 953: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7385 - val_acc: 0.7251\n",
      "Epoch 954/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 954: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.7442 - val_acc: 0.7238\n",
      "Epoch 955/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 955: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7728 - val_acc: 0.7253\n",
      "Epoch 956/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 956: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7545 - val_acc: 0.7258\n",
      "Epoch 957/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 957: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.7864 - val_acc: 0.7226\n",
      "Epoch 958/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 958: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7617 - val_acc: 0.7243\n",
      "Epoch 959/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 959: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7993 - val_acc: 0.7248\n",
      "Epoch 960/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 960: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.7787 - val_acc: 0.7250\n",
      "Epoch 961/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 961: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7505 - val_acc: 0.7266\n",
      "Epoch 962/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 962: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8075 - val_acc: 0.7244\n",
      "Epoch 963/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 963: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8318 - val_acc: 0.7258\n",
      "Epoch 964/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0024 - acc: 0.9987\n",
      "Epoch 964: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9987 - val_loss: 3.5339 - val_acc: 0.7277\n",
      "Epoch 965/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9984\n",
      "Epoch 965: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0030 - acc: 0.9984 - val_loss: 3.5748 - val_acc: 0.7242\n",
      "Epoch 966/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 966: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.5707 - val_acc: 0.7220\n",
      "Epoch 967/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9984\n",
      "Epoch 967: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9984 - val_loss: 3.4944 - val_acc: 0.7273\n",
      "Epoch 968/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0027 - acc: 0.9983\n",
      "Epoch 968: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0027 - acc: 0.9983 - val_loss: 3.4494 - val_acc: 0.7238\n",
      "Epoch 969/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9984\n",
      "Epoch 969: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0029 - acc: 0.9984 - val_loss: 3.5393 - val_acc: 0.7237\n",
      "Epoch 970/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9985\n",
      "Epoch 970: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9985 - val_loss: 3.5250 - val_acc: 0.7256\n",
      "Epoch 971/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0026 - acc: 0.9984\n",
      "Epoch 971: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9984 - val_loss: 3.4892 - val_acc: 0.7286\n",
      "Epoch 972/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9984\n",
      "Epoch 972: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0025 - acc: 0.9984 - val_loss: 3.5216 - val_acc: 0.7284\n",
      "Epoch 973/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 973: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.5508 - val_acc: 0.7268\n",
      "Epoch 974/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9984\n",
      "Epoch 974: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6005 - val_acc: 0.7254\n",
      "Epoch 975/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 975: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6004 - val_acc: 0.7266\n",
      "Epoch 976/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 976: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6495 - val_acc: 0.7266\n",
      "Epoch 977/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 977: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6747 - val_acc: 0.7260\n",
      "Epoch 978/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 978: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7557 - val_acc: 0.7250\n",
      "Epoch 979/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 979: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7254 - val_acc: 0.7263\n",
      "Epoch 980/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 980: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7165 - val_acc: 0.7273\n",
      "Epoch 981/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 981: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7062 - val_acc: 0.7250\n",
      "Epoch 982/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 982: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7330 - val_acc: 0.7255\n",
      "Epoch 983/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 983: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.7233 - val_acc: 0.7257\n",
      "Epoch 984/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 984: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7407 - val_acc: 0.7256\n",
      "Epoch 985/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9986\n",
      "Epoch 985: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0024 - acc: 0.9985 - val_loss: 3.6552 - val_acc: 0.7271\n",
      "Epoch 986/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 986: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6792 - val_acc: 0.7240\n",
      "Epoch 987/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.2539 - acc: 0.9552\n",
      "Epoch 987: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2539 - acc: 0.9552 - val_loss: 2.6504 - val_acc: 0.7298\n",
      "Epoch 988/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9976\n",
      "Epoch 988: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0065 - acc: 0.9976 - val_loss: 2.9936 - val_acc: 0.7261\n",
      "Epoch 989/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9983\n",
      "Epoch 989: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0033 - acc: 0.9983 - val_loss: 3.1523 - val_acc: 0.7296\n",
      "Epoch 990/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9980\n",
      "Epoch 990: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0036 - acc: 0.9980 - val_loss: 3.1500 - val_acc: 0.7266\n",
      "Epoch 991/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 991: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9986 - val_loss: 3.2201 - val_acc: 0.7243\n",
      "Epoch 992/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 992: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.3352 - val_acc: 0.7255\n",
      "Epoch 993/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9985\n",
      "Epoch 993: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.3484 - val_acc: 0.7270\n",
      "Epoch 994/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 994: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.3672 - val_acc: 0.7270\n",
      "Epoch 995/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 995: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.4211 - val_acc: 0.7256\n",
      "Epoch 996/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 996: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.4574 - val_acc: 0.7263\n",
      "Epoch 997/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 997: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.4447 - val_acc: 0.7285\n",
      "Epoch 998/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 998: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.4799 - val_acc: 0.7280\n",
      "Epoch 999/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 999: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9984 - val_loss: 3.4862 - val_acc: 0.7260\n",
      "Epoch 1000/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1000: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.4858 - val_acc: 0.7268\n",
      "Epoch 1001/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1001: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.5176 - val_acc: 0.7242\n",
      "Epoch 1002/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1002: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.5196 - val_acc: 0.7258\n",
      "Epoch 1003/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 1003: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.5754 - val_acc: 0.7270\n",
      "Epoch 1004/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1004: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.5640 - val_acc: 0.7256\n",
      "Epoch 1005/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1005: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.5625 - val_acc: 0.7244\n",
      "Epoch 1006/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1006: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.5837 - val_acc: 0.7263\n",
      "Epoch 1007/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1007: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.5806 - val_acc: 0.7249\n",
      "Epoch 1008/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 1008: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6061 - val_acc: 0.7264\n",
      "Epoch 1009/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 1009: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.5741 - val_acc: 0.7266\n",
      "Epoch 1010/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1010: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6210 - val_acc: 0.7266\n",
      "Epoch 1011/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
      "Epoch 1011: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0021 - acc: 0.9988 - val_loss: 3.6245 - val_acc: 0.7246\n",
      "Epoch 1012/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1012: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6293 - val_acc: 0.7255\n",
      "Epoch 1013/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1013: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6367 - val_acc: 0.7255\n",
      "Epoch 1014/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1014: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6479 - val_acc: 0.7256\n",
      "Epoch 1015/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1015: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6533 - val_acc: 0.7252\n",
      "Epoch 1016/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1016: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6773 - val_acc: 0.7273\n",
      "Epoch 1017/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1017: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6529 - val_acc: 0.7253\n",
      "Epoch 1018/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9988\n",
      "Epoch 1018: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9988 - val_loss: 3.6622 - val_acc: 0.7253\n",
      "Epoch 1019/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1019: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6700 - val_acc: 0.7282\n",
      "Epoch 1020/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9984\n",
      "Epoch 1020: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0023 - acc: 0.9984 - val_loss: 3.6645 - val_acc: 0.7253\n",
      "Epoch 1021/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 1021: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.6841 - val_acc: 0.7252\n",
      "Epoch 1022/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1022: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6545 - val_acc: 0.7252\n",
      "Epoch 1023/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1023: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6398 - val_acc: 0.7254\n",
      "Epoch 1024/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 1024: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6918 - val_acc: 0.7235\n",
      "Epoch 1025/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1025: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6867 - val_acc: 0.7256\n",
      "Epoch 1026/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1026: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6891 - val_acc: 0.7256\n",
      "Epoch 1027/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1027: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.6964 - val_acc: 0.7277\n",
      "Epoch 1028/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1028: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7186 - val_acc: 0.7248\n",
      "Epoch 1029/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1029: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7154 - val_acc: 0.7264\n",
      "Epoch 1030/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 1030: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.7027 - val_acc: 0.7284\n",
      "Epoch 1031/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1031: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7400 - val_acc: 0.7270\n",
      "Epoch 1032/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1032: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7507 - val_acc: 0.7260\n",
      "Epoch 1033/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1033: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.7522 - val_acc: 0.7266\n",
      "Epoch 1034/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1034: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.7820 - val_acc: 0.7259\n",
      "Epoch 1035/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 1035: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9987 - val_loss: 3.7465 - val_acc: 0.7274\n",
      "Epoch 1036/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1036: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7469 - val_acc: 0.7264\n",
      "Epoch 1037/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1037: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.7892 - val_acc: 0.7256\n",
      "Epoch 1038/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1038: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7700 - val_acc: 0.7265\n",
      "Epoch 1039/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1039: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.7431 - val_acc: 0.7263\n",
      "Epoch 1040/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1040: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.7597 - val_acc: 0.7260\n",
      "Epoch 1041/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1041: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7793 - val_acc: 0.7241\n",
      "Epoch 1042/1100\n",
      "227/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 1042: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.8002 - val_acc: 0.7249\n",
      "Epoch 1043/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1043: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.7928 - val_acc: 0.7243\n",
      "Epoch 1044/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1044: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8074 - val_acc: 0.7247\n",
      "Epoch 1045/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9984\n",
      "Epoch 1045: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9983 - val_loss: 3.8214 - val_acc: 0.7238\n",
      "Epoch 1046/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1046: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8046 - val_acc: 0.7263\n",
      "Epoch 1047/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1047: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8182 - val_acc: 0.7256\n",
      "Epoch 1048/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9987\n",
      "Epoch 1048: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.8195 - val_acc: 0.7258\n",
      "Epoch 1049/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1049: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.8452 - val_acc: 0.7256\n",
      "Epoch 1050/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 1050: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.8352 - val_acc: 0.7235\n",
      "Epoch 1051/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1051: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8284 - val_acc: 0.7266\n",
      "Epoch 1052/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1052: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8216 - val_acc: 0.7254\n",
      "Epoch 1053/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1053: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8606 - val_acc: 0.7254\n",
      "Epoch 1054/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1054: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8338 - val_acc: 0.7258\n",
      "Epoch 1055/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 1055: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8473 - val_acc: 0.7264\n",
      "Epoch 1056/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1056: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8614 - val_acc: 0.7256\n",
      "Epoch 1057/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1057: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8517 - val_acc: 0.7258\n",
      "Epoch 1058/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 1058: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.8556 - val_acc: 0.7248\n",
      "Epoch 1059/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1059: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8606 - val_acc: 0.7275\n",
      "Epoch 1060/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1060: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8595 - val_acc: 0.7248\n",
      "Epoch 1061/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1061: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8729 - val_acc: 0.7275\n",
      "Epoch 1062/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1062: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8694 - val_acc: 0.7255\n",
      "Epoch 1063/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1063: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8700 - val_acc: 0.7265\n",
      "Epoch 1064/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1064: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.8891 - val_acc: 0.7282\n",
      "Epoch 1065/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1065: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.8771 - val_acc: 0.7242\n",
      "Epoch 1066/1100\n",
      "221/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1066: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8646 - val_acc: 0.7268\n",
      "Epoch 1067/1100\n",
      "223/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 1067: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8707 - val_acc: 0.7246\n",
      "Epoch 1068/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1068: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8786 - val_acc: 0.7241\n",
      "Epoch 1069/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1069: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8839 - val_acc: 0.7255\n",
      "Epoch 1070/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1070: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8813 - val_acc: 0.7252\n",
      "Epoch 1071/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1071: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.8895 - val_acc: 0.7269\n",
      "Epoch 1072/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1072: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.8931 - val_acc: 0.7254\n",
      "Epoch 1073/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1073: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.9036 - val_acc: 0.7248\n",
      "Epoch 1074/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 1074: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9987 - val_loss: 3.9025 - val_acc: 0.7268\n",
      "Epoch 1075/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1075: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.9261 - val_acc: 0.7258\n",
      "Epoch 1076/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1076: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.8467 - val_acc: 0.7236\n",
      "Epoch 1077/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0026 - acc: 0.9984\n",
      "Epoch 1077: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0026 - acc: 0.9984 - val_loss: 3.7246 - val_acc: 0.7284\n",
      "Epoch 1078/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9981\n",
      "Epoch 1078: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0039 - acc: 0.9981 - val_loss: 3.5316 - val_acc: 0.7204\n",
      "Epoch 1079/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0041 - acc: 0.9981\n",
      "Epoch 1079: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0041 - acc: 0.9981 - val_loss: 3.3647 - val_acc: 0.7248\n",
      "Epoch 1080/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9980\n",
      "Epoch 1080: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0038 - acc: 0.9980 - val_loss: 3.3174 - val_acc: 0.7229\n",
      "Epoch 1081/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0027 - acc: 0.9985\n",
      "Epoch 1081: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0028 - acc: 0.9984 - val_loss: 3.3675 - val_acc: 0.7238\n",
      "Epoch 1082/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1082: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.4476 - val_acc: 0.7273\n",
      "Epoch 1083/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1083: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.4214 - val_acc: 0.7262\n",
      "Epoch 1084/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9986\n",
      "Epoch 1084: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.4714 - val_acc: 0.7289\n",
      "Epoch 1085/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1085: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.5508 - val_acc: 0.7285\n",
      "Epoch 1086/1100\n",
      "232/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1086: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.5827 - val_acc: 0.7282\n",
      "Epoch 1087/1100\n",
      "230/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1087: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.5750 - val_acc: 0.7280\n",
      "Epoch 1088/1100\n",
      "226/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1088: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.5723 - val_acc: 0.7278\n",
      "Epoch 1089/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1089: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.5958 - val_acc: 0.7266\n",
      "Epoch 1090/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1090: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.6108 - val_acc: 0.7270\n",
      "Epoch 1091/1100\n",
      "225/235 [===========================>..] - ETA: 0s - loss: 0.0021 - acc: 0.9988\n",
      "Epoch 1091: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0021 - acc: 0.9988 - val_loss: 3.6342 - val_acc: 0.7250\n",
      "Epoch 1092/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1092: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.6428 - val_acc: 0.7268\n",
      "Epoch 1093/1100\n",
      "233/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1093: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.6584 - val_acc: 0.7272\n",
      "Epoch 1094/1100\n",
      "228/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1094: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.6707 - val_acc: 0.7267\n",
      "Epoch 1095/1100\n",
      "229/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9987\n",
      "Epoch 1095: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9986 - val_loss: 3.6709 - val_acc: 0.7260\n",
      "Epoch 1096/1100\n",
      "222/235 [===========================>..] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1096: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.6725 - val_acc: 0.7266\n",
      "Epoch 1097/1100\n",
      "234/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1097: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.7017 - val_acc: 0.7260\n",
      "Epoch 1098/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9985\n",
      "Epoch 1098: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0022 - acc: 0.9985 - val_loss: 3.6729 - val_acc: 0.7271\n",
      "Epoch 1099/1100\n",
      "235/235 [==============================] - ETA: 0s - loss: 0.0021 - acc: 0.9985\n",
      "Epoch 1099: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9985 - val_loss: 3.7126 - val_acc: 0.7249\n",
      "Epoch 1100/1100\n",
      "231/235 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9986\n",
      "Epoch 1100: val_loss did not improve from 3.94504\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.0021 - acc: 0.9986 - val_loss: 3.6930 - val_acc: 0.7268\n",
      "<keras.src.engine.sequential.Sequential object at 0x7fcdf69816c0>\n"
     ]
    }
   ],
   "source": [
    "#조기종료 없을 경우\n",
    "\n",
    "import re\n",
    "import os\n",
    "from tensorflow.keras.layers import Embedding,Dense,LSTM,Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "embedding_dim=100\n",
    "hidden_dim=128\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "model=Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size,embedding_dim))\n",
    "\n",
    "model.add(Bidirectional(LSTM(hidden)))\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\n",
    "#es=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=4) #조기종료\n",
    "mc=ModelCheckpoint('test_model.h5',moniter='val_acc',mode='max',verbose=1,save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(x_train,y_train,epochs=1100,callbacks=[mc],batch_size=256,validation_split=0.2)\n",
    "\n",
    "load_model=load_model('test_model.h5')\n",
    "print(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4401f-26ef-4bac-aacd-34215fb53462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3d73fe-a83c-4cfe-b692-7bc2f96550e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f7314-31fa-4a10-8f53-eb7efb02596d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf9147-b16a-4eb2-98a2-2a42cbb1d4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e118621a-8b94-4068-ad82-4ee86741e598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826866b-4549-4d80-9014-1dabf4c7398b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d33a81-b4ce-4021-b179-0a270bd07bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfea9f1-6d49-401f-b8e8-b9a0ad0480b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f0a64-fbbd-42c8-a999-1caae135db6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86495f5c-fc82-42b1-9103-dbe69891f3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80e2ab7-4fe7-48fb-8207-341cc423f7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b636b-92e6-4d6b-a226-62cf65bbec67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda81899-d0f5-47cf-aebf-07d6f1389aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bcf3a-4684-45e7-a225-300dd104161d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544751f3-20b6-4cde-8404-cf3a8f19bed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed03b0-13ec-4b9b-a8be-f82d16740fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
